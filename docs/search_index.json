[["index.html", "Data Analysis in R for Consumer Science Chapter 1 Introduction", " Data Analysis in R for Consumer Science Morten Arendt Rasmussens 2022-09-23 Chapter 1 Introduction This material is to cover data analysis using R for consumer science targeting the following courses: Meal Systems and Technologies Food Consumer Research Meal Consumer Research Thematic Course in Food Innovation and Health Public Health Nutrition But others may benefit from the material as well…. The initial chapters of the book introduce R and the background for the different statistical methods used in the book. The following parts of the book is divided according to the course hence the data types encountered in the above mentioned courses, The format of the book is simple - first read the written text and when you reach a code, copy the code and try it out yourself. "],["introduction-to-pca.html", "Chapter 2 Introduction to PCA", " Chapter 2 Introduction to PCA In this book Principal Component Analysis (PCA) is used several times. This chapter will shortly explain the theory behind PCA and the interpretation of relevant plots. PCA is a tool for looking a correlation structure between variables, and groupings of samples. All through visualizations. Check out youtube on the subject for an introduction. [TILFØJ: skrives kort om PCA, skal der ikke stå lidt mere???] knitr::include_url(&quot;https://youtube.com/embed/NFIkD9-MuTY&quot;) [INDSÆTTE: flere film // you tube klip] "],["introduction-to-r.html", "Chapter 3 Introduction to R 3.1 How to import data", " Chapter 3 Introduction to R R is a free software with a complete programming language for statistical computing and graphics. It is used at many universities and companies since it is always updated and open source. Before staring your calculations in R you should always update the R version on your computer. You can install a graphical interface for R, called R studio. It will use the underlying version of R on your computer – so you have to have R installed too. The principles by R and R Studio are the same – BUT R Studio has a better interface for non-programmers. Both R and R Studio can be used on all types of computers. R is command-line based and it provides a wide variety of statistical methods (linear and nonlinear modelling, classical statistical tests, classification, clustering, …). Advanced methods are available via extension packages (more than 10.000 at the moment) Always be sure to have the latest version of both R and R Studio on your computer - update version just before you need to use the program. [INDSÆTTE: Bodil der snakker om opsætningen af R Studio. Forslag er slide 4-5-6 i Bodils R intro præsentation] ##How to get started - understanding R To get started go to upper left corner and open a new script. Remember to save your script as well. To out our codes: 1+2 ## [1] 3 a &lt;- 2+2 b &lt;- 5+3 What happens if you write the letter a in the editor and run it? What about the letter b? a ## [1] 4 b ## [1] 8 a+b ## [1] 12 … or this one? A+B R is essentially one really but case sensitive calculator. In the Editor: “#” is the start of a comment (means: will not evaluated/ read by the program). This is how you can make comments in your script “:”Generates a sequence (e.g. 1:10 is the numbers from 1 to 10) In the Console: “&gt;” indicates that R is ready for a new code. “+” Instead of “&gt;” means that the program is waiting for you. (you probably made a mistake in the script you tried to run) – by [ESC] the “+” turns to a “&gt;” again “NA” (Not Available) is indicating a missing value “NaN” (Not a Number) is the result of an ‘illegal’ operation e.g. log(-1) Red sentences means there is an error. R will stop calculating at the first error it meets. 3.1 How to import data Before you start you data import, you have to make sure the data set contains all the information you need and the format of the data (columns and rows) is correct. You can import in many different ways. This book will show you two: [LAVE PUNKTOPSTILLLING?? MEN VED IKKE HVORDAN??] Importing a csv file Importing an Excel file/sheet 3.1.1 Importing a csv file [TEKST MANGLER] 3.1.2 Importing an Excel file/sheet If your Excel file contains more than one sheet, you have to import each sheet separately. Here we use the package readxl with the function read_excel.If the data is not in the same folder as your script, then include the path to the data, or move the data to the script’s location. When you have to find the path for the file on your computer, you …. [INDSÆT BESKRIVELSE af hvordan man gør dette med Tabulator knappen til at finde stien]. The first part of the model sentence is what we want to call our dataset, here “DATASET1”. You decide the names/titles of your datasets and model, just do not use other signs than “.” and avoid non-English letters. [SPØRGSMÅL // INDSÆT: Vi skal bruge et datasæt til at lave import af, skal det bare det samme for for buffet data? ] library(readxl) DATASET1 &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;BuffetData&#39;) DATASET2 &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;SurveyData&#39;) 3.1.3 Looking at the imported elements Have a look at the imported elements to ensure that indeed, they mimic the Excel-sheets. Use the functions head(), str() and View() is your tools. They will give you the headlines in your data, how your variables are categorised and open the dataset in a new tab. head(DATASET1) str(DATASET1) View(DATASET1) [NEW STUFF NEEDED IF DATA CHANGED: We see that the coloum with names (Person and StationName) is interpreted as characters (chr) while the stuff which should be numbers (Comsuption) is numeric (num). If that is not the case, you will need to transform them using as.numeric() or as.character().] ##How to save the data (MANGLER!!) ##How to export tables and plots (MANGLER!!) 3.1.4 Save the data You can export any data frame from R to excel (for instance using the rio package), as well as saving it as .RData for further analysis. Use save.image() to save everything, or use save() to specify which elements to save save.image(file = &#39;iBuffetSurveyDataEverything.RData&#39;) # everything save(file = &#39;iBuffetSurveyData.RData&#39;, list = c(&#39;Survey&#39;,&#39;Surveylong_buffet&#39;, &#39;Surveylong&#39;,&#39;Buffet_survey&#39;,&#39;Surveyscales&#39;)) # just the usesul and non-redundant stuff. rio::export(Surveylong_buffet,file = &#39;Surveylong_buffet.xlsx&#39;) # export one data frame [TILFØJ: Indtalt forklaring på koderne?] 3.1.5 Ready for analysis Once you have saved the data, you can simply load the data directly, and you do not need to do the import-setup every time you want to do an analysis on the data. This part is not a part of the data import, but it is a good idea just to check that the data indeed is setup as expected. load(&#39;iBuffetSurveyData.RData&#39;) "],["how-to-import-your-data.html", "Chapter 4 How to import your data 4.1 iBuffet data - Read in data from Excel", " Chapter 4 How to import your data In this book several datasets are used targeting different research questions. However, a fair part of the analysis tools are common. That is, descriptive analysis, plots, response correlations etc. The data is included in the R-packgage devtools you get by running the code below. Be aware that you need devtools package to install from github, so you need to run both code lines. # install data-package install.packages(&#39;devtools&#39;) devtools::install_github(&#39;mortenarendt/data4consumerscience&#39;) The data is also available as excel sheets, and can be loaded using packages capable of reading from Excel. Below is an example using data from the so-called iBuffet. 4.1 iBuffet data - Read in data from Excel The data from the iBuffet comes in the form of csv or Excel files. Lad os se om skidtet virker… - take 2 These can be in the form of Consumption data from the Buffet Survey data on liking, motivation, choices etc linked to the particular buffet data Survey data on demographics for the participants such as age, gender, eating habits etc. These are general and different from the former, in that they have nothing to do with the current buffet. [Måske en ide at TILFØJE noget a la dette: In the dataset you should have one line per buffet station per participant per experimental day. In the example below: P07 have chosen both from the Pasta with legumes and the Pasta with mushrooms buffet on Day 1, whereas P07 have only chosen from the Pasta with mushrooms buffet on Day 2. The consumption is in gram. See example below.] Example of Buffet data 4.1.1 Example of Survey data [TILFØJ: In the dataset you should have one line per participant per day. If the survey is not related to the buffet data (assessment of either samples or meals) you only need to have one line per participant. In the example below: P01 has answered questions in coloumn C and on both days.] Example of Survey data [RETTELSE AF SCREEN SHOTS: Excel skal zoomes mere ind, da det er ulæseligt. Måske endda også cutte “toppen” af på 2.1 Read in data from Excel - iBuffet, så det kun er selve Excel arket der ses?] 4.1.2 Example of Survey Scale [TILFØJ: When you have statements as answers in your survey, you might need to translate these in to numbers. In the example below, you can see which statement corresponds to which number, if the scale is conveterted to a numerical scale SLET: toppen af Excel arket, så det er mere læseligt] Survey Scale used 4.1.3 Edit your dataset in Excel Turn the individual files (Buffet data and survey data) into sheets in Excel collecting all your data in one file. Setup the data in Excel such that they match the above in terms of format. What is important is: First row is used on headings and none of these are repeated. I.e. all unique within a sheet Data comes from row 2 All rows should contain data (empty cell as also data, e.g. an unanswered question), so all empty rows are removed Headings between sheets referring to the same: e.g. participant ID should have exactly similar heading. If you have calculated stuff within Excel such as a sum of the numbers in a column, then these should be removed from the sheet. It is not data! We suggest that you keep both the original version of the data as a sheet, and the ready-to-import version as a sheet, so you do not accidentially delete data. 4.1.4 Importing to R Each of the Excel sheets are imported separately. Here we use the package readxl with the function read_excel. If the data is not in the same folder as your script, then include the path to the data, or move the data to the script’s location. Be aware that the SurveyScale sheet (see above) does not have a heading. Here we import without (col_names = F), and set it manually afterwards, but you can also put it in manually in Excel beforehand. [MORTEN: Skal der stå noget om hvordan de gemmer et script?? når du selv skriver noget med at gemme ovenfor. vi kan risikere de ikke ved noget om det… Eller skal vi beholde min intro som indtalt, hvor de kommer igennem import, faktorer osv.?] library(readxl) Buffet &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;BuffetData&#39;) Survey &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;SurveyData&#39;) Surveyscales &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;, sheet = &#39;SurveyScale&#39;, col_names = F) colnames(Surveyscales) &lt;- c(&#39;answ&#39;,&#39;number&#39;) [TILFØJ: Her kunne det være en ide med enten en mp3 hvor koden gennemgåes eller en mp4 hvor skærmen optages med koden på samtidig med, der tegnes på skærmen rundt om delene og forklares. Jeg har sendt dig et forslag] Have a look at the imported elements to ensure that indeed, they mimic the Excel-sheets. head(), str() and View() is your tools. head(Buffet) ## # A tibble: 6 × 4 ## Person Day StationName Consumption ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 P01 1 Pasta with legumes 90 ## 2 P01 1 Pasta with mushroom 148 ## 3 P02 1 Pasta with legumes 172 ## 4 P02 1 Pasta with mushroom 40 ## 5 P03 1 Pasta with legumes 56 ## 6 P03 1 Pasta with mushroom 52 str(Buffet) ## tibble [60 × 4] (S3: tbl_df/tbl/data.frame) ## $ Person : chr [1:60] &quot;P01&quot; &quot;P01&quot; &quot;P02&quot; &quot;P02&quot; ... ## $ Day : num [1:60] 1 1 1 1 1 1 1 1 1 1 ... ## $ StationName: chr [1:60] &quot;Pasta with legumes&quot; &quot;Pasta with mushroom&quot; &quot;Pasta with legumes&quot; &quot;Pasta with mushroom&quot; ... ## $ Consumption: num [1:60] 90 148 172 40 56 52 66 86 304 336 ... [TILFØJ: speak eller film som beskrevet ovenfor] [TILFØJ: view() i kodeline ovenfor] We see that the coloum with names (Person and StationName) is interpreted as characters (chr) while the stuff which should be numbers (Comsuption) is numeric (num). If that is not the case, you will need to transform them using as.numeric() or as.character(). [TILFØJ: Morten, kan du skrive script eksempel ind på dem?] 4.1.5 Editing in R The Buffet data is optimal as is. We have the data as long format with all repsonses in one coloumn and then the next columns clarifying the design, time, type, person etc. However the Survey data is not optimal directly. Things to fix: * For the last four questions, we to encode the the 7-point answers as a numerical factor, and have it correctly leveled. * The data can additionally be versioned in both long and wide format. [RET: var det meningen ovensåtende skulle være som punktform?] [For this we use the function Tidyverse … MORE INFO Morten] [TILFØJ: synes der skal være en speak til denne også.. Kan du hjælpe mig med den?] library(tidyverse) Surveylong &lt;- Survey %&gt;% gather(question,answ, `Pasta with legumes is visually appealing to me. `: `I like the taste of pasta with mushrooms! `) %&gt;% mutate(answ = answ %&gt;% factor(levels = Surveyscales$answ), answnum = answ %&gt;% as.numeric()) Surveywide &lt;- Surveylong %&gt;% select(-answ) %&gt;% spread(question,answnum) 4.1.6 Merging the data For the sake of being able to compare consumption (obtained from buffet data) with liking and motives (obtained from the survey data) these data frames needs to be merged. There are several merge options, here we use left_join() but full_join() and right_join() might more suited in some situations. [TILFØJ: If you feel more comfortable with Excel, you can also merge the two data frames in one Excel sheet before importing it] [MORTEN: Du skal forklare forskelle på disse tre ellers kun bruge en, da det er forvirrende hvad forskellen er] 4.1.6.1 Adding survey to buffets Merging should be done such that Person and Day in each separate sheet match. If you additionally have demographic data (gender, age, etc.) then obviously only Person should match, as the data is constant over Days. [TILFØJ: Indtalt forklaring på koden?] Buffet_survey &lt;- Buffet %&gt;% left_join(Surveywide, by = c(&#39;Person&#39;,&#39;Day&#39;)) 4.1.6.2 Adding buffet to survey Similarly, merging should be done such that Person and Day match. If you additionally have demographic data (gender, age, etc.) then obviously only Person should match, as the data is constant over Days. Further, we use the long format of the survey data here. Surveylong_buffet &lt;- Surveylong %&gt;% left_join(Buffet, by = c(&#39;Person&#39;,&#39;Day&#39;)) Due to not having a total overlap of information, some responses (here for consumption) will be missing. That you can see using the table function. table(is.na(Surveylong_buffet$Consumption)) ## ## FALSE ## 240 [TILFØJ: Indtalt forklaring på koderne?] 4.1.7 Save the data You can export any data frame from R to excel (for instance using the rio package), as well as saving it as .RData for further analysis. Use save.image() to save everything, or use save() to specify which elements to save save.image(file = &#39;iBuffetSurveyDataEverything.RData&#39;) # everything save(file = &#39;iBuffetSurveyData.RData&#39;, list = c(&#39;Survey&#39;,&#39;Surveylong_buffet&#39;, &#39;Surveylong&#39;,&#39;Buffet_survey&#39;,&#39;Surveyscales&#39;)) # just the usesul and non-redundant stuff. rio::export(Surveylong_buffet,file = &#39;Surveylong_buffet.xlsx&#39;) # export one data frame [TILFØJ: Indtalt forklaring på koderne?] 4.1.8 Ready for analysis Once you have saved the data, you can simply load the data directly, and you do not need to do the import-setup every time you want to do an analysis on the data. This part is not a part of the data import, but it is a good idea just to check that the data indeed is setup as expected. load(&#39;iBuffetSurveyData.RData&#39;) "],["libraries.html", "Chapter 5 Libraries", " Chapter 5 Libraries R comes with a bit of functionality. However, most of the useful tools in R is distributed as packages. There are +10.000 package for R, so it is a jungle to figure out what the most easy solution to your problem at hand is. However, the teams who have made tidyverse and ggplot2 etc. have made a lot of things much more easy, and we strongly rely on their tools and routines in data analysis. To install packages from CRAN (the main repo where R-packages are distributed) install.packages(&#39;somepackage&#39;) To install packages from github (the place where all the development and general code sharing is distributed) devtools::install_github(&#39;developername/packagename&#39;) To make packages available within your analysis use library(), or use the package name followed by :: and the function library(ggplot2) # lets plot dadta library(tidyverse) library(ggpubr) # lets add stats to the plots library(knitr) # lets make nice tables ggplot2::qplot(rnorm(100)) # example of a function call without library&#39;ing the package. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. "],["descriptive-statistics-and-plotting.html", "Chapter 6 Descriptive statistics and plotting 6.1 Distributions of count data 6.2 Scatter plots", " Chapter 6 Descriptive statistics and plotting load(&#39;iBuffetSurveyData.RData&#39;) 6.1 Distributions of count data The table function is good in getting how many observations there are within a given vector, or combinations of several vectors. Here used on the Survey answers. table(Surveylong$answ) # across everything ## ## A. Strongly disagree B. Disagree C. More or less disagree ## 0 2 6 ## D. Neither agree nor disagree E. More or less agree F. Agree ## 7 21 32 ## G. Strongly agree ## 52 table(Surveylong$question,Surveylong$answ ) # across question ## ## A. Strongly disagree B. Disagree ## I like the taste of pasta with legumes!  0 1 ## I like the taste of pasta with mushrooms!  0 0 ## Pasta with legumes is visually appealing to me.  0 1 ## Pasta with mushrooms is visually appealing to me.  0 0 ## ## C. More or less disagree D. Neither agree nor disagree ## I like the taste of pasta with legumes!  3 2 ## I like the taste of pasta with mushrooms!  0 0 ## Pasta with legumes is visually appealing to me.  3 3 ## Pasta with mushrooms is visually appealing to me.  0 2 ## ## E. More or less agree F. Agree G. Strongly agree ## I like the taste of pasta with legumes!  5 8 11 ## I like the taste of pasta with mushrooms!  4 11 15 ## Pasta with legumes is visually appealing to me.  6 7 10 ## Pasta with mushrooms is visually appealing to me.  6 6 16 You see that most of the answers are in agreement with question, and that there are no observations in the Strongly disagree category. The tidyverse way Lets do exactly the same just using tidyverse functions count(), group_by(), mutate(), and summarise(). Further, lets print the results in a nice looking table using kable() from the knitr package. # tb &lt;- Surveylong %&gt;% # count(question,answ,Day,name = &quot;no_rows&quot;, .drop = F) # kable(tb, caption = &#39;some caption&#39;) The numbers are absolute, but may be better represented by proportions. tb &lt;- Surveylong %&gt;% group_by(question,Day) %&gt;% dplyr::mutate(ntot = n()) %&gt;% group_by(question,answ,Day) %&gt;% dplyr::summarise(n = n(), prc = 100*n / ntot[1]) ## `summarise()` has grouped output by &#39;question&#39;, &#39;answ&#39;. You can override using the `.groups` argument. kable(tb, caption = &#39;some caption&#39;, digits = 1) Table 6.1: some caption question answ Day n prc I like the taste of pasta with legumes!  B. Disagree 1 1 6.7 I like the taste of pasta with legumes!  C. More or less disagree 2 3 20.0 I like the taste of pasta with legumes!  D. Neither agree nor disagree 1 1 6.7 I like the taste of pasta with legumes!  D. Neither agree nor disagree 2 1 6.7 I like the taste of pasta with legumes!  E. More or less agree 1 4 26.7 I like the taste of pasta with legumes!  E. More or less agree 2 1 6.7 I like the taste of pasta with legumes!  F. Agree 1 3 20.0 I like the taste of pasta with legumes!  F. Agree 2 5 33.3 I like the taste of pasta with legumes!  G. Strongly agree 1 6 40.0 I like the taste of pasta with legumes!  G. Strongly agree 2 5 33.3 I like the taste of pasta with mushrooms!  E. More or less agree 1 2 13.3 I like the taste of pasta with mushrooms!  E. More or less agree 2 2 13.3 I like the taste of pasta with mushrooms!  F. Agree 1 5 33.3 I like the taste of pasta with mushrooms!  F. Agree 2 6 40.0 I like the taste of pasta with mushrooms!  G. Strongly agree 1 8 53.3 I like the taste of pasta with mushrooms!  G. Strongly agree 2 7 46.7 Pasta with legumes is visually appealing to me.  B. Disagree 1 1 6.7 Pasta with legumes is visually appealing to me.  C. More or less disagree 1 1 6.7 Pasta with legumes is visually appealing to me.  C. More or less disagree 2 2 13.3 Pasta with legumes is visually appealing to me.  D. Neither agree nor disagree 1 2 13.3 Pasta with legumes is visually appealing to me.  D. Neither agree nor disagree 2 1 6.7 Pasta with legumes is visually appealing to me.  E. More or less agree 1 1 6.7 Pasta with legumes is visually appealing to me.  E. More or less agree 2 5 33.3 Pasta with legumes is visually appealing to me.  F. Agree 1 5 33.3 Pasta with legumes is visually appealing to me.  F. Agree 2 2 13.3 Pasta with legumes is visually appealing to me.  G. Strongly agree 1 5 33.3 Pasta with legumes is visually appealing to me.  G. Strongly agree 2 5 33.3 Pasta with mushrooms is visually appealing to me.  D. Neither agree nor disagree 1 1 6.7 Pasta with mushrooms is visually appealing to me.  D. Neither agree nor disagree 2 1 6.7 Pasta with mushrooms is visually appealing to me.  E. More or less agree 1 3 20.0 Pasta with mushrooms is visually appealing to me.  E. More or less agree 2 3 20.0 Pasta with mushrooms is visually appealing to me.  F. Agree 2 6 40.0 Pasta with mushrooms is visually appealing to me.  G. Strongly agree 1 11 73.3 Pasta with mushrooms is visually appealing to me.  G. Strongly agree 2 5 33.3 … and a plot of it tb %&gt;% ggplot(data = ., aes(answ,prc, fill = factor(Day))) + geom_bar(stat = &#39;identity&#39;, position = position_dodge()) + facet_wrap(~question) + theme(axis.text.x = element_text(angle = 45,hjust = 1), legend.position = &#39;top&#39;) 6.1.1 Descriptives for a continouos variable Here just across the entire sample set. mean(Buffet_survey$Consumption) ## [1] 138 median(Buffet_survey$Consumption) ## [1] 126 sd(Buffet_survey$Consumption) ## [1] 85.45472 IQR(Buffet_survey$Consumption) ## [1] 112.5 summary(Buffet_survey$Consumption) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 74.0 126.0 138.0 186.5 378.0 This is a very high level representation, and we usually want to compare means (or other metrics) between different groups. We use the consumption and split it according to day and pasta-type. tb2 &lt;- Buffet_survey %&gt;% group_by(StationName, Day) %&gt;% dplyr::summarise(nobs = n(), mean = mean(Consumption), median = median(Consumption), sd = sd(Consumption), iqr = IQR(Consumption), q25 = quantile(Consumption,0.25), q75 = quantile(Consumption,0.75)) ## `summarise()` has grouped output by &#39;StationName&#39;. You can override using the `.groups` argument. kable(tb2, digits = 1, caption = &#39;some relevant caption&#39;) Table 6.2: some relevant caption StationName Day nobs mean median sd iqr q25 q75 Pasta with legumes 1 15 132.3 130 81.2 101 71 172 Pasta with legumes 2 15 155.7 138 103.7 139 71 210 Pasta with mushroom 1 15 135.1 92 88.7 99 76 175 Pasta with mushroom 2 15 128.9 124 71.3 85 86 171 Corresponding plot of data Buffet_survey %&gt;% ggplot(data = ., aes(StationName,Consumption, fill = factor(Day))) + geom_violin() Buffet_survey %&gt;% ggplot(data = ., aes(StationName,Consumption, fill = factor(Day))) + geom_boxplot() + geom_jitter() Corresponding plots of the results ggplot(data= tb2, aes(factor(StationName):factor(Day),mean, color = factor(Day), ymin = mean-sd, ymax = mean + sd)) + geom_point() + geom_errorbar(width = 0.3)+ theme(axis.text.x = element_text(angle = 45,hjust = 1)) + ylab(&#39;mean+/- 1*sd&#39;) 6.1.1.1 In relation to protein considerations Try to make these descriptive analysis and plots taking into account whether the participants considered protein content, and why they did. Here is some inspiration. Buffet_survey %&gt;% ggplot(data = ., aes(`Did you consider the protein content of the dish(es) you chose?`, Consumption)) + geom_boxplot() + geom_jitter() + facet_wrap(~StationName) 6.2 Scatter plots Lets plot the consumption as a function of the answers to the liking-scale questions of the survey, and split it into day and type of vegetable. If you think about it, it is pretty many plots, but the ggplot2 functionality facet_wrap() on a long format data frame does it in few lines: Surveylong_buffet %&gt;% filter(!is.na(StationName )) %&gt;% mutate(question2 = question %&gt;% substr(1,34)) %&gt;% # The label is to long, so lets just represent the first 30 letters. ggplot(data = ., aes(answnum,Consumption, color = factor(Day))) + geom_point() + stat_smooth(se = F, method = lm) + stat_cor() + facet_grid(question2 ~ StationName) + theme_bw() + theme(legend.position = &#39;bottom&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; kable(Surveyscales, caption = &#39;Just a table to have what the 7point likert scale numbers mean&#39;) Table 6.3: Just a table to have what the 7point likert scale numbers mean answ number A. Strongly disagree 1 B. Disagree 2 C. More or less disagree 3 D. Neither agree nor disagree 4 E. More or less agree 5 F. Agree 6 G. Strongly agree 7 Get this stuff to work, and try to interpret what you see: which factors seems important for the portion size ( Consumption )? The summary-stats in the scatter plots above: What is this? How is it interpreted? test4 "],["pca-on-survey-answers.html", "Chapter 7 PCA on survey answers 7.1 Bi-plot", " Chapter 7 PCA on survey answers [RETTES: Jeg ville rette overskriften her, så det blot hedder PCA? Så kan vi have PCA på survey data længere nede, også overskrift med PCA på CATA data og en på ??? (ved ikke om der skal mere på)] [TILFØJ: In this chapter we will look at how to conduct Principal Component Analysis (PCA).] [INDSÆT/FLYT denne tekst, som er sat ind nedenfor: PCA is a tool for looking a correlation structure between variables, and groupings of samples. All through visualizations. Check out youtube on the subject for an introduction. ] knitr::include_url(&quot;https://youtube.com/embed/NFIkD9-MuTY&quot;) [SPØRGSMÅL FRA BODIL: hvad med pakke til at regne modellen ud? bør den ikke også stå på listen] We use a package called ggbiplot for plotting the PCA model. It is located on github and installed by the follow two code lines: install.packages(&#39;devtools&#39;) devtools::install_github(&#39;vqv/ggbiplot&#39;) [SPØRGSMÅL FRA BODIL: når du kun skriver ggbiplot behøves, hvorfor står der så en lang liste over pakken nedenunder her også - det forvirrer. Så enten er de unødvendige eller også skal der stå noget mere tekst på dem, så det hele matcher i måden det gøres på?] library(ggplot2) # lets plot dadta library(tidyverse) library(broom) library(broom.mixed) library(lme4) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack library(ggbiplot) ## Loading required package: plyr ## ---------------------------------------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ---------------------------------------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following object is masked from &#39;package:ggpubr&#39;: ## ## mutate ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact ## Loading required package: scales ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor ## Loading required package: grid library(ggpubr) # lets add stats to the plots library(knitr) # lets make nice tables [MANGLER: også en forklaring på denne pakke under her, hvad bruges den til? og min R siger den ikke findes? Er det fordi du selv er ved at lave den?] library(data4consumerscience) PCA is a tool for looking a correlation structure between variables, and groupings of samples. All through visualizations. Check out youtube on the subject for an introduction. PCA takes numerical data as input, so we use the likert-scales in the form of 1 to 7. Further the yes/no answers are included, and also needs to be changed. x &lt;- pasta %&gt;% mutate(Did_you_take_food_from_both_Dish1_and_Dish2 = Did_you_take_food_from_both_Dish1_and_Dish2 %&gt;% factor %&gt;% as.numeric(), Did_you_consider_the_proteincontent_of_the_dishes_you_choose = Did_you_consider_the_proteincontent_of_the_dishes_you_choose %&gt;% factor() %&gt;% as.numeric()) %&gt;% mutate_if(is.factor, as.numeric) %&gt;% filter(Day==1) %&gt;% # the survey part is the same for both days and both stations. That is what we keep. filter(str_detect(StationName,&#39;leg&#39;)) PCAmdl &lt;- prcomp(x[,c(5:6,8:11)],scale. = T) 7.1 Bi-plot And a plot of the model ggbiplot(PCAmdl, varname.size = 5) + ylim(c(-4,4)) + xlim(c(-2,5)) What does component 1 (PC1) reflect? What does PC2 reflect? Lets plot the model and color the samples according to the consumption (of legumes) cutted at the median. ggbiplot(PCAmdl, groups = factor(x$Consumption&gt;130), ellipse = T, varname.size = 5) + ylim(c(-4,4)) + xlim(c(-3,5)) 7.1.1 Extract the components and run all associations. We are interested in if any of the likert/survey traits reflected by PCA is correlated with consumption. It is a little complicated, but here goes scores &lt;- data.frame(Person = x$Person, PCAmdl$x[,1:2]) # take out the first two components. tbmixed &lt;- pasta %&gt;% left_join(scores, by = &#39;Person&#39;) %&gt;% gather(comp,score,PC1:PC2) %&gt;% group_by(StationName,comp) %&gt;% do(lmer(data = ., Consumption~score + Day + (1|Person)) %&gt;% tidy(conf.int = T)) … Make a table and a plot of the results. tbmixed %&gt;% filter(term==&#39;score&#39;) %&gt;% select(-effect,-group) %&gt;% kable(x = .,caption = &#39;Slopes according to components&#39;, digits = 2) Table 7.1: Slopes according to components StationName comp term estimate std.error statistic conf.low conf.high Pasta with legumes PC1 score 14.28 13.26 1.08 -11.70 40.27 Pasta with legumes PC2 score -19.47 17.80 -1.09 -54.34 15.41 Pasta with mushroom PC1 score 5.15 10.69 0.48 -15.80 26.11 Pasta with mushroom PC2 score -4.93 14.43 -0.34 -33.21 23.35 tbmixed %&gt;% filter(term==&#39;score&#39;) %&gt;% ggplot(data = ., aes(comp,estimate,ymin = conf.low, ymax = conf.high)) + geom_errorbar(width = 0.1) +geom_point()+ geom_hline(yintercept = 0) + facet_grid(~StationName) + theme(legend.position = &#39;bottom&#39;) Interpret the results. "],["linear-models.html", "Chapter 8 Linear models 8.1 Example 8.2 Run a bunch of models at once", " Chapter 8 Linear models Linear models is a general term for models with a single univariate response (dependent variable - \\(y\\) in the formula below), which we want to describe using one or several predictors (independent variables - \\(x\\) in the formula below). \\[ y = a + b \\cdot x + e \\] Here the informative parameter is the slope (\\(b\\)) which indicates the relation between \\(x\\) and \\(y\\). (\\(e\\) is the missfit / residuals of the model). We use tidyverse coding as this makes life much easier. As an tidyverse add on, we use broom for the linear models, broom.mixed and lme4 for the linear mixed models. library(ggplot2) # lets plot dadta library(tidyverse) library(broom) library(broom.mixed) library(lme4) library(ggpubr) # lets add stats to the plots library(knitr) # lets make nice tables The data is already imported, and formated (see Getting_data_in.pdf for details). We simply load this file. library(data4consumerscience) 8.1 Example As response variable, the amount of Consumption of Pasta with mushrooms and use the likert scale I like the taste of pasta with mushrooms! as predictor. We use ONLY Day 1 results. First a plot: pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) %&gt;% filter(Day==1) %&gt;% ggplot(data = ., aes(I_like_taste_of_pasta_with_mushrooms,Consumption)) + geom_point() + stat_smooth(method = lm, se = F) ## `geom_smooth()` using formula &#39;y ~ x&#39; It seems as there is something. So lets build a linear model on this # subset the data x &lt;- pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) %&gt;% filter(Day==1) mdl &lt;- lm(data = x, Consumption~I_like_taste_of_pasta_with_mushrooms) mdl ## ## Call: ## lm(formula = Consumption ~ I_like_taste_of_pasta_with_mushrooms, ## data = x) ## ## Coefficients: ## (Intercept) I_like_taste_of_pasta_with_mushroomsAgree ## 89.00 6.60 ## I_like_taste_of_pasta_with_mushroomsStrongly agree ## 82.25 summary(mdl) ## ## Call: ## lm(formula = Consumption ~ I_like_taste_of_pasta_with_mushrooms, ## data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -119.25 -49.60 -3.00 35.58 164.75 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89.00 60.43 1.473 0.167 ## I_like_taste_of_pasta_with_mushroomsAgree 6.60 71.50 0.092 0.928 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 82.25 67.56 1.217 0.247 ## ## Residual standard error: 85.46 on 12 degrees of freedom ## Multiple R-squared: 0.2043, Adjusted R-squared: 0.07173 ## F-statistic: 1.541 on 2 and 12 DF, p-value: 0.2537 The the slope indicates that by increasing liking by one unit the consumption increase is \\(50.2 \\pm 30.4\\), however, this apparent effect is not statistically significant (\\(p = 0.12\\)). 8.2 Run a bunch of models at once We want to model consumption of both pasta with mushrooms andd legumes, and look at all the likert scales questions as predictors. Further we want to do this for both days. First we create a new long format data frame pastalong &lt;- pasta %&gt;% gather(question,answ,I_like_taste_of_pasta_with_legumes:Pasta_with_mushrooms_is_visually_appealing) %&gt;% mutate(answnum = factor(answ,labels = c(&#39;Disagree&#39;,&#39;More or less disagree&#39;,&#39;Neither agree nor disagree&#39;,&#39;More or less agree&#39;,&#39;Agree&#39;,&#39;Strongly agree&#39;)) %&gt;% as.numeric()) ## Warning: attributes are not identical across measure variables; ## they will be dropped 8.2.1 A plot pastalong %&gt;% filter(!is.na(StationName )) %&gt;% mutate(question2 = question %&gt;% substr(1,34)) %&gt;% # The label is to long, so lets just represent the first 30 letters. ggplot(data = ., aes(answnum,Consumption, color = factor(Day))) + geom_point() + stat_smooth(se = F, method = lm) + stat_cor() + facet_grid(question2 ~ StationName) + theme_bw() + theme(legend.position = &#39;bottom&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; This we similary can run as several linear models. tb &lt;- pastalong %&gt;% filter(!is.na(StationName )) %&gt;% group_by(StationName,question,Day) %&gt;% do(lm(data = ., Consumption~answnum) %&gt;% tidy(conf.int = T)) tb %&gt;% filter(term==&#39;answnum&#39;) %&gt;% select(-statistic) %&gt;% kable(x = .,caption = &#39;All linear models&#39;, digits = 2) Table 8.1: All linear models StationName question Day term estimate std.error p.value conf.low conf.high Pasta with legumes I_like_taste_of_pasta_with_legumes 1 answnum 8.51 10.65 0.44 -14.49 31.51 Pasta with legumes I_like_taste_of_pasta_with_legumes 2 answnum -4.24 13.27 0.75 -32.91 24.42 Pasta with legumes I_like_taste_of_pasta_with_mushrooms 1 answnum 1.93 9.47 0.84 -18.53 22.39 Pasta with legumes I_like_taste_of_pasta_with_mushrooms 2 answnum 15.10 11.16 0.20 -9.00 39.21 Pasta with legumes Pasta_with_legumes_is_visually_appealing 1 answnum 23.36 7.75 0.01 6.61 40.10 Pasta with legumes Pasta_with_legumes_is_visually_appealing 2 answnum 8.50 16.06 0.61 -26.19 43.19 Pasta with legumes Pasta_with_mushrooms_is_visually_appealing 1 answnum 11.75 17.95 0.52 -27.04 50.54 Pasta with legumes Pasta_with_mushrooms_is_visually_appealing 2 answnum 13.17 12.21 0.30 -13.21 39.55 Pasta with mushroom I_like_taste_of_pasta_with_legumes 1 answnum -10.34 11.56 0.39 -35.32 14.63 Pasta with mushroom I_like_taste_of_pasta_with_legumes 2 answnum 4.33 9.07 0.64 -15.27 23.93 Pasta with mushroom I_like_taste_of_pasta_with_mushrooms 1 answnum 16.00 9.36 0.11 -4.22 36.23 Pasta with mushroom I_like_taste_of_pasta_with_mushrooms 2 answnum 3.75 8.12 0.65 -13.80 21.29 Pasta with mushroom Pasta_with_legumes_is_visually_appealing 1 answnum 8.89 10.75 0.42 -14.35 32.12 Pasta with mushroom Pasta_with_legumes_is_visually_appealing 2 answnum -13.86 10.47 0.21 -36.48 8.75 Pasta with mushroom Pasta_with_mushrooms_is_visually_appealing 1 answnum 16.81 19.38 0.40 -25.05 58.67 Pasta with mushroom Pasta_with_mushrooms_is_visually_appealing 2 answnum -10.72 8.24 0.22 -28.52 7.08 .. A plot of these results for a quick interpretation. tb %&gt;% filter(term==&#39;answnum&#39;) %&gt;% ggplot(data = ., aes(question,estimate,ymin = conf.low, ymax = conf.high, color = factor(Day))) + geom_errorbar(width = 0.1, position = position_dodge()) +geom_point()+ geom_hline(yintercept = 0) + coord_flip() +facet_grid(~StationName) + theme(legend.position = &#39;bottom&#39;) Seems as some of the legume consumptions there is a significant association with likert scales. Not as strong for consumption of mushrooms. "],["mixed-models.html", "Chapter 9 Mixed models 9.1 With several variables", " Chapter 9 Mixed models Mixed models are used when there is repetitions in the response due to (here) the person conducting the trial. The two days are repetitions, and hence we can use all the data (not splitting in to days), but need to account for the person in the model. # subset the data x &lt;- pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) mdl &lt;- lmer(data = x, Consumption~I_like_taste_of_pasta_with_mushrooms + Day + (1|Person)) summary(mdl) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Consumption ~ I_like_taste_of_pasta_with_mushrooms + Day + (1 | Person) ## Data: x ## ## REML criterion at convergence: 309 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.1229 -0.5717 -0.1507 0.4085 2.0757 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Person (Intercept) 1561 39.51 ## Residual 4794 69.24 ## Number of obs: 30, groups: Person, 15 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 112.594 56.211 2.003 ## I_like_taste_of_pasta_with_mushroomsAgree -1.222 47.725 -0.026 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 48.220 46.189 1.044 ## Day -2.837 25.380 -0.112 ## ## Correlation of Fixed Effects: ## (Intr) I_____ I____a ## I_lk_t____A -0.595 ## I_lk_____Sa -0.662 0.748 ## Day -0.678 -0.035 0.028 This is the joint effect between the two days. Think of an average of the two slopes - one for each day -. Here taking into account that each person has provided two responses of the consumption of pasta with mushrooms. This can also be accomplised using the tidyverse setup engined by the broom.mixed package. In principle, we simply do not loop over Day, but include it in the formula along with person. tbmixed &lt;- pastalong %&gt;% filter(!is.na(StationName )) %&gt;% group_by(StationName,question) %&gt;% do(lmer(data = ., Consumption~answnum + Day + (1|Person)) %&gt;% tidy(conf.int = T)) The output here is a bit different than the lm() model. But it is still the slope of answnum which carries the interesting stuff. tbmixed %&gt;% filter(term==&#39;answnum&#39;) %&gt;% select(-effect,-group) %&gt;% kable(x = .,caption = &#39;All mixed linear models&#39;, digits = 2) Table 9.1: All mixed linear models StationName question term estimate std.error statistic conf.low conf.high Pasta with legumes I_like_taste_of_pasta_with_legumes answnum 1.71 7.86 0.22 -13.71 17.12 Pasta with legumes I_like_taste_of_pasta_with_mushrooms answnum 6.12 7.65 0.80 -8.87 21.11 Pasta with legumes Pasta_with_legumes_is_visually_appealing answnum 14.16 8.17 1.73 -1.86 30.18 Pasta with legumes Pasta_with_mushrooms_is_visually_appealing answnum 6.23 8.83 0.71 -11.08 23.53 Pasta with mushroom I_like_taste_of_pasta_with_legumes answnum -2.12 7.36 -0.29 -16.56 12.31 Pasta with mushroom I_like_taste_of_pasta_with_mushrooms answnum 10.25 6.53 1.57 -2.55 23.05 Pasta with mushroom Pasta_with_legumes_is_visually_appealing answnum -4.21 7.87 -0.54 -19.63 11.20 Pasta with mushroom Pasta_with_mushrooms_is_visually_appealing answnum -4.33 8.38 -0.52 -20.76 12.10 tbmixed %&gt;% filter(term==&#39;answnum&#39;) %&gt;% ggplot(data = ., aes(question,estimate,ymin = conf.low, ymax = conf.high)) + geom_errorbar(width = 0.1) +geom_point()+ geom_hline(yintercept = 0) + coord_flip() +facet_grid(~StationName) + theme(legend.position = &#39;bottom&#39;) Do the associations match as expected? 9.1 With several variables We can add several predictors to the model, here that could several likert-scale questions, and maybe demographics with the consumption as response. This is in principle the same for both linear models and linear mixed models. x &lt;- pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) mdl &lt;- lmer(data = x, Consumption~I_like_taste_of_pasta_with_mushrooms + Pasta_with_mushrooms_is_visually_appealing + Day + (1|Person)) summary(mdl) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Consumption ~ I_like_taste_of_pasta_with_mushrooms + Pasta_with_mushrooms_is_visually_appealing + ## Day + (1 | Person) ## Data: x ## ## REML criterion at convergence: 278.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.0180 -0.6058 -0.2061 0.3717 1.9421 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Person (Intercept) 1602 40.03 ## Residual 5399 73.48 ## Number of obs: 30, groups: Person, 15 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 106.913 86.627 1.234 ## I_like_taste_of_pasta_with_mushroomsAgree 9.942 59.439 0.167 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 62.756 70.044 0.896 ## Pasta_with_mushrooms_is_visually_appealingMore or less agree 26.432 74.905 0.353 ## Pasta_with_mushrooms_is_visually_appealingAgree 23.132 85.788 0.270 ## Pasta_with_mushrooms_is_visually_appealingStrongly agree -1.810 79.813 -0.023 ## Day -12.589 32.740 -0.385 ## ## Correlation of Fixed Effects: ## (Intr) I_____ I____a P__ola P_____ P____a ## I_lk_t____A -0.325 ## I_lk_____Sa -0.307 0.824 ## Ps_____Mola -0.659 0.001 -0.005 ## Pst_wt____A -0.331 -0.313 -0.452 0.682 ## Pst_w____Sa -0.531 -0.365 -0.486 0.726 0.849 ## Day -0.556 -0.032 0.044 -0.008 -0.247 0.061 mdl %&gt;% tidy(conf.int = T) ## # A tibble: 9 × 8 ## effect group term estim…¹ std.e…² stati…³ conf.…⁴ conf.…⁵ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fixed &lt;NA&gt; (Intercept) 107. 86.6 1.23 -62.9 277. ## 2 fixed &lt;NA&gt; I_like_taste_of_pasta_with_mushroomsAgree 9.94 59.4 0.167 -107. 126. ## 3 fixed &lt;NA&gt; I_like_taste_of_pasta_with_mushroomsStrongly ag… 62.8 70.0 0.896 -74.5 200. ## 4 fixed &lt;NA&gt; Pasta_with_mushrooms_is_visually_appealingMore … 26.4 74.9 0.353 -120. 173. ## 5 fixed &lt;NA&gt; Pasta_with_mushrooms_is_visually_appealingAgree 23.1 85.8 0.270 -145. 191. ## 6 fixed &lt;NA&gt; Pasta_with_mushrooms_is_visually_appealingStron… -1.81 79.8 -0.0227 -158. 155. ## 7 fixed &lt;NA&gt; Day -12.6 32.7 -0.385 -76.8 51.6 ## 8 ran_pars Person sd__(Intercept) 40.0 NA NA NA NA ## 9 ran_pars Residual sd__Observation 73.5 NA NA NA NA ## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic, ⁴​conf.low, ⁵​conf.high Try to interpret the slopes? Are the slopes significantly different from 0 (i.e. the point of no association). .. And hey! Why is the slope for visual all of a sudden negative?… Does that mean that consumption increase the less you like the visual appearance? .. Or what? Complete the same analysis with legumes. "],["latent-factor-models.html", "Chapter 10 Latent Factor Models", " Chapter 10 Latent Factor Models There are many…. "],["check-all-that-applies-cata.html", "Chapter 11 Check All That Applies (CATA) 11.1 An example from Beer profiling 11.2 Two versions of the data 11.3 PCA 11.4 Cochranes Q-test 11.5 Exerecise", " Chapter 11 Check All That Applies (CATA) Check All That Apply (CATA) data is in its raw form binary indicating whether a judge finds a product to have the attribute (1) or not (0). Usually, such data is organized in a matrix where each row corresponds to the evaluation of one product by one judge. And the coloumns are then the attributes. Say you for instance have 26 judges/consummers and 4 products, and further that all products are evaluated by all judges once on 13 attributes. Your data matrix would then have 104 rows and 13 coloums (with responses) and additionally coloumns indicating judge, product, record id, date, etc. 11.1 An example from Beer profiling Six different commercial beers from Danish craft brewers, evaluated by \\(160\\) consumers on a range of different questions: Background information: a range of questions, including appropriateness ratings for 27 sensory descriptors on a 7-points scale (e.g. how appropriate do you think it is for a beer to be bitter?). The two semantic anchors were 1 = not at all appropriate and 7 = extremely appropriate. This dataset is called beercata. Hedonics: Their liking/hedonic responses of the beer on a 7-point Likert scale (1-7). This dataset is called beerliking. library(data4consumerscience) library(tidyverse) data(&quot;beercata&quot;) beercata %&gt;% head() ## Consumer.ID Beer S_Flowers S_Beans S_Intense berries S_Caramel S_Nuts S_Savoury spices ## 1 a01 Wheat IPA 0 0 0 0 1 0 ## 2 a02 Wheat IPA 0 0 0 0 0 0 ## 3 a03 Wheat IPA 0 0 0 0 0 0 ## 4 a04 Wheat IPA 0 1 0 0 0 1 ## 5 a05 Wheat IPA 0 0 0 0 0 0 ## 6 a06 Wheat IPA 0 0 0 0 0 0 ## S_Dessert spices S_Regional spices S_Herbs S_Citrus fruit S_Berries S_Fruit S_Dried fruit S_Liquor ## 1 0 0 1 0 0 0 0 0 ## 2 0 1 0 0 0 0 0 0 ## 3 0 0 0 0 1 0 0 0 ## 4 0 0 0 0 0 0 0 0 ## 5 0 0 0 1 0 0 0 0 ## 6 0 0 1 0 0 0 1 0 ## S_Bitter S_Sparkling S_Refreshing S_Fruity S_Aromatic S_Pungent S_Still S_Smoked S_Foamy S_Sour S_Sweet ## 1 1 0 0 0 1 1 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 1 0 ## 3 1 0 0 0 0 0 0 1 1 0 0 ## 4 1 0 1 0 0 0 0 0 0 0 0 ## 5 1 0 1 0 0 0 0 0 0 0 0 ## 6 1 1 0 0 0 0 0 1 0 0 0 ## S_Warming S_Vinous ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 table(beercata$Beer) ## ## Brown Ale NY Lager Porse Bock Ravnsborg Red River Beer Wheat IPA ## 160 160 160 160 160 160 length(unique(beercata$Consumer.ID)) ## [1] 160 11.2 Two versions of the data RAW data with each row being responses from one evaluation Agglomerated to counts, with each row being one product [Put in the pictures from Rinnan et al 2015] The agglomerated version is computed by: # Questions we want answer using these data # - Are the products different / similar? # - Which attributes drives discrimination? # - Are there any judges who are really of? beercatasum &lt;- beercata %&gt;% gather(attrib, val, S_Flowers:S_Vinous) %&gt;% group_by(Beer,attrib) %&gt;% dplyr::summarize(n = sum(val)) %&gt;% spread(attrib,n) ## `summarise()` has grouped output by &#39;Beer&#39;. You can override using the `.groups` argument. beercatasum ## # A tibble: 6 × 28 ## # Groups: Beer [6] ## Beer S_Aro…¹ S_Beans S_Ber…² S_Bit…³ S_Car…⁴ S_Cit…⁵ S_Des…⁶ S_Dri…⁷ S_Flo…⁸ S_Foamy S_Fruit S_Fru…⁹ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brown Ale 57 54 6 63 52 10 13 21 4 27 11 22 ## 2 NY Lager 37 10 5 69 16 30 20 6 46 27 15 45 ## 3 Porse Bock 20 2 5 67 7 34 16 6 27 26 8 22 ## 4 Ravnsborg… 52 25 6 71 35 11 19 14 17 16 11 26 ## 5 River Beer 22 12 4 65 3 29 8 4 22 30 18 24 ## 6 Wheat IPA 29 9 9 57 9 30 24 6 45 26 21 43 ## # … with 15 more variables: S_Herbs &lt;dbl&gt;, `S_Intense berries` &lt;dbl&gt;, S_Liquor &lt;dbl&gt;, S_Nuts &lt;dbl&gt;, ## # S_Pungent &lt;dbl&gt;, S_Refreshing &lt;dbl&gt;, `S_Regional spices` &lt;dbl&gt;, `S_Savoury spices` &lt;dbl&gt;, ## # S_Smoked &lt;dbl&gt;, S_Sour &lt;dbl&gt;, S_Sparkling &lt;dbl&gt;, S_Still &lt;dbl&gt;, S_Sweet &lt;dbl&gt;, S_Vinous &lt;dbl&gt;, ## # S_Warming &lt;dbl&gt;, and abbreviated variable names ¹​S_Aromatic, ²​S_Berries, ³​S_Bitter, ⁴​S_Caramel, ## # ⁵​`S_Citrus fruit`, ⁶​`S_Dessert spices`, ⁷​`S_Dried fruit`, ⁸​S_Flowers, ⁹​S_Fruity … and visualized by for instance a barplot. # summary counts over attrobite beercatasum %&gt;% gather(attrib, n, S_Flowers:S_Vinous) %&gt;% ggplot(data = ., aes(attrib,n, fill = Beer)) + geom_bar(stat = &#39;identity&#39;, position = position_dodge()) + coord_flip() 11.3 PCA A PCA on the agglomerated counts, reveal the attributes associated with the individual products: mdlPCA &lt;- prcomp(beercatasum[,-1], scale. = T) ggbiplot::ggbiplot(mdlPCA, labels = beercatasum$Beer) The attributes Bean, Caramel, Warming, Aromatic etc is associated to the beer Brown ale, while Berrie, Dessert, Pungent, etc. is characteristic of Wheat IPA [Maybe add some more narrative] 11.4 Cochranes Q-test Cochranes Q-test is a statistical test for the comparison of several products, where the response is binary, and there is repeated responses across several judges. We need a package (RVAideeeeMemoire). For one response variable: S_Flowers library(RVAideMemoire) ## *** Package RVAideMemoire v 0.9-81-2 *** ## ## Attaching package: &#39;RVAideMemoire&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## dummy ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap m &lt;- cochran.qtest(S_Flowers ~ Beer | Consumer.ID, data = beercata) m ## ## Cochran&#39;s Q test ## ## data: S_Flowers by Beer, block = Consumer.ID ## Q = 63.252, df = 5, p-value = 2.581e-12 ## alternative hypothesis: true difference in probabilities is not equal to 0 ## sample estimates: ## proba in group &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 0.02500 0.28750 0.16875 0.10625 0.13750 0.28125 The p.value is strongly significant, indicating that we cannot assumme the same level of S_Flower in all beers. I.e. the beers seems different based on this characteristics. This is in agreement with the barplot above, where S_Flower is high in NY Lager and really low for Brown ale. 11.4.1 Post hoc contrasts As we observe differences based on this attribute, we pursue the question on which products sticks out? And are there products which are similar? This is done by pairwise comparisons: library(rcompanion) PT = pairwiseMcnemar(S_Flowers ~ Beer | Consumer.ID, data = beercata, test = &quot;permutation&quot;, method = &quot;fdr&quot;, digits = 3) PT$Pairwise %&gt;% arrange(-abs(as.numeric(Z))) %&gt;% data.frame() ## Comparison Z p.value p.adjust ## 1 Brown Ale - NY Lager = 0 -6.48 9.13e-11 1.37e-09 ## 2 Brown Ale - Wheat IPA = 0 -5.86 4.71e-09 3.53e-08 ## 3 Brown Ale - Porse Bock = 0 -4.27 1.95e-05 9.75e-05 ## 4 NY Lager - Ravnsborg Red = 0 4.14 3.43e-05 1.29e-04 ## 5 Ravnsborg Red - Wheat IPA = 0 -3.96 7.5e-05 2.25e-04 ## 6 NY Lager - River Beer = 0 3.54 0.000402 8.48e-04 ## 7 Brown Ale - River Beer = 0 -3.53 0.000415 8.48e-04 ## 8 River Beer - Wheat IPA = 0 -3.51 0.000452 8.48e-04 ## 9 Brown Ale - Ravnsborg Red = 0 -2.98 0.00286 4.77e-03 ## 10 NY Lager - Porse Bock = 0 2.52 0.0118 1.77e-02 ## 11 Porse Bock - Wheat IPA = 0 -2.36 0.0181 2.47e-02 ## 12 Porse Bock - Ravnsborg Red = 0 1.54 0.123 1.54e-01 ## 13 Porse Bock - River Beer = 0 0.845 0.398 4.26e-01 ## 14 Ravnsborg Red - River Beer = 0 -0.845 0.398 4.26e-01 ## 15 NY Lager - Wheat IPA = 0 0.135 0.893 8.93e-01 The table is sorted with the most different pairs at the top, and the least different at the bottom. Hence most products are different, while Porse Bock and Ravnsborg Red are fairly alike. 11.4.2 For all Attributes We use tidyverse and broom for this, but need a function capable of handling Cochranes Q-test outputs. library(broom) tidy.RVtest &lt;- function(m){ r &lt;- data.frame(statistic = m$statistic,df = m$parameter, p.value= m$p.value, method = m$method.test) return(r) } tb_cochran &lt;- beercata %&gt;% gather(attrib, val, S_Flowers:S_Vinous) %&gt;% group_by(attrib) %&gt;% do(cochran.qtest(val ~ Beer | Consumer.ID, data = .) %&gt;% tidy) tb_cochran %&gt;% arrange(p.value) ## # A tibble: 27 × 5 ## # Groups: attrib [27] ## attrib statistic df p.value method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 S_Beans 114. 5 7.19e-23 Cochran&#39;s Q test ## 2 S_Caramel 108. 5 1.08e-21 Cochran&#39;s Q test ## 3 S_Flowers 63.3 5 2.58e-12 Cochran&#39;s Q test ## 4 S_Aromatic 45.8 5 9.90e- 9 Cochran&#39;s Q test ## 5 S_Sweet 36.3 5 8.24e- 7 Cochran&#39;s Q test ## 6 S_Warming 35.9 5 1.01e- 6 Cochran&#39;s Q test ## 7 S_Smoked 33.9 5 2.47e- 6 Cochran&#39;s Q test ## 8 S_Liquor 33.9 5 2.55e- 6 Cochran&#39;s Q test ## 9 S_Citrus fruit 29.4 5 1.96e- 5 Cochran&#39;s Q test ## 10 S_Dried fruit 26.0 5 8.81e- 5 Cochran&#39;s Q test ## # … with 17 more rows This output indicates that S_Beans is the most discriminatory attribute, while S_Pungent is the least. 11.4.3 PLSDA This needs more love. library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift mdl &lt;- plsda(data.frame(beercata[,3:29]),factor(beercata$Beer),ncomp = 3) scores &lt;- mdl$scores %&gt;% unclass %&gt;% as.data.frame %&gt;% cbind(beercata) loadings &lt;- mdl$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% mutate(attrib2 = substr(attrib,3,50)) # lets remove the S_ g1 &lt;- ggplot(data = loadings, aes(`Comp 1`, `Comp 2`, label = attrib2)) + # geom_point() + geom_text() g2 &lt;- ggplot(data = scores, aes(`Comp 1`, `Comp 2`, color = Beer)) + # geom_point() + stat_ellipse(level = 0.5) library(patchwork) g1 + g2 # do multiple splithalfs # INPUT: judge id. CATA, class, ncomp X &lt;- beercata[,3:29] clss &lt;- factor(beercata$Beer) judge &lt;- beercata$Consumer.ID k &lt;- 3 A &lt;- 30 mdl0 &lt;- plsda(X,clss,ncomp = k) lds0 &lt;- mdl0$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% gather(cmp,val0,-attrib) unjudge &lt;- unique(judge) nindiv &lt;- length(unjudge) LOADS &lt;- data.frame() for (i in 1:A){ ic &lt;- judge %in% sample(unjudge)[1:round(nindiv/2)] mdlSH &lt;- plsda(X[ic,],clss[ic],ncomp = k) df_flip &lt;- data.frame(sng = sign(diag(t(mdl0$loadings) %*% mdlSH$loadings))) %&gt;% rownames_to_column(&#39;cmp&#39;) lds &lt;- mdlSH$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% gather(cmp,val,-attrib) %&gt;% left_join(df_flip, by = &#39;cmp&#39;) %&gt;% mutate(SHiter = i, val = val*sng) LOADS &lt;- bind_rows(LOADS,lds) } fc &lt;- (1 / A)*((A - 1)/A) sdloads &lt;- LOADS %&gt;% left_join(lds0, by = c(&#39;attrib&#39;,&#39;cmp&#39;)) %&gt;% group_by(attrib,cmp) %&gt;% dplyr::summarise(sd = sum((val-val0)^2) *fc) %&gt;% mutate(cmp = paste(&#39;sd&#39;,cmp,sep = &#39;&#39;)) %&gt;% spread(cmp,sd) ## `summarise()` has grouped output by &#39;attrib&#39;. You can override using the `.groups` argument. loadsSH &lt;- lds0 %&gt;% spread(cmp,val0) %&gt;% left_join(sdloads, by = &#39;attrib&#39;) library(ggforce) ggplot(data = loadsSH, aes(x0 = `Comp 1`,y0 = `Comp 2`,a = `sdComp 1`,b = `sdComp 2`,angle = 0)) + geom_ellipse() 11.5 Exerecise Take 5-10 minutes to look at the publication to get an overview [hvilken publ?] 11.5.1 Exercise 1: PCA on consumer background From this exercise you should be able to describe who your consumers are. Make the data available: data(&quot;beerdemo&quot;) Calculate a PCA model including the Variables 7 ( Interest in food ) to 39 ( App_Vinous ). Remember to standardize/scale the variables mdlPCA &lt;- prcomp(beerdemo[,7:39],scale. = T) Plot the scores and loadings in a biplot and look for groupings of the consumers in the scores. Group and color according to the background information not used in the model (Gender, Age,..) library(ggbiplot) ggbiplot(mdlPCA, groups = beerdemo$Gender, ellipse = T) Describe what you find. 11.5.2 Exercise 2: PCA on CATA counts From this exercise you should be able to describe your samples (beers) from the CATA counts. Collated (summed) for each beer of CATA score from all consumers. Setup the collated version as described above. beercatasum &lt;- beercata %&gt;% gather(attrib, ... Calculate a PCA model including all Variables and all Objects. PCAmdl &lt;- prcomp(beercatasum, scale. = T) Plot the scores and describe the groupings of the samples. Plot the loadings and describe the correlations between the variables. ggbiplot(PCAmdl) Use this biplot to find out which samples are described by which words. 11.5.3 Exercise 3: PCA on liking From this exercise you should be able to describe the liking of the beer samples and see how the consumers do this. Calculate a PCA model including all Variables and all Objects. include_these &lt;- complete.cases(beerliking) PCAliking &lt;- prcomp(beerliking[include_these,-1], scale. = T) Plot a biplot or loading plot, and use the loadings and describe the correlations between the variables (liking of beers in this case). ggbiplot(PCAliking) Plot the scores and describe the groupings of the samples by colouring the score plot according to the consumer background variables. Note that the 160 rows in both datasets match each-other, so we can glue the demo information directly onto the liking model. If that was not the case, matching using left_join() or inner_join() would be nessesary before analysis. ggbiplot(PCAliking,groups = beerdemo$Age[include_these], ellipse = T) Any trends? For instance, how is liking related to the individual consumer diversity of beer (Beer types/month)? … Some code to get all 7-scale demo information plots. You may want to export and view in a pdf viewer for zooming etc. gall &lt;- cbind(PCAliking$x[,1:2], beerdemo[include_these,]) %&gt;% gather(var,val,`Interest in food`:App_Vinous) %&gt;% ggplot(data = ., aes(PC1,PC2, color = factor(val))) + geom_point() + stat_ellipse() + facet_wrap(~var) ggsave(filename = &#39;anicebigfigure.pdf&#39;,gall, height = 20, width = 20) 11.5.4 Exercise 4: PLS on CATA counts and liking From this exercise you should be able to conclude what drives the liking of your samples (beers). For each beer, the collated CATA counts is the predictors, and the averaged liking is the response. likingsum &lt;- beerliking %&gt;% gather(Beer, liking, -`Consumer ID`) %&gt;% group_by(Beer) %&gt;% dplyr::summarise(lik = mean(liking, na.rm = T)) Check that the rows are ordered in the same way: likingsum$beer ## Warning: Unknown or uninitialised column: `beer`. ## NULL beercatasum$Beer ## [1] &quot;Brown Ale&quot; &quot;NY Lager&quot; &quot;Porse Bock&quot; &quot;Ravnsborg Red&quot; &quot;River Beer&quot; &quot;Wheat IPA&quot; CATAlik &lt;- list() CATAlik$CATA &lt;- scale(as.matrix(beercatasum[,-1])) CATAlik$lik &lt;- scale(likingsum$lik) rownames(CATAlik$lik) &lt;- rownames(CATAlik$CATA) &lt;- beercatasum$Beer Calculate a PLS model where CATA features are predictors and liking is response for all Objects. library(pls) ## ## Attaching package: &#39;pls&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## R2 ## The following object is masked from &#39;package:stats&#39;: ## ## loadings catalik.pls &lt;- plsr(lik ~ CATA, ncomp = 2, data = CATAlik, validation = &quot;LOO&quot;) corrplot(catalik.pls, labels = colnames(beercatasum)[-1]) biplot(catalik.pls) Plot the loadings and study which X variables are important for the liking score. Advanced: Plot the Regression coefficients (scaled) and try to interpret the meaning of this plot (Hint: use your findings from the loadings plot). 11.5.5 Exercise 5: Mixed modelling on the liking Dataset: Beer_XYZmatrix.xlsx, sheet “Z and Y liking” Import the datasheet in to R Studio. Check to see if all variables have the correct description/denomination (factor, numerical etc.) Are there any significant product differences for the liking? If so, what does the Tukey tell us? How does this fit with what you have done in the PCA/PLS exercises. Is the liking in general affected by the age, gender, household size or beer knowledge? What is the effect? Try to think of a plot that can show the significant differences. Do men and women score the samples significantly different in liking? Calculate the sample/gender differences in averages, try to use Pivot Tables in Excel. 11.5.6 Exercise 6: Comparing CATA binary data and counts Dataset: Beer_XYZmatrix.xlsx, sheets “X CATA collated, counts” and “Z + Y + X unfolded” If time… Calculate two PCA models: one on the X unfolded matrix (CATA answers in binary codes, more columns and you just choose the ones you need) and one on the CATA counts. Compare the outcome of the two models. Evaluate explained variance Evaluate loadings plots Is this expected when looking at counts and “raw” data What type of information is lost by looking at the CATA counts? 11.5.7 Exercise 7: Cochran’s Q test on CATA binary data Dataset: Beer_XYZmatrix.xlsx, sheet “X unfolded” If time… Import the datasheet in to R Studio using the CSV format (save the file as CSV in Excel). Choose 4 relevant CATA attributes (based on your previous results today) to make a Cochran’s Q test for, comment on the results (i.e. the sample differences). "],["projective-mapping.html", "Chapter 12 Projective mapping 12.1 Example from mapping of XX 12.2 A Collated version of the data 12.3 PCA on Collated data 12.4 ", " Chapter 12 Projective mapping [An image of a PM] 12.1 Example from mapping of XX library(data4consumerscience) data(&quot;tempetotemperature&quot;) tempetotemperature ## Productname Product Sousvidetemperature_C Sousvidetime_days id Assessor X1 Y1 X2 Y2 X3 Y3 X4 ## 1 45C-2D Sample 1 45 2 903 1 32.2 34.9 0.0 0.0 0 0 0 ## 2 45C-3D Sample 2 45 3 714 1 15.1 23.2 0.0 0.0 0 0 0 ## 3 45C-4D Sample 3 45 4 31 1 6.6 34.6 0.0 0.0 0 0 0 ## 4 50C-2D Sample 4 50 2 487 1 30.1 16.5 0.0 0.0 0 0 0 ## 5 50C-3D Sample 5 50 3 133 1 42.8 20.1 0.0 0.0 0 0 0 ## 6 50C-4D Sample 6 50 4 827 1 38.6 8.0 0.0 0.0 0 0 0 ## 7 55C-2D Sample 7 55 2 538 1 3.8 17.1 0.0 0.0 0 0 0 ## 8 55C-3D Sample 8 55 3 215 1 44.8 12.4 0.0 0.0 0 0 0 ## 9 55C-4D Sample 9 55 4 341 1 45.4 4.2 0.0 0.0 0 0 0 ## 10 45C-2D Sample 1 45 2 903 2 0.0 0.0 41.8 20.0 0 0 0 ## 11 45C-3D Sample 2 45 3 714 2 0.0 0.0 42.5 36.6 0 0 0 ## 12 45C-4D Sample 3 45 4 31 2 0.0 0.0 23.3 25.1 0 0 0 ## 13 50C-2D Sample 4 50 2 487 2 0.0 0.0 32.6 23.7 0 0 0 ## 14 50C-3D Sample 5 50 3 133 2 0.0 0.0 26.5 30.3 0 0 0 ## Y4 X5 Y5 X6 Y6 X7 Y7 X8 Y8 X9 Y9 X10 Y10 X11 Y11 Miso Soft Bitter Sweet Sour Mild Caramel Umami Deep ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 ## 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 ## 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 ## 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 ## Balanced Nutty Hard Dry Roasted Strong Fish sauce Dark Chocolate Moisty Brown Dried fruits Beany Smooth ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 7 0 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 1 0 0 1 1 1 0 0 0 0 0 ## 9 0 0 0 1 1 1 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## 12 0 0 0 0 0 0 0 1 0 1 0 0 0 0 ## 13 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## Dense Alcoholic Firm Grainy Soy sauce Chewy Yeasty Fruity Wine Gummy Malty Liquorise Bread Salty Meaty ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Creamy Stout ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## 7 0 0 ## 8 0 0 ## 9 0 0 ## 10 0 0 ## 11 0 0 ## 12 0 0 ## 13 0 0 ## 14 0 0 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 85 rows ] table(tempetotemperature$Productname) ## ## 45C-2D 45C-3D 45C-4D 50C-2D 50C-3D 50C-4D 55C-2D 55C-3D 55C-4D ## 11 11 11 11 11 11 11 11 11 table(tempetotemperature$Assessor) ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 9 9 9 9 9 9 9 9 9 9 9 This dataset consists of 9 products evaluateed by 11 judges. The responses is the 2D coordinates of the procetive mapping, for each judge individually (X1, Y1, X2,…, Y11), and CATA data on 30 attribues (Miso, Soft, Bitter,…, Stout). 12.2 A Collated version of the data 12.3 PCA on Collated data downweeeigting of attributes. On everything with normal scaling. Include judge-loadings in the output plots. 12.4 "],["preference-mapping.html", "Chapter 13 Preference Mapping 13.1 Example of Preference Mapping 13.2 PCA 13.3 Analysis by PLS 13.4 L-PLS [For the future…]", " Chapter 13 Preference Mapping [some narrative] 13.1 Example of Preference Mapping [Which???] 13.2 PCA PCA is nice…. [on beerliking] library(data4consumerscience) data(&quot;beerliking&quot;) mdlPCA &lt;- prcomp(beerliking[complete.cases(beerliking),-1]) ggbiplot::ggbiplot(mdlPCA) Those who like Ravnsborg red also likes NY Lager and to some extend Brown ale…. [some more narrative] 13.3 Analysis by PLS Predictors can be objective characteristics of the products or CATA type data, while response is hedonik liking data. [minimum 5 samples X = CATA (Beer_XYZmatrix.xlsx, sheet = X CATA (coll.)+Y liking (aver.)), Y = Living average Y2 = Liking for each consumer = t(Y (long thin)) Objective = Visualize to get patterns related to liking, and which deescriptors are merely irrelevant. 13.4 L-PLS [For the future…] "],["lpls.html", "Chapter 14 LPLS", " Chapter 14 LPLS L-PLS - include reference. "],["consumer-segmentation.html", "Chapter 15 Consumer Segmentation 15.1 Kmeans", " Chapter 15 Consumer Segmentation 15.1 Kmeans "],["more-pca.html", "Chapter 16 More PCA", " Chapter 16 More PCA "],["logistic-regression.html", "Chapter 17 Logistic Regression", " Chapter 17 Logistic Regression "],["confirmatory-factor-analysis-using-lavaan.html", "Chapter 18 Confirmatory Factor Analysis using lavaan 18.1 Example - Food Neophobia", " Chapter 18 Confirmatory Factor Analysis using lavaan 18.1 Example - Food Neophobia "],["structured-equation-modelling.html", "Chapter 19 Structured Equation Modelling 19.1 Example - Theory of Planned Behaviour", " Chapter 19 Structured Equation Modelling 19.1 Example - Theory of Planned Behaviour 2+2 ## [1] 4 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
