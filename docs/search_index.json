[["index.html", "Data Analysis in R for Consumer Science Chapter 1 Introduction to the book", " Data Analysis in R for Consumer Science Morten Arendt Rasmussen 2022-10-05 Chapter 1 Introduction to the book This material is to cover data analysis using R for consumer science targeting the following courses: Meal Systems and Technologies Food Consumer Research Meal Consumer Research Thematic Course in Food Innovation and Health Public Health Nutrition But others may benefit from the material as well…. The initial chapters of the book introduce R and the background for the different statistical methods used in the book. The following parts of the book is divided according to the course hence the data types encountered in the above mentioned courses, The format of the book is simple - first read the written text and when you reach a code, copy the code and try it out yourself. "],["introduction-to-r.html", "Chapter 2 Introduction to R 2.1 How to get started - understanding R (and RStudio) 2.2 How to import data 2.3 Organise and save scripts 2.4 How to save the data 2.5 How to export data / results 2.6 Ready for analysis 2.7 How to merge two datasets", " Chapter 2 Introduction to R R is a free software with a complete programming language for statistical computing and graphics. It is used at many universities and companies since it is always updated and open source. Before staring your calculations in R you should always update the R version on your computer. You can install a graphical interface for R, called R studio. It will use the underlying version of R on your computer – so you have to have R installed too. The principles by R and R Studio are the same – BUT R Studio has a better interface for non-programmers. Both R and R Studio can be used on all types of computers. R is command-line based and it provides a wide variety of statistical methods (linear and nonlinear modelling, classical statistical tests, classification, clustering, …). Advanced methods are available via extension packages (more than 10.000 at the moment) Always be sure to have the latest version of both R and R Studio on your computer - update version just before you need to use the program. [INDSÆTTE: Bodil der snakker om opsætningen af R Studio. Forslag er slide 4-5-6 i Bodils R intro præsentation] 2.1 How to get started - understanding R (and RStudio) To get started go to upper left corner and open a new script. Remember to save your script as well. To out our codes: 1+2 ## [1] 3 a &lt;- 2+2 b &lt;- 5+3 What happens if you write the letter a in the editor and run it? What about the letter b? a ## [1] 4 b ## [1] 8 a+b ## [1] 12 … or this one? A+B R is essentially (also a) calculator, but case sensitive. In the Editor: “#” is the start of a comment (means: will not evaluated/ read by the program). This is how you can make comments in your script: # I want to add 2 and 5 2+5 ## [1] 7 # whoop it is 7! “:”Generates a sequence (e.g. 1:10 is the numbers from 1 to 10) from1to10 &lt;- 1:10 from10to1 &lt;- 10:1 from1to10 ## [1] 1 2 3 4 5 6 7 8 9 10 from10to1 ## [1] 10 9 8 7 6 5 4 3 2 1 In the Console: “&gt;” indicates that R is ready for a new code. “+” Instead of “&gt;” means that the program is waiting for you. (you probably made a mistake in the script you tried to run) – by [ESC] the “+” turns to a “&gt;” again “NA” (Not Available) is indicating a missing value “NaN” (Not a Number) is the result of an ‘illegal’ operation e.g. log(-1) Red sentences means there is an error. R will stop calculating at the first error it meets. 2.2 How to import data 2.2.1 Import data from R-package In this book several datasets are used targeting different research questions. However, a fair part of the analysis tools are common. That is, descriptive analysis, plots, response correlations etc. The data is included in the R-packgage data4consumerscience you get by running the code below. Be aware that you need devtools package to install packages from github, so you need to run both code lines. # install data-package install.packages(&#39;devtools&#39;) devtools::install_github(&#39;mortenarendt/data4consumerscience&#39;) The data is also available as excel sheets, and can be loaded using packages capable of reading from Excel. Before you start you data import, you have to make sure the data set contains all the information you need and the format of the data (columns and rows) is correct. You can import in many different ways. This book will show you two: 2.2.2 Importing a csv file If the data is not already an csv file, but an excel file, you need to convert it: Open your Excel file, as it is in xls or xlsx format. Convert this file to csv format. NB: Some data collection tools will provide you with your data in csv and some xlsx/xls format. In Excel, you choose the “save as” and then choose *.csv. Then move in to R, and write: DATASET1 &lt;- read.csv2(file.choose()) The file.choose() function makes you point towards the file you want. You can also simply write the path to the file directly. Actually, by using the file.choose() the first time you import data will prombt the path, and you can simply copy paste this from the console to your script avoiding point and click every time you want to analyse these data. DATASET1 &lt;- read.csv2(&#39;~/path/to/the/data/myfile.csv&#39;) You decide the names/titles of your datasets and models, just do not use other signs than “.” and avoid non-English letters. We called it “DATASET1”. R will open a new window (sometimes hidden behind your other open windows), open the window to choose the wanted csv file. The data set will now also appear in the upper right corner as a line. If you double click a data set in this box, it will open in the editor window You can import any *.csv format dataset, when you try it out. Trouble shooting: * Try new csv format in Excel when saving the file in csv format * Try to write read.csv(file.choose()) instead * Try another import function (see below) 2.2.3 Importing an Excel file/sheet If you have data as excel, you may utilize packages for directly importing, without the need to convert to csv. If your Excel file contains more than one sheet, you have to import each sheet separately. Here we use the package readxl with the function read_excel. If the data is not in the same folder as your script, then include the path to the data, or move the data to the script’s location. When you have to find the path for the file on your computer, you place your cursor within the ’’ in the command and click the tabulator button. Your computer files will appear, and you can find the path for your file easily. If you cannot find the path, try to use the file.choose() command to find the file, and then copy paste the path from the Console (where you find your output). library(readxl) BuffetConsumption &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;BuffetData&#39;) BuffetSurvey &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;BuffetSurvey&#39;) SurveyScale &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;, sheet = &#39;SurveyScale&#39;, col_names = c(&#39;answ&#39;,&#39;answnum&#39;)) [MORTEN: Datasæt navnene skal rettes her - Excelfilen hedder ikke dette mere og vi bruger ikke SurveyScale delen mere] The first part of the model sentence is what we want to call our dataset, here we chose “BuffetConsumption” in the first line (that is the same as the sheet in the Excel file for simplicity). You decide the names/titles of your datasets and model, just do not use other signs than “.” and avoid non-English letters. Try to import both the BuffetConsumption sheet and the BuffetSurvey sheet. BuffetConsumption is consumption data in grams from a buffet. The data is from 16 different persons, who came on Day 1 and Day 2 to eat Pasta with legumes and/or Pasta with mushroom. In the dataset, there is one line per buffet station per participant per experimental day. BuffetSurvey is survey data collected in SurveyXact. The dataset contains data on liking, motivation, choices etc. linked to the particular buffet data. Survey could also contain demographics for the participants such as age, gender, eating habits etc. These are general and different from the former, in that they have nothing to do with the current buffet. This type of data is not included in the SurveyData. The last part of the code imports the scale used in the survey - these will be used later. But have a look at what the col_names = c(‘answ’,‘answnum’) does, and also what happens if you remove it or change it to col_names = TRUE. 2.2.4 Clipboard import Last resort is to import via your clipboard. Go to Excel and mark the data you want to import. Make sure there are headings in the data you have marked. Copy the marked data to the clipboard. Go to the Editor and write the following command line: DATASET2 &lt;- read.table(file=&quot;clipboard&quot;, header=TRUE, sep=&quot;\\t&quot;) Meaning read the table you saved in your clipboard and save it as the name “DATASET2” (remember you choose this name). The data has headers and should be separated in cells. Regardless of importation method – the dataset will appear in the upper right corner environment as a line, please check it looks correct. You can import from any Excel dataset, when you try it out. 2.2.5 Looking at the imported elements Have a look at the imported elements to ensure that indeed, they mimic the Excel sheets. Use the functions head(), str() and View() is your tools. They will give you the headlines in your data, how your variables are categorized and open the dataset in a new tab. head(BuffetConsumption) str(BuffetConsumption) View(BuffetConsumption) Try to use the BuffetConsumption dataset. If it does not look as expected, try to import it again using a different method. 2.2.6 Numbers and factors - changing categorisation During the import R will automatically categorise your variables: if they are read as numbers or letters. For instance, if day of the experiment is called 1 and 2 in the data file and is then read as numeric (num). As Day 2 is not double the value of Day 1, we need to change this variable into a factor (Factor) or character (chr). Use the str() function to check your variables before your change them. You transform your variables using as.numeric() or as.character(). BuffetConsumption$Day&lt;-as.factor(BuffetConsumption$Day) Meaning take the variable Day in the dataset you called BuffetConsumption, make it a factor and put it into the same variable name (overwrites it). If you want to have a new variable coded and then keep the old one, simply just give it a new name, e.g. “DayFactor”. The dataset will then be extended with one variable, but sometimes it is nice to have both versions. BuffetConsumption$DayFactor&lt;-as.factor(BuffetConsumption$Day) 2.3 Organise and save scripts A script is a rundown from A-Z (start to end) of data analysis. A script should be self-contained. I.e. the first lines sets the libraries and imports the data, there after you may want to wrangle the data a bit (changing features as.numeric, as.factor,…, renaming columns, etc.). Thereafter the analysis starts. Think of a script like making a meal You need raw-materials (carrots, onions,…) - That is the data You need a kitchen - That is R as the software You need knifes, pots and pans - That is the packages All is needed to work and hence you need to specify them in the script. In larger projects where the same dataset may be used for several different analysis, it may be wise to have several scripts. One for importing data and modifying it (starts with import and ends with save() as an .RData file). One for descriptive analysis, one for inference, one for plots etc. So you can create a sequence of scripts to keep overview. However, this is only needed for larger projects. In small analysis you can easily include all in one script. ) Remember to put a little narrative (after a “#” at the top off you script explaining the purpose. Remember to save your scripts! 2.4 How to save the data Use save.image() to save everything in the Environment, or use save() to specify which elements to save save.image(file = &#39;iBuffetSurveyDataEverything.RData&#39;) # everything save(file = &#39;iBuffetSurveyData.RData&#39;, list = c(&#39;Survey&#39;,&#39;Surveylong_buffet&#39;, &#39;Surveylong&#39;,&#39;Buffet_survey&#39;,&#39;SurveyScale&#39;)) # just the usual and non-redundant stuff. 2.5 How to export data / results You can export any data frame from R to excel (for instance using the rio package), as well as saving it as .RData for further analysis. This can obviously be used for exporting your data after some modifications. BUT it is also very useful for exporting data frames with results from analysis. When exporting data, it is also important to tell R where to place the exported file. You do this by specifying the path to the desired folder, followed by the name that you choose for the exported file (often it makes sense to choose the same name as the data frame in R). It is also important to specify the file-extension, to ensure that you create the right file type - in this case .xlsx, but rio can also export to other formats such as .txt or .csv. rio::export(Surveylong_buffet,file = &#39;./data/YourFolderForNiceTables/Surveylong_buffet.xlsx&#39;) # export one data frame 2.6 Ready for analysis Once you have saved the data, you can simply load the data directly, and you do not need to do the import-setup every time you want to do an analysis on the data. This part is not a part of the data import, but it is a good idea just to check that the data indeed is setup as expected. load(&#39;iBuffetSurveyData.RData&#39;) 2.7 How to merge two datasets Sometimes you have to merge two data sets. This is needed if you have for instance consumption data in one Excel sheet and survey data in another Excel sheet. Setup the data in Excel such that they match the below in terms of format. What is important is: First row is used on headings and none of these are repeated. I.e. all unique within a sheet Data comes from row 2 and then on to the right All rows should contain data (NB: empty cell is also data, e.g. an unanswered questions), so all empty rows are removed (not cells) Headings between sheets referring to the same: e.g. participant ID should have exactly similar heading If you have calculated stuff within Excel such as a sum of the numbers in a column, then these should be removed from the sheet. It is not data! We suggest that you keep both the original version of the data as a sheet, and the ready-to-import version as a sheet, so you do not accidentally delete data. 2.7.1 Import, edit and merge in R Each of the Excel sheets are imported separately. Here we use the package readxl with the function read_excel. [NB NB NB: Herfra og så til efter koden er 100% magen til tekst og kode for import ovenfor i tidligere afsnit - copy-paste] 2.7.1.1 Import If the data is not in the same folder as your script, then include the path to the data, or move the data to the script’s location. library(readxl) BuffetConsumption &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;BuffetData&#39;) BuffetSurvey &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;,sheet = &#39;BuffetSurvey&#39;) SurveyScale &lt;- read_excel(&#39;./data/iBuffet.xlsx&#39;, sheet = &#39;SurveyScale&#39;, col_names = c(&#39;answ&#39;,&#39;answnum&#39;)) Have a look at the imported elements to ensure that indeed, they mimic the Excel-sheets. head(), str() and View() is your tools. We see that the coloum with names (Person and StationName) is interpreted as characters (chr) while the stuff which should be numbers (Comsuption) is numeric (num). If that is not the case, you will need to transform them using as.numeric() or as.character(). 2.7.1.2 Edit The comsumption data is optimal as is. We have the data as long format with all repsonses in one coloumn and then the next columns clarifying the design, time, type, person etc. However the Survey data is not optimal directly. We need to revert the data to both long and wide format. library(tidyverse) Surveylong &lt;- BuffetSurvey %&gt;% gather(question,answ, `Pasta with legumes is visually appealing to me. `: `I like the taste of pasta with mushrooms! `) %&gt;% mutate(answ = answ %&gt;% factor(levels = Surveyscales$answ), answnum = answ %&gt;% as.numeric()) Surveywide &lt;- Surveylong %&gt;% select(-answ) %&gt;% spread(question,answnum) The code above does exactly that, with Surveylong and Surveywide as the resulting data sets. What is also introduced here is the operator %&gt;%. It originates from the dplyr-package inside the tidyverse-package, and is a handy tool for data manipulation. The way it works is, that whatever is written on the right side of the operator, will be used as the first argument in the function written on the left side of the operator. This means, that x %&gt;% f(y) will result in f(x,y) - or in our case: BuffetSurvey %&gt;% gather(question,answ, Pasta with legumes is visually appealing to me.:I like the taste of pasta with mushrooms!) will be equal to: gather(BuffetSurvey,question,answ, Pasta with legumes is visually appealing to me.: I like the taste of pasta with mushrooms!) The idea is then to “chain” (or “pipe” as it is also known) %&gt;% together line after line, using different functions in a sequence, which makes the code more readable and often shorter as well. gather will lengthen the data, by stacking the columns we specified on top of each other, resulting in each row being one single answer to one of the 4 questions. The answer will be in the answ-column, the question in the question-column, and the rest of the columns can then be used to e.g. group the data. mutate will add a column to the data as specified, in our case it will add the numerical equivalent of the answers to each question. select will (surprise) select the columns specified, in our case all columns BUT the answ-column (hence the “-” in front of answ. spread will do the opposite as gather, and spread one column in several columns depending on what the columns contains. In our case the question-column in spread into 4 columns, one for each question from the survey, with the numerical values of the answers as their values. You might have also noticed, that when a variable name contains a space, R needs help understanding that this is indeed a variable. Different symbols are added, and while you CAN write everything just the way R know how to read it, there is an easier way to make sure that everything is written correctly. You can call the variables from the data frame that they originate from, using “dataframe$” and then hit TAB. A list of the variables of which the data frame consist will appear, and from this you can choose the right one - always spelled correctly, and the way R knows how the interpret it. [JULIUS: Tjek om ovenstående giver mening] 2.7.1.3 Merge For the sake of being able to compare consumption (obtained from buffet data) with liking and motives (obtained from the survey data) these data frames needs to be merged. There are several merge options, here we use left_join() but full_join() and right_join() might more suited in some situations - depending on which data set you want to have appear first, and how you want to merge them. If you feel more comfortable with Excel, you can also merge the two data frames in one Excel sheet before importing it to R. 2.7.1.3.1 Adding survey to buffets Merging should be done such that Person and Day in each separate sheet match. If you additionally have demographic data (gender, age, etc.) then obviously only Person should match, as the data is constant over Days. Buffet_plus_survey &lt;- BuffetConsumption %&gt;% left_join(Surveywide, by = c(&#39;Person&#39;,&#39;Day&#39;)) left_join checks in Surveywide and BuffetConsumption in columns “Person” and “Day”, and will add rows from Surveywide to BuffetConsumption when values in both columns are the same. 2.7.1.3.2 Adding buffet to survey Similarly, merging should be done such that Person and Day match. If you additionally have demographic data (gender, age, etc.) then obviously only Person should match, as the data is constant over Days. Further, we use the long format of the survey data here. Surveylong_buffet &lt;- Surveylong %&gt;% left_join(BuffetConsumption, by = c(&#39;Person&#39;,&#39;Day&#39;)) Due to not having a total overlap of information, some responses (here for consumption) will be missing. That you can see using the table function. table(is.na(Surveylong_buffet$Consumption)) ## ## FALSE ## 60 [TILFØJ: Forklaring på koderne nedenunder dem OG så skal koderne rettes til så det passer med mine nye Excelnavne] [Julius: Forstår ikke hvad der skal vises her, der er ikke nogen NA når jeg kører koden?] "],["introduction-linear-and-mixed-models.html", "Chapter 3 Introduction linear and mixed models", " Chapter 3 Introduction linear and mixed models [ALT MANGLER - forslag er at lave et kortere afsnit her som en pendant til PCA. Håber I synes det er en god ide?] "],["libraries.html", "Chapter 4 Libraries", " Chapter 4 Libraries R comes with a bit of functionality. However, most of the useful tools in R is distributed as packages. There are +10.000 package for R, so it is a jungle to figure out what the most easy solution to your problem at hand is. However, the teams who have made tidyverse and ggplot2 etc. have made a lot of things much more easy, and we strongly rely on their tools and routines in data analysis. To install packages from CRAN (the main repo where R-packages are distributed) install.packages(&#39;somepackagename&#39;) To install packages from github (the online place where all the development and general code sharing is distributed) devtools::install_github(&#39;developername/somepackagename&#39;) To make packages available within your analysis use library(), or use the package name followed by :: and the function. The library function will activate the installed package. library(ggplot2) # lets plot dadta library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ tibble 3.1.8 ✔ dplyr 1.0.9 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.1 ## ✔ readr 2.1.2 ✔ forcats 0.5.2 ## ✔ purrr 0.3.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggpubr) # lets add stats to the plots library(knitr) # lets make nice tables ggplot2::qplot(rnorm(100)) # example of a function call without library&#39;ing the package. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. [MANGLER: info på tidyverse] "],["descriptive-statistics.html", "Chapter 5 Descriptive statistics 5.1 Descriptives for a continouos variable 5.2 Distributions of count data 5.3 Aggregate 5.4 Tidyverse", " Chapter 5 Descriptive statistics In this Chapter we will go through the main elements of descriptive statistics. In principle, descriptive statistics is the act of taking a bunch of data and represent them in few numbers, such as mean, median, standard deviation etc. For a more thorough introduction you can check every introductory stats book: The first couple of chapters will cover this. 5.1 Descriptives for a continouos variable 5.1.1 Mean / median vembedr::embed_youtube(&quot;https://www.youtube.com/watch?v=uhxtUt_-GyM&amp;list=PL1328115D3D8A2566&amp;index=1&quot;) 5.1.2 Variance vembedr::embed_youtube(&quot;https://www.youtube.com/watch?v=Qf3RMGXR-h8&amp;list=PL1328115D3D8A2566&amp;index=4&quot;) 5.1.3 Standard deviation vembedr::embed_youtube(&quot;https://www.youtube.com/watch?v=HvDqbzu0i0E&amp;list=PL1328115D3D8A2566&amp;index=5&quot;) For examples we will use two datasets: chili: where and green tea in combination is added to meals and the resulting ad-libitum consumption is recorded, and pasta which is iBuffet data with a survey of preferences for each Person. The data is made available by: library(data4consumerscience) data(chili) data(pasta) # we subset to only have the &quot;Pasta with legumes&quot; data. pasta &lt;- pasta[pasta$StationName==&#39;Pasta with legumes&#39;,] If you do not have imported the data4consumerscience package see: Import data from R-package If you need to import data see: How to import data To compute mean, median, variance, standard deviation, etc. there are functions working directly on vectors: mean(chili$Totalg) ## [1] 1699.977 median(chili$Totalg) ## [1] 1740.4 sd(chili$Totalg) ## [1] 459.6292 var(chili$Totalg) ## [1] 211259 IQR(chili$Totalg) ## [1] 660.6 summary(chili$Totalg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 849.6 1289.1 1740.4 1700.0 1949.7 2905.2 5.2 Distributions of count data If the response can take certain values or categories, then the table function is good in getting how many observations there are within a given vector, or combinations of several vectors. table(pasta$I_like_taste_of_pasta_with_legumes) ## ## Disagree More or less disagree ## 1 3 ## Neither agree nor disagree More or less agree ## 2 5 ## Agree Strongly agree ## 8 11 table(pasta$I_like_taste_of_pasta_with_legumes, pasta$Did_you_consider_the_proteincontent_of_the_dishes_you_choose) ## ## No Yes ## Disagree 0 1 ## More or less disagree 2 1 ## Neither agree nor disagree 1 1 ## More or less agree 1 4 ## Agree 0 8 ## Strongly agree 5 6 You see that most of the answers are in agreement with question, and that there are no observations in the Strongly disagree category. This is a very high level representation, and we usually want to compare means (or other metrics) between different groups. That is to compute descriptive statistics for subsets of the data. There are two ways to do this. Either using the aggregate() function or use the group_by() and summarize() from the tidyverse framework. Below both is shown to characterize Totalg on each of the products 5.3 Aggregate aggregate(chili$Totalg,by = list(chili$Treatment),mean) ## Group.1 x ## 1 Capsaicin 1716.195 ## 2 Green tea 1664.504 ## 3 CH19 1708.977 ## 4 Capsaicin+ Green tea 1649.495 ## 5 placebo 1759.754 aggregate(chili$Totalg,by = list(chili$Treatment),sd) ## Group.1 x ## 1 Capsaicin 473.1042 ## 2 Green tea 462.2564 ## 3 CH19 452.1225 ## 4 Capsaicin+ Green tea 450.3940 ## 5 placebo 468.4211 5.4 Tidyverse library(tidyverse) tb &lt;- chili %&gt;% group_by(Treatment) %&gt;% # specify which grouping vector to use summarise(n = n(), # compute n mn = mean(Totalg), # compute mean s = sd(Totalg), # compute s q1 = quantile(Totalg,0.25), # compute lower 25% quartile q3 = quantile(Totalg,0.75)) # compute upper 75% quartile tb ## # A tibble: 5 × 6 ## Treatment n mn s q1 q3 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Capsaicin 55 1716. 473. 1314. 1943. ## 2 Green tea 53 1665. 462. 1213. 1912. ## 3 CH19 54 1709. 452. 1323. 1959. ## 4 Capsaicin+ Green tea 54 1649. 450. 1271. 1906. ## 5 placebo 54 1760. 468. 1302. 1992 Further, lets print the results in a nice looking table using kable() from the knitr package. library(knitr) kable(tb, caption = &#39;some caption&#39;, digits = 0, format = &#39;simple&#39;) Table 5.1: some caption Treatment n mn s q1 q3 Capsaicin 55 1716 473 1314 1943 Green tea 53 1665 462 1213 1912 CH19 54 1709 452 1323 1959 Capsaicin+ Green tea 54 1649 450 1271 1906 placebo 54 1760 468 1302 1992 [MORTEN: kan vi lave en anden form for tabel her? Meld gerne ind hvad type data det kunne være?? for den man får ud giver ikke så meget mening set fra vores side. og så skal vi overveje om “tidyverse” er nice to know viden?] … and a plot of it tb %&gt;% ggplot(data = ., aes(Treatment,mn, ymin = q1, ymax = q3)) + geom_point() + geom_errorbar() test4 "],["plotting-data.html", "Chapter 6 Plotting data 6.1 Scatter plots 6.2 How to export plots", " Chapter 6 Plotting data Visualizing data is of utmost importance. Especially, looking at the raw data will enable you to point towards outliers and tendencies which may be lost when representing the data with descriptive statistics. ggplot2 is a wide spread library for plotting data and used by a lot of users. Check it out on videos vembedr::embed_youtube(&quot;https://www.youtube.com/watch?v=HPJn1CMvtmI&quot;) There is a very condensed cheat-sheet for ggplot2 on RStudios webpage s library(data4consumerscience) data(chili) data(pasta) # we subset to only have the &quot;Pasta with legumes&quot; data. pasta &lt;- pasta[pasta$StationName==&#39;Pasta with legumes&#39;,] To show the total consumption a histogram is useful ggplot(data = chili, aes(Totalg)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Here there is a clear bi-modal distribution with an upper tail. These observations comes from 5 different treatments, 27 different judges and at 10 test days. We can infer this using a boxplot with jittered raw data on top. ggplot(data = chili, aes(Treatment,Totalg)) + geom_boxplot() + geom_jitter() ggplot(data = chili, aes(factor(Judge),Totalg)) + geom_boxplot() + geom_jitter() None of these explains the bi-modality. However, we can combine test-day information, which essentially is reflecting whether it is first or second trial splitting at day 5: (TestDay&gt;5) ggplot(data = chili, aes(factor(Judge),Totalg, color = factor(TestDay&gt;5))) + geom_boxplot(aes(group = factor(Judge))) + geom_jitter() 6.1 Scatter plots Lets plot the consumption (Totalg) as a function of LikingAppearance, and add a tendency line: ggplot(data = chili, aes(LikingAppearance,Totalg)) + geom_point() + stat_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 6 rows containing non-finite values (stat_smooth). ## Warning: Removed 6 rows containing missing values (geom_point). Now, lets funk this up by splitting into test-days and get colors according to product: ggplot(data = chili, aes(Hunger,Totalg, color = Treatment)) + geom_point() + stat_smooth(se = F, method = lm) + facet_wrap(~ TestDay&gt;5) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). 6.2 How to export plots The plots shown in the Plots pane (lower right of RStudio) can be saved using the Export bottom. You can also save the plots to a file using the ggsave() function. If you run ggsave() without specifying which plot to export, it will use the latest. You can also directly specify the plot to export: myplot &lt;- ggplot(data = chili, aes(Hunger,Totalg, color = Treatment)) + geom_point() + stat_smooth(se = F, method = lm) + facet_wrap(~ TestDay&gt;5) ggsave(&#39;./data/YourFolderForNicePlots/hunger_vs_consumption.pdf&#39;,myplot) ggsave() supports different formats (.png, .tiff, .pdf,…) and further allows for editing the size (height = , width = ). "],["introduction-to-pca-and-multivariate-data.html", "Chapter 7 Introduction to PCA and multivariate data", " Chapter 7 Introduction to PCA and multivariate data Multivariate data is defined as a set of (multiple) response variables measured on the same set of samples. In principle these response variables can be of any nature (continuous, ordinal, binary), but mostly PCA is used for analysis of continnuous. In this book Principal Component Analysis (PCA) is used several times. This chapter will shortly explain the theory behind PCA and the interpretation of relevant plots. PCA is a tool for looking a correlation structure between variables, and groupings of samples. All through visualizations. Check out youtube on the subject for an introduction. A conceptual introduction is given here: knitr::include_url(&quot;https://youtube.com/embed/NFIkD9-MuTY&quot;) A bit of math The multivariate dataset is organized in a matrix \\(\\mathbf{X}\\) with \\(n\\) samples and \\(p\\) variables. This matrix is factorized into so called scores (\\(\\mathbf{T}\\)) and loadings (\\(\\mathbf{P}\\)). \\[ \\mathbf{X} = \\mathbf{T}\\mathbf{P} + \\mathbf{E} \\] This estimation is done such that the residuals (\\(\\mathbf{E}\\)) is minimized in a least squares sense. The upside of using PCA is that we characterize the sample distribution (what is similar and different) using plots of \\(\\mathbf{T}\\), and the correlation of the responses using plots of \\(\\mathbf{P}\\). In addition to PCA there exists a range of methods for factorizing multivariate datasets including Partial Least Squares (PLS), confirmatory factor analysis (CFA), Correspondence Analysis (CA), Redundancy Analysis (RA), and a lot more. Conceptually, all these methods aims to make a latent-factor model just like PCA, and hence understanding the idea and especially how PCA is used, opens up for using a wide range of variants. [Question to Aasmund: What should we extend with? - another video?] "],["pca-on-survey-answers.html", "Chapter 8 PCA on survey answers 8.1 Bi-plot", " Chapter 8 PCA on survey answers [TILFØJ: In this chapter we will look at how to conduct Principal Component Analysis (PCA).] [INDSÆT/FLYT denne tekst, som er sat ind nedenfor: PCA is a tool for looking a correlation structure between variables, and groupings of samples. All through visualizations. Check out youtube on the subject for an introduction. ] knitr::include_url(&quot;https://youtube.com/embed/NFIkD9-MuTY&quot;) [SPØRGSMÅL FRA BODIL: hvad med pakke til at regne modellen ud? bør den ikke også stå på listen] We use a package called ggbiplot for plotting the PCA model. It is located on github and installed by the follow two code lines: install.packages(&#39;devtools&#39;) devtools::install_github(&#39;vqv/ggbiplot&#39;) [SPØRGSMÅL FRA BODIL: når du kun skriver ggbiplot behøves, hvorfor står der så en lang liste over pakken nedenunder her også - det forvirrer. Så enten er de unødvendige eller også skal der stå noget mere tekst på dem, så det hele matcher i måden det gøres på?] library(ggplot2) # lets plot dadta library(tidyverse) library(broom) library(broom.mixed) library(lme4) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack library(ggbiplot) ## Loading required package: plyr ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following object is masked from &#39;package:ggpubr&#39;: ## ## mutate ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact ## Loading required package: scales ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor ## Loading required package: grid library(ggpubr) # lets add stats to the plots library(knitr) # lets make nice tables [MANGLER: også en forklaring på denne pakke under her, hvad bruges den til? og min R siger den ikke findes? Er det fordi du selv er ved at lave den?] library(data4consumerscience) PCA is a tool for looking a correlation structure between variables, and groupings of samples. All through visualizations. Check out youtube on the subject for an introduction. PCA takes numerical data as input, so we use the likert-scales in the form of 1 to 7. Further the yes/no answers are included, and also needs to be changed. x &lt;- pasta %&gt;% mutate(Did_you_take_food_from_both_Dish1_and_Dish2 = Did_you_take_food_from_both_Dish1_and_Dish2 %&gt;% factor %&gt;% as.numeric(), Did_you_consider_the_proteincontent_of_the_dishes_you_choose = Did_you_consider_the_proteincontent_of_the_dishes_you_choose %&gt;% factor() %&gt;% as.numeric()) %&gt;% mutate_if(is.factor, as.numeric) %&gt;% filter(Day==1) %&gt;% # the survey part is the same for both days and both stations. That is what we keep. filter(str_detect(StationName,&#39;leg&#39;)) PCAmdl &lt;- prcomp(x[,c(5:6,8:11)],scale. = T) 8.1 Bi-plot And a plot of the model ggbiplot(PCAmdl, varname.size = 5) + ylim(c(-4,4)) + xlim(c(-2,5)) What does component 1 (PC1) reflect? What does PC2 reflect? Lets plot the model and color the samples according to the consumption (of legumes) cutted at the median. ggbiplot(PCAmdl, groups = factor(x$Consumption&gt;130), ellipse = T, varname.size = 5) + ylim(c(-4,4)) + xlim(c(-3,5)) 8.1.1 Extract the components and run all associations. We are interested in if any of the likert/survey traits reflected by PCA is correlated with consumption. It is a little complicated, but here goes scores &lt;- data.frame(Person = x$Person, PCAmdl$x[,1:2]) # take out the first two components. tbmixed &lt;- pasta %&gt;% left_join(scores, by = &#39;Person&#39;) %&gt;% gather(comp,score,PC1:PC2) %&gt;% group_by(StationName,comp) %&gt;% do(lmer(data = ., Consumption~score + Day + (1|Person)) %&gt;% tidy(conf.int = T)) … Make a table and a plot of the results. tbmixed %&gt;% filter(term==&#39;score&#39;) %&gt;% select(-effect,-group) %&gt;% kable(x = .,caption = &#39;Slopes according to components&#39;, digits = 2, format = &#39;simple&#39;) Table 8.1: Slopes according to components StationName comp term estimate std.error statistic conf.low conf.high Pasta with legumes PC1 score 14.28 13.26 1.08 -11.70 40.27 Pasta with legumes PC2 score -19.47 17.80 -1.09 -54.34 15.41 tbmixed %&gt;% filter(term==&#39;score&#39;) %&gt;% ggplot(data = ., aes(comp,estimate,ymin = conf.low, ymax = conf.high)) + geom_errorbar(width = 0.1) +geom_point()+ geom_hline(yintercept = 0) + facet_grid(~StationName) + theme(legend.position = &#39;bottom&#39;) Interpret the results. "],["linear-models.html", "Chapter 9 Linear models 9.1 Example 9.2 Run a bunch of models at once", " Chapter 9 Linear models Linear models is a general term for models with a single univariate response (dependent variable - \\(y\\) in the formula below), which we want to describe using one or several predictors (independent variables - \\(x\\) in the formula below). \\[ y = a + b \\cdot x + e \\] Here the informative parameter is the slope (\\(b\\)) which indicates the relation between \\(x\\) and \\(y\\). (\\(e\\) is the missfit / residuals of the model). We use tidyverse coding as this makes life much easier. As a tidyverse add on, we use broom for the linear models, broom.mixed and lme4 for the linear mixed models. library(ggplot2) # lets plot dadta library(tidyverse) library(broom) library(broom.mixed) library(lme4) library(ggpubr) # lets add stats to the plots library(knitr) # lets make nice tables The data is already imported, and formated (see Getting_data_in.pdf for details). We simply load this file. library(data4consumerscience) data(pasta) 9.1 Example As response variable, the amount of Consumption of Pasta with mushrooms and use the likert scale I like the taste of pasta with mushrooms! as predictor. We use ONLY Day 1 results. First a plot: pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) %&gt;% filter(Day==1) %&gt;% ggplot(data = ., aes(I_like_taste_of_pasta_with_mushrooms,Consumption)) + geom_point() + stat_smooth(method = lm, se = F) ## `geom_smooth()` using formula &#39;y ~ x&#39; It seems as there is something. So lets build a linear model on this # subset the data x &lt;- pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) %&gt;% filter(Day==1) mdl &lt;- lm(data = x, Consumption~I_like_taste_of_pasta_with_mushrooms) mdl ## ## Call: ## lm(formula = Consumption ~ I_like_taste_of_pasta_with_mushrooms, ## data = x) ## ## Coefficients: ## (Intercept) ## 89.00 ## I_like_taste_of_pasta_with_mushroomsAgree ## 6.60 ## I_like_taste_of_pasta_with_mushroomsStrongly agree ## 82.25 summary(mdl) ## ## Call: ## lm(formula = Consumption ~ I_like_taste_of_pasta_with_mushrooms, ## data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -119.25 -49.60 -3.00 35.58 164.75 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 89.00 60.43 1.473 ## I_like_taste_of_pasta_with_mushroomsAgree 6.60 71.50 0.092 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 82.25 67.56 1.217 ## Pr(&gt;|t|) ## (Intercept) 0.167 ## I_like_taste_of_pasta_with_mushroomsAgree 0.928 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 0.247 ## ## Residual standard error: 85.46 on 12 degrees of freedom ## Multiple R-squared: 0.2043, Adjusted R-squared: 0.07173 ## F-statistic: 1.541 on 2 and 12 DF, p-value: 0.2537 The the slope indicates that by increasing liking by one unit the consumption increase is \\(50.2 \\pm 30.4\\), however, this apparent effect is not statistically significant (\\(p = 0.12\\)). 9.2 Run a bunch of models at once We want to model consumption of both pasta with mushrooms andd legumes, and look at all the likert scales questions as predictors. Further we want to do this for both days. First we create a new long format data frame pastalong &lt;- pasta %&gt;% gather(question,answ,I_like_taste_of_pasta_with_legumes:Pasta_with_mushrooms_is_visually_appealing) %&gt;% mutate(answnum = factor(answ,labels = c(&#39;Disagree&#39;,&#39;More or less disagree&#39;,&#39;Neither agree nor disagree&#39;,&#39;More or less agree&#39;,&#39;Agree&#39;,&#39;Strongly agree&#39;)) %&gt;% as.numeric()) ## Warning: attributes are not identical across measure variables; ## they will be dropped 9.2.1 A plot pastalong %&gt;% filter(!is.na(StationName )) %&gt;% mutate(question2 = question %&gt;% substr(1,34)) %&gt;% # The label is to long, so lets just represent the first 30 letters. ggplot(data = ., aes(answnum,Consumption, color = factor(Day))) + geom_point() + stat_smooth(se = F, method = lm) + stat_cor() + facet_grid(question2 ~ StationName) + theme_bw() + theme(legend.position = &#39;bottom&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; This we similary can run as several linear models. tb &lt;- pastalong %&gt;% filter(!is.na(StationName )) %&gt;% group_by(StationName,question,Day) %&gt;% do(lm(data = ., Consumption~answnum) %&gt;% tidy(conf.int = T)) tb %&gt;% filter(term==&#39;answnum&#39;) %&gt;% select(-statistic) %&gt;% kable(x = .,caption = &#39;All linear models&#39;, digits = 2, format = &#39;simple&#39;) Table 9.1: All linear models StationName question Day term estimate std.error p.value conf.low conf.high Pasta with legumes I_like_taste_of_pasta_with_legumes 1 answnum 8.51 10.65 0.44 -14.49 31.51 Pasta with legumes I_like_taste_of_pasta_with_legumes 2 answnum -4.24 13.27 0.75 -32.91 24.42 Pasta with legumes I_like_taste_of_pasta_with_mushrooms 1 answnum 1.93 9.47 0.84 -18.53 22.39 Pasta with legumes I_like_taste_of_pasta_with_mushrooms 2 answnum 15.10 11.16 0.20 -9.00 39.21 Pasta with legumes Pasta_with_legumes_is_visually_appealing 1 answnum 23.36 7.75 0.01 6.61 40.10 Pasta with legumes Pasta_with_legumes_is_visually_appealing 2 answnum 8.50 16.06 0.61 -26.19 43.19 Pasta with legumes Pasta_with_mushrooms_is_visually_appealing 1 answnum 11.75 17.95 0.52 -27.04 50.54 Pasta with legumes Pasta_with_mushrooms_is_visually_appealing 2 answnum 13.17 12.21 0.30 -13.21 39.55 Pasta with mushroom I_like_taste_of_pasta_with_legumes 1 answnum -10.34 11.56 0.39 -35.32 14.63 Pasta with mushroom I_like_taste_of_pasta_with_legumes 2 answnum 4.33 9.07 0.64 -15.27 23.93 Pasta with mushroom I_like_taste_of_pasta_with_mushrooms 1 answnum 16.00 9.36 0.11 -4.22 36.23 Pasta with mushroom I_like_taste_of_pasta_with_mushrooms 2 answnum 3.75 8.12 0.65 -13.80 21.29 Pasta with mushroom Pasta_with_legumes_is_visually_appealing 1 answnum 8.89 10.75 0.42 -14.35 32.12 Pasta with mushroom Pasta_with_legumes_is_visually_appealing 2 answnum -13.86 10.47 0.21 -36.48 8.75 Pasta with mushroom Pasta_with_mushrooms_is_visually_appealing 1 answnum 16.81 19.38 0.40 -25.05 58.67 Pasta with mushroom Pasta_with_mushrooms_is_visually_appealing 2 answnum -10.72 8.24 0.22 -28.52 7.08 .. A plot of these res ults for a quick interpretation. tb %&gt;% filter(term==&#39;answnum&#39;) %&gt;% ggplot(data = ., aes(question,estimate,ymin = conf.low, ymax = conf.high, color = factor(Day))) + geom_errorbar(width = 0.1, position = position_dodge()) +geom_point()+ geom_hline(yintercept = 0) + coord_flip() +facet_grid(~StationName) + theme(legend.position = &#39;bottom&#39;) Seems as some of the legume consumptions there is a significant association with likert scales. Not as strong for consumption of mushrooms. "],["mixed-models.html", "Chapter 10 Mixed models 10.1 With several variables", " Chapter 10 Mixed models Mixed models are used when there is repetitions in the response due to (here) the person conducting the trial. The two days are repetitions, and hence we can use all the data (not splitting in to days), but need to account for the person in the model. # subset the data x &lt;- pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) mdl &lt;- lmer(data = x, Consumption~I_like_taste_of_pasta_with_mushrooms + Day + (1|Person)) summary(mdl) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Consumption ~ I_like_taste_of_pasta_with_mushrooms + Day + (1 | ## Person) ## Data: x ## ## REML criterion at convergence: 309 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.1229 -0.5717 -0.1507 0.4085 2.0757 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Person (Intercept) 1561 39.51 ## Residual 4794 69.24 ## Number of obs: 30, groups: Person, 15 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 112.594 56.211 2.003 ## I_like_taste_of_pasta_with_mushroomsAgree -1.222 47.725 -0.026 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 48.220 46.189 1.044 ## Day -2.837 25.380 -0.112 ## ## Correlation of Fixed Effects: ## (Intr) I_____ I____a ## I_lk_t____A -0.595 ## I_lk_____Sa -0.662 0.748 ## Day -0.678 -0.035 0.028 This is the joined effect between the two days. Think of an average of the two slopes - one for each day -. Here taking into account that each person has provided two responses of the consumption of pasta with mushrooms. This can also be accomplished using the tidyverse setup engined by the broom.mixed package. In principle, we simply do not loop over Day, but include it in the formula along with person. tbmixed &lt;- pastalong %&gt;% filter(!is.na(StationName )) %&gt;% group_by(StationName,question) %&gt;% do(lmer(data = ., Consumption~answnum + Day + (1|Person)) %&gt;% tidy(conf.int = T)) The output here is a bit different than the lm() model. But it is still the slope of answnum which carries the interesting stuff. tbmixed %&gt;% filter(term==&#39;answnum&#39;) %&gt;% select(-effect,-group) %&gt;% kable(x = .,caption = &#39;All mixed linear models&#39;, digits = 2, format = &#39;simple&#39;) Table 10.1: All mixed linear models StationName question term estimate std.error statistic conf.low conf.high Pasta with legumes I_like_taste_of_pasta_with_legumes answnum 1.71 7.86 0.22 -13.71 17.12 Pasta with legumes I_like_taste_of_pasta_with_mushrooms answnum 6.12 7.65 0.80 -8.87 21.11 Pasta with legumes Pasta_with_legumes_is_visually_appealing answnum 14.16 8.17 1.73 -1.86 30.18 Pasta with legumes Pasta_with_mushrooms_is_visually_appealing answnum 6.23 8.83 0.71 -11.08 23.53 Pasta with mushroom I_like_taste_of_pasta_with_legumes answnum -2.12 7.36 -0.29 -16.56 12.31 Pasta with mushroom I_like_taste_of_pasta_with_mushrooms answnum 10.25 6.53 1.57 -2.55 23.05 Pasta with mushroom Pasta_with_legumes_is_visually_appealing answnum -4.21 7.87 -0.54 -19.63 11.20 Pasta with mushroom Pasta_with_mushrooms_is_visually_appealing answnum -4.33 8.38 -0.52 -20.76 12.10 tbmixed %&gt;% filter(term==&#39;answnum&#39;) %&gt;% ggplot(data = ., aes(question,estimate,ymin = conf.low, ymax = conf.high)) + geom_errorbar(width = 0.1) +geom_point()+ geom_hline(yintercept = 0) + coord_flip() +facet_grid(~StationName) + theme(legend.position = &#39;bottom&#39;) Do the associations match as expected? 10.1 With several variables We can add several predictors to the model, here that could several likert-scale questions, and maybe demographics with the consumption as response. This is in principle the same for both linear models and linear mixed models. x &lt;- pasta %&gt;% filter(str_detect(StationName,&#39;mush&#39;)) mdl &lt;- lmer(data = x, Consumption~I_like_taste_of_pasta_with_mushrooms + Pasta_with_mushrooms_is_visually_appealing + Day + (1|Person)) summary(mdl) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: ## Consumption ~ I_like_taste_of_pasta_with_mushrooms + Pasta_with_mushrooms_is_visually_appealing + ## Day + (1 | Person) ## Data: x ## ## REML criterion at convergence: 278.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.0180 -0.6058 -0.2061 0.3717 1.9421 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Person (Intercept) 1602 40.03 ## Residual 5399 73.48 ## Number of obs: 30, groups: Person, 15 ## ## Fixed effects: ## Estimate ## (Intercept) 106.913 ## I_like_taste_of_pasta_with_mushroomsAgree 9.942 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 62.756 ## Pasta_with_mushrooms_is_visually_appealingMore or less agree 26.432 ## Pasta_with_mushrooms_is_visually_appealingAgree 23.132 ## Pasta_with_mushrooms_is_visually_appealingStrongly agree -1.810 ## Day -12.589 ## Std. Error t value ## (Intercept) 86.627 1.234 ## I_like_taste_of_pasta_with_mushroomsAgree 59.439 0.167 ## I_like_taste_of_pasta_with_mushroomsStrongly agree 70.044 0.896 ## Pasta_with_mushrooms_is_visually_appealingMore or less agree 74.905 0.353 ## Pasta_with_mushrooms_is_visually_appealingAgree 85.788 0.270 ## Pasta_with_mushrooms_is_visually_appealingStrongly agree 79.813 -0.023 ## Day 32.740 -0.385 ## ## Correlation of Fixed Effects: ## (Intr) I_____ I____a P__ola P_____ P____a ## I_lk_t____A -0.325 ## I_lk_____Sa -0.307 0.824 ## Ps_____Mola -0.659 0.001 -0.005 ## Pst_wt____A -0.331 -0.313 -0.452 0.682 ## Pst_w____Sa -0.531 -0.365 -0.486 0.726 0.849 ## Day -0.556 -0.032 0.044 -0.008 -0.247 0.061 mdl %&gt;% tidy(conf.int = T) ## # A tibble: 9 × 8 ## effect group term estim…¹ std.e…² stati…³ conf.…⁴ conf.…⁵ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fixed &lt;NA&gt; (Intercept) 107. 86.6 1.23 -62.9 277. ## 2 fixed &lt;NA&gt; I_like_taste_of_pas… 9.94 59.4 0.167 -107. 126. ## 3 fixed &lt;NA&gt; I_like_taste_of_pas… 62.8 70.0 0.896 -74.5 200. ## 4 fixed &lt;NA&gt; Pasta_with_mushroom… 26.4 74.9 0.353 -120. 173. ## 5 fixed &lt;NA&gt; Pasta_with_mushroom… 23.1 85.8 0.270 -145. 191. ## 6 fixed &lt;NA&gt; Pasta_with_mushroom… -1.81 79.8 -0.0227 -158. 155. ## 7 fixed &lt;NA&gt; Day -12.6 32.7 -0.385 -76.8 51.6 ## 8 ran_pars Person sd__(Intercept) 40.0 NA NA NA NA ## 9 ran_pars Residual sd__Observation 73.5 NA NA NA NA ## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic, ## # ⁴​conf.low, ⁵​conf.high Try to interpret the slopes? Are the slopes significantly different from 0 (i.e. the point of no association). .. And hey! Why is the slope for visual all of a sudden negative?… Does that mean that consumption increase the less you like the visual appearance? .. Or what? Complete the same analysis with legumes. "],["cata-data-check-all-that-apply.html", "Chapter 11 CATA data (Check-All-That-Apply) 11.1 Importing and looking at the beer data 11.2 Two versions of the data 11.3 Cochran’s Q test 11.4 PCA on CATA data", " Chapter 11 CATA data (Check-All-That-Apply) Check-All-That-Apply (CATA) data is in its raw form binary indicating whether a participant finds a product to have the attribute (1) or not (0). Usually, such data is organized in a matrix where each row corresponds to the evaluation of one product by one judge. And the coloumns are then the attributes. Say you for instance have 26 participants and 4 products, and further that all products are evaluated by all judges once on 13 attributes. Your data matrix would then have 104 rows and 13 coloumns (with responses) and additionally columns indicating judge, product, record id, date, etc. In the CATA section of this book, we will use a data set with Beer. Six different commercial beers from Danish craft brewers were evaluated by \\(160\\) consumers on a range of different questions: Background information: a range of questions, including appropriateness ratings for 27 sensory descriptors on a 7-points scale (e.g. how appropriate do you think it is for a beer to be bitter?). The two semantic anchors were 1 = not at all appropriate and 7 = extremely appropriate. This dataset is called beercata. Hedonics: Their liking/hedonic responses of the beer on a 7-point Likert scale (1-7). This dataset is called Beerliking. [NB NB NB nye navne ift det nye Exceldatasæt, så det skal rettes i scripts nedenfor og tjekkes om de rette Excel ark er kopieret med over? Bør måske se ud som det vil for de studerende?] [BOM: skrive mere info til her om data måske?] [Der skal i alt være load data//import data, view datastructure, histogrammer, fortolkning af disse, cochrans Q test + fortolkning heraf, post hos en eller anden form + fortolkning heraf + PCA] 11.1 Importing and looking at the beer data The data appear as a part of the data4consumerscience package (see Import data from R-package) We first have to import or load the data. Here is the import, remember to change the path for the Excel file to match your own settings: library(readxl) beercata &lt;- read_excel(&#39;DatasetRbook.xlsx&#39;,sheet = &#39;BeerCATA&#39;) BeerBackground &lt;- read_excel(&#39;DatasetRbook.xlsx&#39;,sheet = &#39;BeerBackground&#39;) beerliking &lt;- read_excel(&#39;DatasetRbook.xlsx&#39;,sheet = &#39;BeerLiking&#39;) [PAKKEN data4consumerscience FINDES IKKE?] [BODIL: liking datasæt skal sættes ind i Excel filen i Dropbox] The packages you need to run the analyses are activated with the library function and the package name. If the package is not installed, please do this first: library(data4consumerscience) library(tidyverse) data(&quot;beercata&quot;) beercata %&gt;% head() ## Consumer.ID Beer S_Flowers S_Beans S_Intense berries S_Caramel S_Nuts ## 1 a01 Wheat IPA 0 0 0 0 1 ## 2 a02 Wheat IPA 0 0 0 0 0 ## 3 a03 Wheat IPA 0 0 0 0 0 ## 4 a04 Wheat IPA 0 1 0 0 0 ## 5 a05 Wheat IPA 0 0 0 0 0 ## 6 a06 Wheat IPA 0 0 0 0 0 ## S_Savoury spices S_Dessert spices S_Regional spices S_Herbs S_Citrus fruit ## 1 0 0 0 1 0 ## 2 0 0 1 0 0 ## 3 0 0 0 0 0 ## 4 1 0 0 0 0 ## 5 0 0 0 0 1 ## 6 0 0 0 1 0 ## S_Berries S_Fruit S_Dried fruit S_Liquor S_Bitter S_Sparkling S_Refreshing ## 1 0 0 0 0 1 0 0 ## 2 0 0 0 0 0 0 0 ## 3 1 0 0 0 1 0 0 ## 4 0 0 0 0 1 0 1 ## 5 0 0 0 0 1 0 1 ## 6 0 0 1 0 1 1 0 ## S_Fruity S_Aromatic S_Pungent S_Still S_Smoked S_Foamy S_Sour S_Sweet ## 1 0 1 1 0 0 0 0 0 ## 2 0 0 0 0 0 0 1 0 ## 3 0 0 0 0 1 1 0 0 ## 4 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 1 0 0 0 ## S_Warming S_Vinous ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 table(beercata$Beer) ## ## Brown Ale NY Lager Porse Bock Ravnsborg Red River Beer ## 160 160 160 160 160 ## Wheat IPA ## 160 length(unique(beercata$Consumer.ID)) ## [1] 160 From the above functions, you can see the data structure. Using the str() function will give you all the variable names. The lentgh() function is counting the number of participants as we have asked for the Consumer.ID variable in the dataset beercata. 11.2 Two versions of the data For analyses of CATA data, we need to versions of the data: Raw data (binary, 0/1) with each row being responses from one evaluation (beercata dataset) Agglomerated to counts, with each row being one product [Put in the pictures from Rinnan et al 2015] The agglomerated version is computed by: # Questions we want answer using these data # - Are the products different / similar? # - Which attributes drives discrimination? # - Are there any judges who are really of? beercatasum &lt;- beercata %&gt;% gather(attrib, val, S_Flowers:S_Vinous) %&gt;% group_by(Beer,attrib) %&gt;% dplyr::summarize(n = sum(val)) %&gt;% spread(attrib,n) ## `summarise()` has grouped output by &#39;Beer&#39;. You can override using the ## `.groups` argument. beercatasum ## # A tibble: 6 × 28 ## # Groups: Beer [6] ## Beer S_Aro…¹ S_Beans S_Ber…² S_Bit…³ S_Car…⁴ S_Cit…⁵ S_Des…⁶ S_Dri…⁷ S_Flo…⁸ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brown… 57 54 6 63 52 10 13 21 4 ## 2 NY La… 37 10 5 69 16 30 20 6 46 ## 3 Porse… 20 2 5 67 7 34 16 6 27 ## 4 Ravns… 52 25 6 71 35 11 19 14 17 ## 5 River… 22 12 4 65 3 29 8 4 22 ## 6 Wheat… 29 9 9 57 9 30 24 6 45 ## # … with 18 more variables: S_Foamy &lt;dbl&gt;, S_Fruit &lt;dbl&gt;, S_Fruity &lt;dbl&gt;, ## # S_Herbs &lt;dbl&gt;, `S_Intense berries` &lt;dbl&gt;, S_Liquor &lt;dbl&gt;, S_Nuts &lt;dbl&gt;, ## # S_Pungent &lt;dbl&gt;, S_Refreshing &lt;dbl&gt;, `S_Regional spices` &lt;dbl&gt;, ## # `S_Savoury spices` &lt;dbl&gt;, S_Smoked &lt;dbl&gt;, S_Sour &lt;dbl&gt;, S_Sparkling &lt;dbl&gt;, ## # S_Still &lt;dbl&gt;, S_Sweet &lt;dbl&gt;, S_Vinous &lt;dbl&gt;, S_Warming &lt;dbl&gt;, and ## # abbreviated variable names ¹​S_Aromatic, ²​S_Berries, ³​S_Bitter, ⁴​S_Caramel, ## # ⁵​`S_Citrus fruit`, ⁶​`S_Dessert spices`, ⁷​`S_Dried fruit`, ⁸​S_Flowers We call our new dataset beercatasum. Gather all the variables from S_Flower to S_Vinous and call them attrib. Group all data by the Beer variable (sample name column) and attrib (all of our CATA variables), then sum up the values (val) and call them n. Make a table of attrib and n. Save it all as the new name beercatasum. Then finally shown us the new data set. [MORTEN: er det korrekt fortolket af mig?] … and visualized by for instance a barplot. # summary counts over attribute beercatasum %&gt;% gather(attrib, n, S_Flowers:S_Vinous) %&gt;% ggplot(data = ., aes(attrib,n, fill = Beer)) + geom_bar(stat = &#39;identity&#39;, position = position_dodge()) + coord_flip() For more plot types go to the Chapter on “Plotting data”. 11.3 Cochran’s Q test Cochran’s Q test is a statistical test for the comparison of several products, where the response is binary, and there is repeated responses across several judges. We need a package (RVAideeeeMemoire).The data needs to be structured as the beercata is. We can only run the model independently for one variable at a time. For one response variable: S_Flowers library(RVAideMemoire) ## *** Package RVAideMemoire v 0.9-81-2 *** ## ## Attaching package: &#39;RVAideMemoire&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## dummy ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap m &lt;- cochran.qtest(S_Flowers ~ Beer | Consumer.ID, data = beercata) m ## ## Cochran&#39;s Q test ## ## data: S_Flowers by Beer, block = Consumer.ID ## Q = 63.252, df = 5, p-value = 2.581e-12 ## alternative hypothesis: true difference in probabilities is not equal to 0 ## sample estimates: ## proba in group &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 0.02500 0.28750 0.16875 0.10625 0.13750 ## &lt;NA&gt; ## 0.28125 The p.value is strongly significant, indicating that we cannot assumme the same level of S_Flower in all beers. I.e. the beers seems different based on this characteristics. This is in agreement with the barplot above, where S_Flower is high in NY Lager and really low for Brown ale. But in reality we only know that the beers are different overall, not which specific beers that are different. For this we need at post hoc test. 11.3.1 Post hoc contrasts As we observe differences based on this attribute, we pursue the question on which products stick out? And are there products which are similar? This is done by pairwise comparisons, for this we need the package rcompanion: library(rcompanion) PT = pairwiseMcnemar(S_Flowers ~ Beer | Consumer.ID, data = beercata, test = &quot;permutation&quot;, method = &quot;fdr&quot;, digits = 3) PT$Pairwise %&gt;% arrange(-abs(as.numeric(Z))) %&gt;% data.frame() ## Comparison Z p.value p.adjust ## 1 Brown Ale - NY Lager = 0 -6.48 9.13e-11 1.37e-09 ## 2 Brown Ale - Wheat IPA = 0 -5.86 4.71e-09 3.53e-08 ## 3 Brown Ale - Porse Bock = 0 -4.27 1.95e-05 9.75e-05 ## 4 NY Lager - Ravnsborg Red = 0 4.14 3.43e-05 1.29e-04 ## 5 Ravnsborg Red - Wheat IPA = 0 -3.96 7.5e-05 2.25e-04 ## 6 NY Lager - River Beer = 0 3.54 0.000402 8.48e-04 ## 7 Brown Ale - River Beer = 0 -3.53 0.000415 8.48e-04 ## 8 River Beer - Wheat IPA = 0 -3.51 0.000452 8.48e-04 ## 9 Brown Ale - Ravnsborg Red = 0 -2.98 0.00286 4.77e-03 ## 10 NY Lager - Porse Bock = 0 2.52 0.0118 1.77e-02 ## 11 Porse Bock - Wheat IPA = 0 -2.36 0.0181 2.47e-02 ## 12 Porse Bock - Ravnsborg Red = 0 1.54 0.123 1.54e-01 ## 13 Porse Bock - River Beer = 0 0.845 0.398 4.26e-01 ## 14 Ravnsborg Red - River Beer = 0 -0.845 0.398 4.26e-01 ## 15 NY Lager - Wheat IPA = 0 0.135 0.893 8.93e-01 [MORTEN: Forklaring af koden, taaaaak :-)] The table is sorted with the most different pairs at the top, and the least different at the bottom. Hence most products are significantly different, while Porse Bock and Ravnsborg Red are fairly alike. [MORTEN: skal man bruge p.value eller p.adjust?] 11.3.2 For all attributes in one run (nice to know) We use the packages tidyverse and broom for this, but need a function capable of handling Cochran’s Q-test outputs. library(broom) tidy.RVtest &lt;- function(m){ r &lt;- data.frame(statistic = m$statistic,df = m$parameter, p.value= m$p.value, method = m$method.test) return(r) } tb_cochran &lt;- beercata %&gt;% gather(attrib, val, S_Flowers:S_Vinous) %&gt;% group_by(attrib) %&gt;% do(cochran.qtest(val ~ Beer | Consumer.ID, data = .) %&gt;% tidy) tb_cochran %&gt;% arrange(p.value) ## # A tibble: 27 × 5 ## # Groups: attrib [27] ## attrib statistic df p.value method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 S_Beans 114. 5 7.19e-23 Cochran&#39;s Q test ## 2 S_Caramel 108. 5 1.08e-21 Cochran&#39;s Q test ## 3 S_Flowers 63.3 5 2.58e-12 Cochran&#39;s Q test ## 4 S_Aromatic 45.8 5 9.90e- 9 Cochran&#39;s Q test ## 5 S_Sweet 36.3 5 8.24e- 7 Cochran&#39;s Q test ## 6 S_Warming 35.9 5 1.01e- 6 Cochran&#39;s Q test ## 7 S_Smoked 33.9 5 2.47e- 6 Cochran&#39;s Q test ## 8 S_Liquor 33.9 5 2.55e- 6 Cochran&#39;s Q test ## 9 S_Citrus fruit 29.4 5 1.96e- 5 Cochran&#39;s Q test ## 10 S_Dried fruit 26.0 5 8.81e- 5 Cochran&#39;s Q test ## # … with 17 more rows [MORTEN: MORTEN: Forklaring af koden, taaaaak :-)] Again the table is sorted with the most sigficant at the top, and the least significant at the bottom. This output indicates that S_Beans is the most discriminatory attribute, while S_Pungent is the least. For the pairwise comparisons, please apply the code above per attribute. 11.4 PCA on CATA data For an introduction PCA, please go to the Chapter “Introduction to PCA” in the first part of the book. A PCA on the agglomerated CATA counts will reveal the attributes associated with the individual products: [MANGLER der ikke hvilken pakke man skal installere?] mdlPCA &lt;- prcomp(beercatasum[,-1], scale. = T) ggbiplot::ggbiplot(mdlPCA, labels = beercatasum$Beer) [MORTEN: Forklaing på koden] The attributes Bean, Caramel, Warming, Aromatic etc is associated to the beer Brown ale, while Berrie, Dessert, Pungent, etc. is characteristic of Wheat IPA [BOM: Add some more narrative] "],["liking-scores.html", "Chapter 12 Liking scores 12.1 Plotting liking scores 12.2 Simple mixed models 12.3 Multiway mixed models", " Chapter 12 Liking scores [INTRO NEEDED] Liking scores is often 1-5, 1-7 or 1-9 [more Bom?] Most often we want to know if the liking scored significantly different (not just different by chance) depending on the samples. We might also wna to know if other factors have an influence on the liking. e.g. household income. We also want to know what are the actual differences are. 12.1 Plotting liking scores [MORTEN: meget simpel kode til fx histogram med sprednigner på - gennemsnit per øl] For the beer data we have the liking in a long matrix. We need to import the data: library(readxl) beerliking &lt;- read_excel(&#39;DatasetRbook.xlsx&#39;,sheet = &#39;BeerLiking&#39;) 12.2 Simple mixed models Mixed models are used when there is repetitions in the response due to (here) the person tasting more than one product. [MERE TEKST PÅ HER] [Forklar fixed og random effects? Måske bruge dette:Fixed effects are effects that we anticipate have the same direction, e.g., mutual differences between products. Would typically be the same from one experiment to another as the products are unchanging entities. Random effects are effects that we cannot predict, e.g., mutual differences between consumers may differ from one experiment to another as consumers are affected by various emotional, environmental, physiological or other influences in their lives] library(lmerTest) ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step mdl &lt;- lmer(data = beerliking, Liking ~ Beer + (1|Consumer.ID)) anova(mdl) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Beer 110.52 22.105 5 763.12 8.3054 1.175e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 [MORTEN: Sensorikere er virkelig glade for p-værdier for en variabel - det kan jeg ikke se, jeg kan få ud med lme4, derfor foreslår jeg pakken lmerTest i stedet for. Lavet af Per Brockhoff til sensorikdata. og så også anova() i stedet for summary(). Så kommer der een overordnet p-værdi ud ] To explain the model: take the dataset called beerliking and calculate a model where Beer is the fixed effect and the consumer is the random effect for the response variable Liking. Use the function lmer and save the output as mdl. The anova() function will provide you with the p value(s). Remember you choose your own title of your model. [MANGLER: forklaring på output; At least two of the products by name are scored significantly different for the liking.] 12.2.1 Post hoc contrasts [HEDDER DET DET? korrekt titel?] [SKAL SKRIVES DET HELE] To calculate pairwise comparisons between e.g. samples and find letter-based representation you need a the package multcomp. library(multcomp) ## Loading required package: mvtnorm ## Loading required package: survival ## Loading required package: TH.data ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## ## Attaching package: &#39;TH.data&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## geyser cld(glht(mdl, linfct = mcp(Beer = &quot;Tukey&quot;))) ## Brown Ale NY Lager Porse Bock Ravnsborg Red River Beer ## &quot;c&quot; &quot;ab&quot; &quot;a&quot; &quot;bc&quot; &quot;a&quot; ## Wheat IPA ## &quot;a&quot; [MANGLER: forklaring på model og output] The letters are based on the Post Hoc test Tukey, and they are calculated on the basis of the model mdl. Samples with the same letters are not significantly different. You can only use this function on factors. [MORTEN: Hvis man nu har kontinuerte variable, hvordan fortolkes det så?] 12.3 Multiway mixed models [explain + output + interpret] In multiway models, we start with the largest possible, i.e., including all relevant explanatory variables (e.g., including the product variable). We then have to eliminate non-significant variables manually one by one (backwards step-wise elimination). The final model will only contain the significant variables. In the beer dataset it could be nice to calculate if the liking is affected by the gender and age of the consumer. library(lmerTest) mdl.2 &lt;- lmer(data = beerliking, Liking ~ Beer + Gender + Age + (1|Consumer.ID)) summary(mdl.2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Liking ~ Beer + Gender + Age + (1 | Consumer.ID) ## Data: beerliking ## ## REML criterion at convergence: 3600.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.41056 -0.72755 0.08595 0.74181 2.07687 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Consumer.ID (Intercept) 0.3144 0.5607 ## Residual 2.6621 1.6316 ## Number of obs: 920, groups: Consumer.ID, 155 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 4.5904 0.1954 357.9021 23.487 &lt; 2e-16 *** ## BeerNY Lager -0.6437 0.1869 763.4365 -3.443 0.000606 *** ## BeerPorse Bock -0.8393 0.1866 762.7991 -4.498 7.94e-06 *** ## BeerRavnsborg Red -0.2354 0.1866 762.7991 -1.261 0.207542 ## BeerRiver Beer -0.8378 0.1867 764.6555 -4.488 8.30e-06 *** ## BeerWheat IPA -0.9433 0.1869 763.5511 -5.046 5.63e-07 *** ## GenderM 0.1070 0.1448 148.5136 0.739 0.461235 ## Age26 - 35 0.3992 0.2329 149.8802 1.714 0.088585 . ## Age36 - 45 0.4326 0.2521 146.4248 1.716 0.088203 . ## Age46 - 55 0.5538 0.2145 149.6847 2.582 0.010785 * ## Age56 - 65 0.2041 0.2040 147.5124 1.001 0.318700 ## Age66 - 75 0.2173 0.2730 146.4260 0.796 0.427381 ## AgeOver 75 0.2224 0.5953 173.5838 0.374 0.709194 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation matrix not shown by default, as p = 13 &gt; 12. ## Use print(x, correlation=TRUE) or ## vcov(x) if you need it [MANGLER: FORTOLKNING AF OUTPUT] It could be ice to calculate if the liking of specific sample are affected by the age, so we include the interaction between Beer and Age: mdl.3 &lt;- lmer(data = beerliking, Liking ~ Beer + Beer*Age + (1|Consumer.ID)) summary(mdl.2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Liking ~ Beer + Gender + Age + (1 | Consumer.ID) ## Data: beerliking ## ## REML criterion at convergence: 3600.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.41056 -0.72755 0.08595 0.74181 2.07687 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Consumer.ID (Intercept) 0.3144 0.5607 ## Residual 2.6621 1.6316 ## Number of obs: 920, groups: Consumer.ID, 155 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 4.5904 0.1954 357.9021 23.487 &lt; 2e-16 *** ## BeerNY Lager -0.6437 0.1869 763.4365 -3.443 0.000606 *** ## BeerPorse Bock -0.8393 0.1866 762.7991 -4.498 7.94e-06 *** ## BeerRavnsborg Red -0.2354 0.1866 762.7991 -1.261 0.207542 ## BeerRiver Beer -0.8378 0.1867 764.6555 -4.488 8.30e-06 *** ## BeerWheat IPA -0.9433 0.1869 763.5511 -5.046 5.63e-07 *** ## GenderM 0.1070 0.1448 148.5136 0.739 0.461235 ## Age26 - 35 0.3992 0.2329 149.8802 1.714 0.088585 . ## Age36 - 45 0.4326 0.2521 146.4248 1.716 0.088203 . ## Age46 - 55 0.5538 0.2145 149.6847 2.582 0.010785 * ## Age56 - 65 0.2041 0.2040 147.5124 1.001 0.318700 ## Age66 - 75 0.2173 0.2730 146.4260 0.796 0.427381 ## AgeOver 75 0.2224 0.5953 173.5838 0.374 0.709194 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation matrix not shown by default, as p = 13 &gt; 12. ## Use print(x, correlation=TRUE) or ## vcov(x) if you need it Remember you choose your own “name” for the model “+” between variables defines (additive) main effects “:” between variables defines an interaction “*” between variables defines a parameterization with both interaction and main effect terms For model selection here, you start by removing the interaction with the hight p-value and then recalculate the model. You cannot remove a single variable if an interaction including it is significant. [MANGLER: FORTOLKNING AF OUTPUT] "],["pca-on-cata-and-liking-scores.html", "Chapter 13 PCA on CATA and liking scores", " Chapter 13 PCA on CATA and liking scores To include liking scores and CATA data in one analysis, we use PCA. [Dette afsnit bliver i 2022 i en light version, jeg foreslår at vi i 2023 includerer et ##pca og ##pls] [ALT HER SKAL LAVES] A PCA on the agglomerated CATA counts will reveal the attributes associated with the individual products: [MANGLER der ikke hvilken pakke man skal installere?] [MODEL RETTES MORTEN] mdlPCAcataliking &lt;- prcomp(beercatasum[,-1], scale. = T) ggbiplot::ggbiplot(mdlPCAcataliking, labels = beercatasum$Beer) [MORTEN: Forklaing på koden] The attributes Bean, Caramel, Warming, Aromatic etc is associated to the beer Brown ale, while Berrie, Dessert, Pungent, etc. is characteristic of Wheat IPA [BOM: Add some more narrative] "],["projective-mapping.html", "Chapter 14 Projective mapping 14.1 Example from mapping of XX 14.2 A Collated version of the data 14.3 PCA on Collated data 14.4 ", " Chapter 14 Projective mapping [An image of a PM] 14.1 Example from mapping of XX library(data4consumerscience) data(&quot;tempetotemperature&quot;) tempetotemperature ## Productname Product Sousvidetemperature_C Sousvidetime_days id Assessor ## 1 45C-2D Sample 1 45 2 903 1 ## 2 45C-3D Sample 2 45 3 714 1 ## 3 45C-4D Sample 3 45 4 31 1 ## 4 50C-2D Sample 4 50 2 487 1 ## 5 50C-3D Sample 5 50 3 133 1 ## 6 50C-4D Sample 6 50 4 827 1 ## 7 55C-2D Sample 7 55 2 538 1 ## 8 55C-3D Sample 8 55 3 215 1 ## 9 55C-4D Sample 9 55 4 341 1 ## 10 45C-2D Sample 1 45 2 903 2 ## 11 45C-3D Sample 2 45 3 714 2 ## 12 45C-4D Sample 3 45 4 31 2 ## 13 50C-2D Sample 4 50 2 487 2 ## 14 50C-3D Sample 5 50 3 133 2 ## 15 50C-4D Sample 6 50 4 827 2 ## 16 55C-2D Sample 7 55 2 538 2 ## 17 55C-3D Sample 8 55 3 215 2 ## 18 55C-4D Sample 9 55 4 341 2 ## 19 45C-2D Sample 1 45 2 903 3 ## 20 45C-3D Sample 2 45 3 714 3 ## 21 45C-4D Sample 3 45 4 31 3 ## 22 50C-2D Sample 4 50 2 487 3 ## 23 50C-3D Sample 5 50 3 133 3 ## 24 50C-4D Sample 6 50 4 827 3 ## 25 55C-2D Sample 7 55 2 538 3 ## 26 55C-3D Sample 8 55 3 215 3 ## 27 55C-4D Sample 9 55 4 341 3 ## 28 45C-2D Sample 1 45 2 903 4 ## 29 45C-3D Sample 2 45 3 714 4 ## 30 45C-4D Sample 3 45 4 31 4 ## 31 50C-2D Sample 4 50 2 487 4 ## 32 50C-3D Sample 5 50 3 133 4 ## 33 50C-4D Sample 6 50 4 827 4 ## 34 55C-2D Sample 7 55 2 538 4 ## 35 55C-3D Sample 8 55 3 215 4 ## 36 55C-4D Sample 9 55 4 341 4 ## 37 45C-2D Sample 1 45 2 903 5 ## 38 45C-3D Sample 2 45 3 714 5 ## 39 45C-4D Sample 3 45 4 31 5 ## 40 50C-2D Sample 4 50 2 487 5 ## 41 50C-3D Sample 5 50 3 133 5 ## 42 50C-4D Sample 6 50 4 827 5 ## 43 55C-2D Sample 7 55 2 538 5 ## 44 55C-3D Sample 8 55 3 215 5 ## 45 55C-4D Sample 9 55 4 341 5 ## 46 45C-2D Sample 1 45 2 903 6 ## 47 45C-3D Sample 2 45 3 714 6 ## 48 45C-4D Sample 3 45 4 31 6 ## 49 50C-2D Sample 4 50 2 487 6 ## 50 50C-3D Sample 5 50 3 133 6 ## 51 50C-4D Sample 6 50 4 827 6 ## 52 55C-2D Sample 7 55 2 538 6 ## 53 55C-3D Sample 8 55 3 215 6 ## 54 55C-4D Sample 9 55 4 341 6 ## 55 45C-2D Sample 1 45 2 903 7 ## 56 45C-3D Sample 2 45 3 714 7 ## 57 45C-4D Sample 3 45 4 31 7 ## 58 50C-2D Sample 4 50 2 487 7 ## 59 50C-3D Sample 5 50 3 133 7 ## 60 50C-4D Sample 6 50 4 827 7 ## 61 55C-2D Sample 7 55 2 538 7 ## 62 55C-3D Sample 8 55 3 215 7 ## 63 55C-4D Sample 9 55 4 341 7 ## 64 45C-2D Sample 1 45 2 903 8 ## 65 45C-3D Sample 2 45 3 714 8 ## 66 45C-4D Sample 3 45 4 31 8 ## 67 50C-2D Sample 4 50 2 487 8 ## 68 50C-3D Sample 5 50 3 133 8 ## 69 50C-4D Sample 6 50 4 827 8 ## 70 55C-2D Sample 7 55 2 538 8 ## 71 55C-3D Sample 8 55 3 215 8 ## 72 55C-4D Sample 9 55 4 341 8 ## 73 45C-2D Sample 1 45 2 903 9 ## 74 45C-3D Sample 2 45 3 714 9 ## 75 45C-4D Sample 3 45 4 31 9 ## 76 50C-2D Sample 4 50 2 487 9 ## 77 50C-3D Sample 5 50 3 133 9 ## 78 50C-4D Sample 6 50 4 827 9 ## 79 55C-2D Sample 7 55 2 538 9 ## 80 55C-3D Sample 8 55 3 215 9 ## 81 55C-4D Sample 9 55 4 341 9 ## 82 45C-2D Sample 1 45 2 903 10 ## 83 45C-3D Sample 2 45 3 714 10 ## 84 45C-4D Sample 3 45 4 31 10 ## 85 50C-2D Sample 4 50 2 487 10 ## 86 50C-3D Sample 5 50 3 133 10 ## 87 50C-4D Sample 6 50 4 827 10 ## 88 55C-2D Sample 7 55 2 538 10 ## 89 55C-3D Sample 8 55 3 215 10 ## 90 55C-4D Sample 9 55 4 341 10 ## 91 45C-2D Sample 1 45 2 903 11 ## 92 45C-3D Sample 2 45 3 714 11 ## 93 45C-4D Sample 3 45 4 31 11 ## 94 50C-2D Sample 4 50 2 487 11 ## 95 50C-3D Sample 5 50 3 133 11 ## 96 50C-4D Sample 6 50 4 827 11 ## 97 55C-2D Sample 7 55 2 538 11 ## 98 55C-3D Sample 8 55 3 215 11 ## 99 55C-4D Sample 9 55 4 341 11 ## X1 Y1 X2 Y2 X3 Y3 X4 Y4 X5 Y5 X6 Y6 X7 Y7 X8 ## 1 32.2 34.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 2 15.1 23.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 3 6.6 34.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 4 30.1 16.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 5 42.8 20.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 6 38.6 8.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 7 3.8 17.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 8 44.8 12.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 9 45.4 4.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 10 0.0 0.0 41.8 20.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 11 0.0 0.0 42.5 36.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 12 0.0 0.0 23.3 25.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 13 0.0 0.0 32.6 23.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 14 0.0 0.0 26.5 30.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 15 0.0 0.0 12.2 27.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 16 0.0 0.0 36.9 14.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 17 0.0 0.0 11.9 14.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 18 0.0 0.0 1.5 18.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 19 0.0 0.0 0.0 0.0 1.4 1.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 20 0.0 0.0 0.0 0.0 10.3 12.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 21 0.0 0.0 0.0 0.0 26.6 14.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 22 0.0 0.0 0.0 0.0 16.6 4.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 23 0.0 0.0 0.0 0.0 34.5 16.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 24 0.0 0.0 0.0 0.0 43.8 31.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 25 0.0 0.0 0.0 0.0 27.6 23.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 26 0.0 0.0 0.0 0.0 49.9 24.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 27 0.0 0.0 0.0 0.0 58.8 38.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 28 0.0 0.0 0.0 0.0 0.0 0.0 14.3 36.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 29 0.0 0.0 0.0 0.0 0.0 0.0 4.2 36.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 30 0.0 0.0 0.0 0.0 0.0 0.0 9.2 28.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 31 0.0 0.0 0.0 0.0 0.0 0.0 48.3 32.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 32 0.0 0.0 0.0 0.0 0.0 0.0 29.7 21.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 33 0.0 0.0 0.0 0.0 0.0 0.0 46.1 11.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 34 0.0 0.0 0.0 0.0 0.0 0.0 29.1 29.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 35 0.0 0.0 0.0 0.0 0.0 0.0 48.7 17.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 36 0.0 0.0 0.0 0.0 0.0 0.0 53.4 9.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 37 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.4 10.6 0.0 0.0 0.0 0.0 0.0 ## 38 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.3 38.4 0.0 0.0 0.0 0.0 0.0 ## 39 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.6 30.9 0.0 0.0 0.0 0.0 0.0 ## 40 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 8.2 22.7 0.0 0.0 0.0 0.0 0.0 ## 41 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 42.3 22.9 0.0 0.0 0.0 0.0 0.0 ## 42 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 52.2 15.8 0.0 0.0 0.0 0.0 0.0 ## 43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 28.5 36.7 0.0 0.0 0.0 0.0 0.0 ## 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 43.3 35.9 0.0 0.0 0.0 0.0 0.0 ## 45 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 55.0 35.5 0.0 0.0 0.0 0.0 0.0 ## 46 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 45.8 31.8 0.0 0.0 0.0 ## 47 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 55.9 37.2 0.0 0.0 0.0 ## 48 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 37.9 26.2 0.0 0.0 0.0 ## 49 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 54.7 32.2 0.0 0.0 0.0 ## 50 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 32.9 16.1 0.0 0.0 0.0 ## 51 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 13.8 12.0 0.0 0.0 0.0 ## 52 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 22.7 23.4 0.0 0.0 0.0 ## 53 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 9.1 23.0 0.0 0.0 0.0 ## 54 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.7 8.9 0.0 0.0 0.0 ## 55 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.9 7.6 0.0 ## 56 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 38.5 0.0 ## 57 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 18.4 32.2 0.0 ## 58 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 22.4 21.1 0.0 ## 59 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 44.3 17.0 0.0 ## 60 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 42.2 31.8 0.0 ## 61 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 26.1 6.8 0.0 ## 62 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 43.7 7.4 0.0 ## 63 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 51.9 38.6 0.0 ## 64 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 56.3 ## 65 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 45.2 ## 66 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 45.6 ## 67 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 56.1 ## 68 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 32.7 ## 69 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 15.1 ## 70 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 28.0 ## 71 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.5 ## 72 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6.6 ## 73 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 74 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 76 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 77 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 78 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 79 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 80 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 81 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 82 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 83 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 84 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 85 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 86 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 87 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 88 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 89 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 90 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 91 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 92 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 93 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 94 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 95 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 96 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 97 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 98 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 99 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## Y8 X9 Y9 X10 Y10 X11 Y11 Miso Soft Bitter Sweet Sour Mild Caramel ## 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 1 1 0 ## 3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1 1 1 1 1 0 ## 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 1 1 0 0 ## 5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 1 0 0 1 1 ## 6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 1 0 ## 7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 1 0 ## 8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 0 ## 9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 0 ## 10 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 11 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 13 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 14 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 16 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 18 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 19 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 1 0 0 ## 20 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 21 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 22 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 23 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 24 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 1 ## 25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 26 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 1 ## 28 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 29 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 30 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 31 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 32 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 33 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 1 ## 34 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 1 ## 35 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 1 ## 36 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 37 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 0 0 0 ## 38 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 39 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 1 0 1 0 0 ## 40 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 1 0 0 ## 41 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 42 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 1 0 0 ## 43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 45 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 46 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 1 0 0 ## 47 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 48 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 1 0 0 ## 49 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 50 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 51 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 52 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 0 0 0 ## 53 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 54 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 55 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 56 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 57 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 58 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 59 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 60 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 0 ## 61 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 62 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 63 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 64 30.7 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 65 1.3 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 66 11.9 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 67 15.3 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 68 10.1 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 69 13.8 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 70 26.6 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 71 24.7 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 0 0 1 ## 72 14.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 73 0.0 53.4 35.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 74 0.0 6.8 34.4 0.0 0.0 0.0 0.0 0 0 0 1 0 1 0 ## 75 0.0 12.0 21.6 0.0 0.0 0.0 0.0 0 0 0 1 0 0 0 ## 76 0.0 54.6 12.7 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 77 0.0 38.9 13.7 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 78 0.0 9.7 7.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 79 0.0 49.8 28.3 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 80 0.0 30.0 27.5 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 81 0.0 6.0 9.2 0.0 0.0 0.0 0.0 0 0 0 1 0 0 0 ## 82 0.0 0.0 0.0 5.5 7.6 0.0 0.0 0 0 0 0 0 0 0 ## 83 0.0 0.0 0.0 49.5 35.5 0.0 0.0 0 0 0 0 0 0 0 ## 84 0.0 0.0 0.0 31.0 12.9 0.0 0.0 0 0 0 1 0 0 0 ## 85 0.0 0.0 0.0 21.1 21.8 0.0 0.0 0 0 0 0 0 0 0 ## 86 0.0 0.0 0.0 53.8 18.9 0.0 0.0 0 0 0 0 0 0 0 ## 87 0.0 0.0 0.0 40.6 27.8 0.0 0.0 0 0 0 1 0 0 0 ## 88 0.0 0.0 0.0 8.9 33.2 0.0 0.0 0 0 0 0 0 0 0 ## 89 0.0 0.0 0.0 46.6 8.8 0.0 0.0 0 0 0 1 0 0 0 ## 90 0.0 0.0 0.0 56.6 8.2 0.0 0.0 0 0 0 0 0 0 0 ## 91 0.0 0.0 0.0 0.0 0.0 18.3 12.5 0 0 0 1 0 0 0 ## 92 0.0 0.0 0.0 0.0 0.0 4.6 26.2 0 0 1 0 0 0 0 ## 93 0.0 0.0 0.0 0.0 0.0 30.2 4.6 0 0 0 0 0 0 0 ## 94 0.0 0.0 0.0 0.0 0.0 16.8 22.0 0 0 0 0 0 1 0 ## 95 0.0 0.0 0.0 0.0 0.0 45.3 31.3 0 0 0 0 0 0 0 ## 96 0.0 0.0 0.0 0.0 0.0 27.2 24.3 0 0 0 0 0 1 0 ## 97 0.0 0.0 0.0 0.0 0.0 19.2 31.2 0 0 0 0 0 0 0 ## 98 0.0 0.0 0.0 0.0 0.0 29.0 34.2 0 0 0 0 0 0 0 ## 99 0.0 0.0 0.0 0.0 0.0 34.6 17.9 0 0 0 0 0 0 0 ## Umami Deep Balanced Nutty Hard Dry Roasted Strong Fish sauce Dark Chocolate ## 1 1 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 1 1 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 1 0 0 0 0 0 ## 5 0 0 0 0 0 0 1 0 0 0 0 ## 6 0 0 0 0 0 0 0 1 0 0 0 ## 7 0 0 0 0 1 1 0 0 0 0 0 ## 8 0 0 0 0 0 1 0 0 1 1 1 ## 9 0 0 0 0 0 1 1 1 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 0 0 ## 12 0 0 0 0 0 0 0 0 0 1 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 ## 15 0 0 0 0 0 0 0 0 0 1 0 ## 16 1 0 0 0 0 1 0 0 0 0 0 ## 17 0 0 0 0 0 1 0 0 0 0 0 ## 18 0 0 0 0 0 0 0 0 0 1 0 ## 19 0 0 0 0 0 0 0 0 0 0 0 ## 20 1 0 0 0 0 0 0 0 0 0 0 ## 21 1 0 0 0 0 0 0 0 0 1 0 ## 22 1 0 0 0 0 0 0 0 0 0 0 ## 23 1 0 0 0 0 0 0 0 0 1 0 ## 24 1 0 0 0 0 1 0 0 1 1 0 ## 25 1 0 0 0 0 0 0 0 0 0 0 ## 26 1 0 0 0 0 0 0 0 0 1 0 ## 27 1 0 0 0 0 1 0 0 0 1 0 ## 28 0 0 0 0 0 0 0 0 0 0 0 ## 29 0 0 0 0 0 0 0 0 0 0 0 ## 30 0 0 0 0 0 0 0 0 0 0 0 ## 31 0 0 0 0 0 0 0 0 0 0 0 ## 32 0 0 1 0 0 0 0 0 0 0 0 ## 33 0 0 0 0 0 0 0 0 0 1 0 ## 34 0 0 1 1 0 0 0 0 0 0 0 ## 35 0 0 0 0 0 0 0 0 0 0 0 ## 36 0 0 0 0 0 0 0 0 0 1 0 ## 37 0 0 0 0 0 0 0 0 0 0 0 ## 38 0 0 0 0 0 0 0 1 0 0 0 ## 39 0 0 0 0 0 0 0 0 0 0 0 ## 40 0 0 0 0 0 0 0 0 0 0 0 ## 41 0 0 0 0 0 0 0 0 0 0 0 ## 42 0 0 0 0 0 0 0 0 0 1 0 ## 43 0 0 0 0 0 1 0 0 0 0 0 ## 44 0 0 0 0 0 1 0 1 0 1 0 ## 45 0 0 0 0 0 1 0 1 0 1 0 ## 46 0 0 0 0 0 0 0 0 0 0 0 ## 47 0 0 0 0 0 0 0 0 0 0 0 ## 48 0 0 0 0 0 0 0 0 0 0 0 ## 49 0 0 0 0 0 0 0 0 0 0 0 ## 50 0 0 0 0 0 0 0 0 0 0 0 ## 51 0 0 0 0 0 0 0 0 0 1 0 ## 52 0 0 0 0 0 0 0 0 0 0 0 ## 53 0 0 0 0 0 0 0 0 0 1 0 ## 54 0 0 0 0 0 0 0 0 0 1 0 ## 55 0 0 1 0 0 0 0 0 0 0 0 ## 56 0 0 1 0 0 0 0 0 0 0 0 ## 57 0 0 1 0 0 0 0 0 0 0 0 ## 58 0 0 0 0 0 0 0 0 0 0 0 ## 59 0 0 1 0 0 0 0 0 0 0 0 ## 60 0 0 0 0 0 0 0 0 0 0 0 ## 61 0 0 0 0 0 0 0 0 0 0 0 ## 62 0 0 0 0 0 0 1 0 0 1 0 ## 63 0 0 0 0 0 0 0 0 0 0 1 ## 64 0 0 0 0 0 0 0 0 0 0 0 ## 65 0 0 0 0 0 0 0 0 0 0 0 ## 66 0 0 0 0 0 0 0 0 0 0 0 ## 67 0 0 0 0 0 0 0 0 0 0 0 ## 68 1 0 0 0 0 0 0 0 0 0 0 ## 69 0 0 0 0 0 0 0 0 0 0 0 ## 70 0 0 0 0 0 0 0 0 0 0 0 ## 71 0 0 0 0 0 0 0 0 0 0 0 ## 72 0 0 0 0 0 0 0 0 0 0 0 ## 73 0 0 0 0 0 0 0 0 0 0 0 ## 74 0 0 0 0 0 0 0 0 0 0 0 ## 75 0 0 0 0 0 0 0 0 0 0 0 ## 76 0 0 0 0 0 0 0 0 0 0 0 ## 77 0 0 0 0 0 0 0 0 0 0 0 ## 78 0 0 0 0 0 0 0 0 0 0 0 ## 79 0 0 0 0 0 1 0 0 0 0 0 ## 80 0 0 0 0 0 0 0 0 0 0 0 ## 81 0 0 0 0 0 0 0 0 0 0 0 ## 82 0 0 0 0 0 0 0 0 0 0 0 ## 83 0 1 0 0 0 0 0 0 0 0 0 ## 84 0 0 0 0 0 0 0 0 0 0 0 ## 85 0 0 0 0 0 0 0 0 0 0 0 ## 86 0 0 0 0 0 0 0 0 0 0 0 ## 87 0 0 0 0 0 0 0 0 0 0 0 ## 88 0 0 0 0 0 0 0 0 0 0 0 ## 89 0 0 0 0 0 0 0 0 0 0 0 ## 90 0 1 0 0 0 0 0 0 0 0 0 ## 91 0 0 0 0 0 0 0 0 0 0 0 ## 92 0 0 0 0 0 0 0 0 0 0 0 ## 93 0 0 0 0 0 0 0 0 0 0 0 ## 94 0 0 0 0 0 0 0 0 0 0 0 ## 95 0 0 0 0 0 0 0 0 0 0 0 ## 96 0 0 0 0 0 0 0 0 0 0 0 ## 97 0 0 0 0 0 0 0 0 0 0 0 ## 98 0 0 0 0 0 0 0 0 0 0 0 ## 99 0 0 0 0 1 0 0 0 0 0 0 ## Moisty Brown Dried fruits Beany Smooth Dense Alcoholic Firm Grainy Soy sauce ## 1 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 ## 10 1 0 0 0 0 0 0 0 0 0 ## 11 1 0 0 0 0 0 0 0 0 0 ## 12 1 0 0 0 0 0 0 0 0 0 ## 13 1 0 0 0 0 0 0 0 0 0 ## 14 1 0 0 0 0 0 0 0 0 0 ## 15 1 0 0 0 0 0 0 0 0 0 ## 16 0 0 0 0 0 0 0 0 0 0 ## 17 0 1 0 0 0 0 0 0 0 0 ## 18 1 0 0 0 0 0 0 0 0 0 ## 19 0 0 1 0 0 0 0 0 0 0 ## 20 0 0 1 0 0 0 0 0 0 0 ## 21 0 0 1 0 0 0 1 0 0 0 ## 22 0 0 1 1 1 1 0 0 0 0 ## 23 0 0 0 1 0 0 0 0 0 0 ## 24 0 0 0 1 0 1 0 0 0 1 ## 25 0 0 0 1 0 0 0 1 1 0 ## 26 0 0 0 0 0 0 0 0 0 1 ## 27 0 0 0 1 0 0 0 0 1 1 ## 28 0 0 0 0 0 0 0 0 0 0 ## 29 0 1 0 0 0 1 0 0 0 0 ## 30 0 0 0 0 1 1 0 0 0 0 ## 31 0 0 0 0 1 0 0 1 0 0 ## 32 0 1 0 0 1 0 0 1 0 0 ## 33 0 0 0 0 0 0 1 0 0 0 ## 34 0 1 0 0 0 0 0 0 0 0 ## 35 0 1 0 1 0 0 0 1 1 0 ## 36 0 0 0 0 0 0 0 1 1 0 ## 37 0 0 0 0 0 0 0 0 0 0 ## 38 0 0 0 0 0 0 0 0 0 0 ## 39 0 0 0 0 0 0 0 0 0 0 ## 40 0 0 0 0 0 0 0 0 0 0 ## 41 0 0 0 0 0 0 0 0 0 0 ## 42 0 0 0 0 0 0 0 0 0 0 ## 43 0 0 0 0 0 0 0 0 0 0 ## 44 0 0 0 0 0 0 0 0 0 0 ## 45 0 0 0 0 0 0 0 0 0 0 ## 46 0 1 0 0 0 0 0 0 0 0 ## 47 0 1 0 0 0 0 0 0 0 0 ## 48 0 1 0 0 0 0 0 0 0 0 ## 49 0 1 0 0 0 0 0 0 0 0 ## 50 0 1 0 0 0 0 0 0 0 0 ## 51 0 0 0 1 0 0 0 0 0 0 ## 52 0 1 0 0 0 0 0 0 0 0 ## 53 0 0 0 0 0 0 0 0 1 0 ## 54 0 0 0 1 0 0 0 0 1 0 ## 55 1 0 0 0 0 0 1 0 0 0 ## 56 1 0 0 0 0 0 1 0 0 0 ## 57 0 0 0 0 0 0 0 0 0 0 ## 58 0 0 0 0 0 0 0 0 0 0 ## 59 0 1 0 0 0 0 1 0 0 0 ## 60 0 0 0 0 0 0 0 0 0 0 ## 61 0 0 0 0 0 0 0 0 1 0 ## 62 0 0 0 0 0 0 0 0 1 0 ## 63 0 0 0 0 0 0 0 0 0 0 ## 64 0 0 0 0 0 0 0 0 0 0 ## 65 0 0 0 0 0 0 0 0 0 0 ## 66 0 0 0 0 0 0 0 0 0 0 ## 67 0 0 0 0 0 0 0 0 0 0 ## 68 0 0 0 0 0 0 0 0 0 0 ## 69 0 0 0 0 0 0 0 0 0 1 ## 70 0 0 0 0 0 1 0 0 0 0 ## 71 0 0 0 0 0 0 0 0 0 1 ## 72 0 0 0 0 0 0 0 0 0 1 ## 73 0 0 0 0 0 0 1 0 0 0 ## 74 0 0 0 0 0 0 0 0 0 0 ## 75 0 0 0 0 0 0 0 0 0 0 ## 76 0 0 0 0 0 0 0 0 0 0 ## 77 0 0 0 0 0 0 0 0 0 0 ## 78 0 0 0 0 0 0 0 0 0 1 ## 79 0 0 0 0 0 0 1 0 0 0 ## 80 0 0 0 0 0 0 0 0 1 0 ## 81 0 0 1 0 0 0 0 0 0 1 ## 82 0 0 0 0 0 0 0 0 0 0 ## 83 0 0 0 0 0 0 0 0 0 0 ## 84 0 0 0 0 0 0 0 0 0 0 ## 85 0 0 0 0 0 0 0 0 0 0 ## 86 0 0 0 0 0 0 0 0 0 0 ## 87 0 0 0 0 0 0 0 0 0 0 ## 88 0 0 0 0 0 0 0 0 0 0 ## 89 0 0 0 0 0 0 0 0 0 0 ## 90 0 0 0 0 0 0 0 0 0 0 ## 91 0 0 0 0 0 0 0 0 0 0 ## 92 0 0 0 0 0 0 0 0 0 0 ## 93 0 0 0 0 0 0 0 0 0 0 ## 94 0 0 0 0 0 0 0 0 0 0 ## 95 0 0 0 0 0 0 0 0 1 0 ## 96 0 0 0 0 0 0 0 0 0 0 ## 97 0 0 0 0 0 0 0 0 0 0 ## 98 0 0 0 0 0 0 0 0 1 0 ## 99 0 0 0 0 0 0 0 0 1 0 ## Chewy Yeasty Fruity Wine Gummy Malty Liquorise Bread Salty Meaty Creamy ## 1 0 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 0 0 ## 12 0 0 0 0 0 0 0 0 0 0 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 ## 15 0 0 0 0 0 0 0 0 0 0 0 ## 16 0 0 0 0 0 0 0 0 0 0 0 ## 17 0 0 0 0 0 0 0 0 0 0 0 ## 18 0 0 0 0 0 0 0 0 0 0 0 ## 19 0 0 0 0 0 0 0 0 0 0 0 ## 20 0 0 0 0 0 0 0 0 0 0 0 ## 21 0 0 0 0 0 0 0 0 0 0 0 ## 22 0 0 0 0 0 0 0 0 0 0 0 ## 23 0 0 0 0 0 0 0 0 0 0 0 ## 24 0 0 0 0 0 0 0 0 0 0 0 ## 25 0 0 0 0 0 0 0 0 0 0 0 ## 26 1 0 0 0 0 0 0 0 0 0 0 ## 27 0 0 0 0 0 0 0 0 0 0 0 ## 28 0 0 1 0 0 0 0 0 0 0 0 ## 29 1 1 1 0 0 0 0 0 0 0 0 ## 30 1 0 1 0 0 0 0 0 0 0 0 ## 31 0 0 0 0 0 0 0 0 0 0 0 ## 32 0 0 0 0 1 0 0 0 0 0 0 ## 33 0 0 0 0 0 0 0 0 0 0 0 ## 34 0 0 0 1 0 0 0 0 0 0 0 ## 35 0 0 1 0 0 0 0 0 0 0 0 ## 36 0 0 0 0 0 0 0 0 0 0 0 ## 37 0 0 1 0 0 0 0 0 0 0 0 ## 38 0 0 0 0 0 0 0 0 0 0 0 ## 39 0 0 0 0 0 0 0 0 0 0 0 ## 40 0 0 0 0 0 0 0 0 0 0 0 ## 41 0 0 0 0 0 1 0 0 0 0 0 ## 42 0 0 0 0 0 1 0 0 0 0 0 ## 43 0 0 0 0 0 1 0 0 0 0 0 ## 44 0 0 0 0 0 1 0 0 0 0 0 ## 45 0 0 0 0 0 1 0 0 0 0 0 ## 46 0 0 0 0 0 0 0 0 0 0 0 ## 47 0 0 1 0 0 0 0 0 0 0 0 ## 48 0 0 0 0 0 0 0 0 0 0 1 ## 49 0 0 1 0 0 0 0 0 0 0 0 ## 50 0 1 0 0 0 0 0 0 0 0 0 ## 51 0 0 0 0 0 0 0 0 0 0 0 ## 52 0 1 0 0 0 0 0 0 0 0 0 ## 53 0 1 0 0 0 0 0 0 0 0 0 ## 54 0 0 0 0 0 1 1 0 0 0 0 ## 55 0 0 0 0 0 0 0 0 0 0 0 ## 56 0 0 0 0 0 0 0 0 0 0 0 ## 57 0 0 0 0 0 0 0 0 0 0 0 ## 58 0 0 0 0 0 0 0 0 0 0 0 ## 59 0 0 0 0 0 0 0 0 0 0 0 ## 60 0 0 0 0 0 1 0 0 0 0 0 ## 61 0 0 0 0 0 0 0 0 0 0 0 ## 62 0 0 0 0 0 0 0 0 0 0 0 ## 63 0 0 0 0 0 0 1 1 0 0 0 ## 64 0 0 0 0 1 0 0 1 0 0 0 ## 65 0 0 0 0 0 0 0 0 0 0 0 ## 66 0 0 0 0 0 0 0 0 0 0 0 ## 67 0 0 0 0 0 0 0 1 0 0 0 ## 68 0 0 0 0 0 0 0 0 0 0 0 ## 69 0 0 0 0 0 0 0 0 1 0 0 ## 70 0 0 1 0 0 0 0 0 0 0 0 ## 71 0 0 0 0 0 0 0 0 0 0 0 ## 72 0 0 0 0 0 0 0 0 1 0 0 ## 73 0 0 0 0 0 0 0 0 0 0 0 ## 74 0 0 0 0 0 0 0 0 0 0 0 ## 75 0 0 0 0 0 0 0 0 1 1 0 ## 76 0 0 0 0 0 0 0 0 0 0 0 ## 77 0 0 0 0 0 0 0 0 0 0 0 ## 78 0 0 0 0 0 0 0 0 0 1 0 ## 79 0 0 0 0 0 0 0 0 0 0 0 ## 80 0 0 0 0 0 0 0 0 0 0 0 ## 81 0 0 0 0 0 0 0 0 0 0 0 ## 82 0 0 0 0 0 0 0 0 0 0 0 ## 83 0 1 0 0 0 0 0 0 0 0 0 ## 84 0 0 0 0 0 0 0 0 0 0 0 ## 85 0 1 0 0 0 0 0 0 0 0 0 ## 86 0 0 0 1 0 0 0 0 0 0 0 ## 87 0 0 0 0 0 0 0 0 0 0 0 ## 88 0 0 0 0 0 0 0 0 0 0 0 ## 89 0 0 0 0 0 0 0 0 0 0 0 ## 90 0 0 0 0 0 0 0 0 0 1 0 ## 91 0 0 1 0 0 0 0 0 0 0 0 ## 92 0 0 0 0 0 0 0 0 0 0 1 ## 93 0 0 0 0 0 0 0 0 0 0 1 ## 94 0 0 0 0 0 0 0 0 0 0 0 ## 95 0 0 0 0 0 0 0 0 0 0 0 ## 96 0 0 0 0 0 0 0 0 0 0 1 ## 97 0 0 0 0 0 0 0 0 0 0 0 ## 98 0 0 0 0 0 1 0 0 0 0 0 ## 99 0 0 0 0 0 1 0 0 0 0 0 ## Stout ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## 11 0 ## 12 0 ## 13 0 ## 14 0 ## 15 0 ## 16 0 ## 17 0 ## 18 0 ## 19 0 ## 20 0 ## 21 0 ## 22 0 ## 23 0 ## 24 0 ## 25 0 ## 26 0 ## 27 0 ## 28 0 ## 29 0 ## 30 0 ## 31 0 ## 32 0 ## 33 0 ## 34 0 ## 35 0 ## 36 0 ## 37 0 ## 38 0 ## 39 0 ## 40 0 ## 41 0 ## 42 0 ## 43 0 ## 44 0 ## 45 0 ## 46 0 ## 47 0 ## 48 0 ## 49 0 ## 50 0 ## 51 1 ## 52 0 ## 53 0 ## 54 1 ## 55 0 ## 56 0 ## 57 0 ## 58 0 ## 59 0 ## 60 0 ## 61 0 ## 62 0 ## 63 0 ## 64 0 ## 65 0 ## 66 0 ## 67 0 ## 68 0 ## 69 0 ## 70 0 ## 71 0 ## 72 0 ## 73 0 ## 74 0 ## 75 0 ## 76 0 ## 77 0 ## 78 0 ## 79 0 ## 80 0 ## 81 0 ## 82 0 ## 83 0 ## 84 0 ## 85 0 ## 86 0 ## 87 0 ## 88 0 ## 89 1 ## 90 0 ## 91 0 ## 92 0 ## 93 0 ## 94 0 ## 95 0 ## 96 0 ## 97 0 ## 98 0 ## 99 0 table(tempetotemperature$Productname) ## ## 45C-2D 45C-3D 45C-4D 50C-2D 50C-3D 50C-4D 55C-2D 55C-3D 55C-4D ## 11 11 11 11 11 11 11 11 11 table(tempetotemperature$Assessor) ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 9 9 9 9 9 9 9 9 9 9 9 This dataset consists of 9 products evaluateed by 11 judges. The responses is the 2D coordinates of the procetive mapping, for each judge individually (X1, Y1, X2,…, Y11), and CATA data on 30 attribues (Miso, Soft, Bitter,…, Stout). 14.2 A Collated version of the data 14.3 PCA on Collated data downweeeigting of attributes. On everything with normal scaling. Include judge-loadings in the output plots. 14.4 "],["tfih-exercises.html", "Chapter 15 TFIH Exercises 15.1 Exercise 1: PCA on consumer background 15.2 Exercise 2: PCA on CATA counts 15.3 Exercise 3: PCA on liking 15.4 Exercise 4: PLS on CATA counts and liking 15.5 Exercise 5: Mixed modelling on the liking 15.6 Exercise 6: Comparing CATA binary data and counts 15.7 Exercise 7: Cochran’s Q test on CATA binary data", " Chapter 15 TFIH Exercises Take 5-10 minutes to look at the publication to get an overview [hvilken publ?] 15.1 Exercise 1: PCA on consumer background From this exercise you should be able to describe who your consumers are. Make the data available: data(&quot;beerdemo&quot;) Calculate a PCA model including the Variables 7 ( Interest in food ) to 39 ( App_Vinous ). Remember to standardize/scale the variables mdlPCA &lt;- prcomp(beerdemo[,7:39],scale. = T) Plot the scores and loadings in a biplot and look for groupings of the consumers in the scores. Group and color according to the background information not used in the model (Gender, Age,..) library(ggbiplot) ggbiplot(mdlPCA, groups = beerdemo$Gender, ellipse = T) Describe what you find. 15.2 Exercise 2: PCA on CATA counts From this exercise you should be able to describe your samples (beers) from the CATA counts. Collated (summed) for each beer of CATA score from all consumers. Setup the collated version as described above. beercatasum &lt;- beercata %&gt;% gather(attrib, ... Calculate a PCA model including all Variables and all Objects. PCAmdl &lt;- prcomp(beercatasum, scale. = T) Plot the scores and describe the groupings of the samples. Plot the loadings and describe the correlations between the variables. ggbiplot(PCAmdl) Use this biplot to find out which samples are described by which words. 15.3 Exercise 3: PCA on liking From this exercise you should be able to describe the liking of the beer samples and see how the consumers do this. Calculate a PCA model including all Variables and all Objects. include_these &lt;- complete.cases(beerliking) PCAliking &lt;- prcomp(beerliking[include_these,-1], scale. = T) Plot a biplot or loading plot, and use the loadings and describe the correlations between the variables (liking of beers in this case). ggbiplot(PCAliking) Plot the scores and describe the groupings of the samples by colouring the score plot according to the consumer background variables. Note that the 160 rows in both datasets match each-other, so we can glue the demo information directly onto the liking model. If that was not the case, matching using left_join() or inner_join() would be nessesary before analysis. ggbiplot(PCAliking,groups = beerdemo$Age[include_these], ellipse = T) Any trends? For instance, how is liking related to the individual consumer diversity of beer (Beer types/month)? … Some code to get all 7-scale demo information plots. You may want to export and view in a pdf viewer for zooming etc. gall &lt;- cbind(PCAliking$x[,1:2], beerdemo[include_these,]) %&gt;% gather(var,val,`Interest in food`:App_Vinous) %&gt;% ggplot(data = ., aes(PC1,PC2, color = factor(val))) + geom_point() + stat_ellipse() + facet_wrap(~var) ggsave(filename = &#39;anicebigfigure.pdf&#39;,gall, height = 20, width = 20) 15.4 Exercise 4: PLS on CATA counts and liking From this exercise you should be able to conclude what drives the liking of your samples (beers). For each beer, the collated CATA counts is the predictors, and the averaged liking is the response. likingsum &lt;- beerliking %&gt;% gather(Beer, liking, -`Consumer ID`) %&gt;% group_by(Beer) %&gt;% dplyr::summarise(lik = mean(liking, na.rm = T)) Check that the rows are ordered in the same way: likingsum$beer beercatasum$Beer CATAlik &lt;- list() CATAlik$CATA &lt;- scale(as.matrix(beercatasum[,-1])) CATAlik$lik &lt;- scale(likingsum$lik) rownames(CATAlik$lik) &lt;- rownames(CATAlik$CATA) &lt;- beercatasum$Beer Calculate a PLS model where CATA features are predictors and liking is response for all Objects. library(pls) catalik.pls &lt;- plsr(lik ~ CATA, ncomp = 2, data = CATAlik, validation = &quot;LOO&quot;) corrplot(catalik.pls, labels = colnames(beercatasum)[-1]) biplot(catalik.pls) Plot the loadings and study which X variables are important for the liking score. Advanced: Plot the Regression coefficients (scaled) and try to interpret the meaning of this plot (Hint: use your findings from the loadings plot). 15.5 Exercise 5: Mixed modelling on the liking Dataset: Beer_XYZmatrix.xlsx, sheet “Z and Y liking” Import the datasheet in to R Studio. Check to see if all variables have the correct description/denomination (factor, numerical etc.) Are there any significant product differences for the liking? If so, what does the Tukey tell us? How does this fit with what you have done in the PCA/PLS exercises. Is the liking in general affected by the age, gender, household size or beer knowledge? What is the effect? Try to think of a plot that can show the significant differences. Do men and women score the samples significantly different in liking? Calculate the sample/gender differences in averages, try to use Pivot Tables in Excel. 15.6 Exercise 6: Comparing CATA binary data and counts Dataset: Beer_XYZmatrix.xlsx, sheets “X CATA collated, counts” and “Z + Y + X unfolded” If time… Calculate two PCA models: one on the X unfolded matrix (CATA answers in binary codes, more columns and you just choose the ones you need) and one on the CATA counts. Compare the outcome of the two models. Evaluate explained variance Evaluate loadings plots Is this expected when looking at counts and “raw” data What type of information is lost by looking at the CATA counts? 15.7 Exercise 7: Cochran’s Q test on CATA binary data Dataset: Beer_XYZmatrix.xlsx, sheet “X unfolded” If time… Import the datasheet in to R Studio using the CSV format (save the file as CSV in Excel). Choose 4 relevant CATA attributes (based on your previous results today) to make a Cochran’s Q test for, comment on the results (i.e. the sample differences). "],["preference-mapping.html", "Chapter 16 Preference Mapping 16.1 Example of Preference Mapping 16.2 PCA 16.3 Analysis by PLS 16.4 L-PLS [For the future…]", " Chapter 16 Preference Mapping [some narrative] 16.1 Example of Preference Mapping [Which???] 16.2 PCA PCA is nice…. [on beerliking] library(data4consumerscience) data(&quot;beerliking&quot;) mdlPCA &lt;- prcomp(beerliking[complete.cases(beerliking),-1]) ggbiplot::ggbiplot(mdlPCA) Those who like Ravnsborg red also likes NY Lager and to some extend Brown ale…. [some more narrative] 16.3 Analysis by PLS Predictors can be objective characteristics of the products or CATA type data, while response is hedonik liking data. [minimum 5 samples X = CATA (Beer_XYZmatrix.xlsx, sheet = X CATA (coll.)+Y liking (aver.)), Y = Living average Y2 = Liking for each consumer = t(Y (long thin)) Objective = Visualize to get patterns related to liking, and which deescriptors are merely irrelevant. 16.4 L-PLS [For the future…] "],["latent-factor-models.html", "Chapter 17 Latent Factor Models", " Chapter 17 Latent Factor Models There are many…. "],["lpls.html", "Chapter 18 LPLS", " Chapter 18 LPLS L-PLS - include reference. "],["consumer-segmentation.html", "Chapter 19 Consumer Segmentation 19.1 Kmeans", " Chapter 19 Consumer Segmentation 19.1 Kmeans "],["more-pca.html", "Chapter 20 More PCA", " Chapter 20 More PCA "],["logistic-regression.html", "Chapter 21 Logistic Regression", " Chapter 21 Logistic Regression "],["confirmatory-factor-analysis-using-lavaan.html", "Chapter 22 Confirmatory Factor Analysis using lavaan 22.1 Example - Food Neophobia", " Chapter 22 Confirmatory Factor Analysis using lavaan 22.1 Example - Food Neophobia "],["structured-equation-modelling.html", "Chapter 23 Structured Equation Modelling 23.1 Example - Theory of Planned Behaviour", " Chapter 23 Structured Equation Modelling 23.1 Example - Theory of Planned Behaviour 2+2 ## [1] 4 23.1.1 PLSDA (CATA??)on CATA and liking?? [This needs more love. Skal vente til 2023! hvordan får vi det ud af bogen så?] library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## cluster ## The following object is masked from &#39;package:purrr&#39;: ## ## lift mdl &lt;- plsda(data.frame(beercata[,3:29]),factor(beercata$Beer),ncomp = 3) scores &lt;- mdl$scores %&gt;% unclass %&gt;% as.data.frame %&gt;% cbind(beercata) loadings &lt;- mdl$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% mutate(attrib2 = substr(attrib,3,50)) # lets remove the S_ g1 &lt;- ggplot(data = loadings, aes(`Comp 1`, `Comp 2`, label = attrib2)) + # geom_point() + geom_text() g2 &lt;- ggplot(data = scores, aes(`Comp 1`, `Comp 2`, color = Beer)) + # geom_point() + stat_ellipse(level = 0.5) library(patchwork) ## ## Attaching package: &#39;patchwork&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## area g1 + g2 # do multiple splithalfs # INPUT: judge id. CATA, class, ncomp X &lt;- beercata[,3:29] clss &lt;- factor(beercata$Beer) judge &lt;- beercata$Consumer.ID k &lt;- 3 A &lt;- 30 mdl0 &lt;- plsda(X,clss,ncomp = k) lds0 &lt;- mdl0$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% gather(cmp,val0,-attrib) unjudge &lt;- unique(judge) nindiv &lt;- length(unjudge) LOADS &lt;- data.frame() for (i in 1:A){ ic &lt;- judge %in% sample(unjudge)[1:round(nindiv/2)] mdlSH &lt;- plsda(X[ic,],clss[ic],ncomp = k) df_flip &lt;- data.frame(sng = sign(diag(t(mdl0$loadings) %*% mdlSH$loadings))) %&gt;% rownames_to_column(&#39;cmp&#39;) lds &lt;- mdlSH$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% gather(cmp,val,-attrib) %&gt;% left_join(df_flip, by = &#39;cmp&#39;) %&gt;% mutate(SHiter = i, val = val*sng) LOADS &lt;- bind_rows(LOADS,lds) } fc &lt;- (1 / A)*((A - 1)/A) sdloads &lt;- LOADS %&gt;% left_join(lds0, by = c(&#39;attrib&#39;,&#39;cmp&#39;)) %&gt;% group_by(attrib,cmp) %&gt;% dplyr::summarise(sd = sum((val-val0)^2) *fc) %&gt;% mutate(cmp = paste(&#39;sd&#39;,cmp,sep = &#39;&#39;)) %&gt;% spread(cmp,sd) ## `summarise()` has grouped output by &#39;attrib&#39;. You can override using the ## `.groups` argument. loadsSH &lt;- lds0 %&gt;% spread(cmp,val0) %&gt;% left_join(sdloads, by = &#39;attrib&#39;) library(ggforce) ggplot(data = loadsSH, aes(x0 = `Comp 1`,y0 = `Comp 2`,a = `sdComp 1`,b = `sdComp 2`,angle = 0)) + geom_ellipse() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
