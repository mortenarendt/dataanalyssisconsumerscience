[["index.html", "Data Analysis in R for Sensory and Consumer Science Chapter 1 Introduction to the book", " Data Analysis in R for Sensory and Consumer Science Morten Arendt Rasmussen, Julius Jessen Terp, Helene C. Reinbach and Bodil H. Allesen-Holm October 2022 Chapter 1 Introduction to the book This material is to cover data analysis using R for consumer science targeting the following courses: Meal Systems and Technologies Food Consumer Research Meal Consumer Research Thematic Course in Food Innovation and Health But others may benefit from the material as well…. The initial chapters of the book introduce R and the background for the different statistical methods used in the book. The following parts of the book is divided according to the course hence the data types encountered in the above mentioned courses, The format of the book is simple - first read the written text and when you reach a code, copy the code and try it out yourself. "],["introduction-to-r.html", "Chapter 2 Introduction to R 2.1 How to get started - understanding R (and RStudio) 2.2 How to import data 2.3 How to edit and merge datasets 2.4 How to save the data 2.5 How to export data / results 2.6 How to load your RData 2.7 How to clear your environment", " Chapter 2 Introduction to R R is a free software with a complete programming language for statistical computing and graphics. It is used at many universities and companies since it is always updated and open source. Before staring your calculations in R you should always update the R version on your computer. You can install a graphical interface for R, called R studio. It will use the underlying version of R on your computer – so you have to have R installed too. The principles by R and R Studio are the same – BUT R Studio has a better interface for non-programmers. Both R and R Studio can be used on all types of computers. R is command-line based and it provides a wide variety of statistical methods (linear and nonlinear modelling, classical statistical tests, classification, clustering, …). Advanced methods are available via extension packages (more than 10.000 at the moment) Always be sure to have the latest version of both R and R Studio on your computer - update version just before you need to use the program. 2.1 How to get started - understanding R (and RStudio) A quick intro-tour of R Studio is given here: And a short intro from our courses: 2.1.1 Organise and save scripts A script is a rundown from A-Z (start to end) of data analysis. A script should be self-contained. I.e. the first lines sets the libraries and imports the data, there after you may want to wrangle the data a bit (changing features as.numeric, as.factor,…, renaming columns, etc.). Thereafter the analysis starts. Think of a script like making a meal: You need raw-materials (carrots, onions,…) - That is the data You need a kitchen - That is R as the software. You need knifes, pots and pans - That is the packages. All is needed to work and hence you need to specify them in the script. In larger projects where the same dataset may be used for several different analysis, it may be wise to have several scripts. One for importing data and modifying it (starts with import and ends with save() as an .RData file). One for descriptive analysis, one for inference, one for plots etc. So you can create a sequence of scripts to keep overview. However, this is only needed for larger projects. In small analysis you can easily include all in one script. ) Remember to put a little narrative (after a “#” at the top off you script explaining the purpose. To get started go to upper left corner and open a new script. Remember to save your script as well. After you’ve created a new script, try out our codes: 1+2 ## [1] 3 a &lt;- 2+2 b &lt;- 5+3 What happens if you write the letter a in the editor and run it? What about the letter b? a ## [1] 4 b ## [1] 8 a+b ## [1] 12 … or this one? A+B R is essentially (also a) calculator, but it is case sensitive. In the Editor: “#” is the start of a comment (means: will not evaluated/ read by the program). This is how you can make comments in your script: # I want to add 2 and 5 2+5 ## [1] 7 # whoop it is 7! “:”Generates a sequence (e.g. 1:10 is the numbers from 1 to 10) from1to10 &lt;- 1:10 from10to1 &lt;- 10:1 from1to10 ## [1] 1 2 3 4 5 6 7 8 9 10 from10to1 ## [1] 10 9 8 7 6 5 4 3 2 1 In the Console: “&gt;” indicates that R is ready for a new code. “+” Instead of “&gt;” means that the program is waiting for you. (you probably made a mistake in the script you tried to run) – by [ESC] the “+” turns to a “&gt;” again “NA” (Not Available) is indicating a missing value “NaN” (Not a Number) is the result of an ‘illegal’ operation e.g. log(-1) Red sentences means there is an error. R will stop calculating at the first error it meets. 2.2 How to import data 2.2.1 Import data from R-package In this book several datasets are used targeting different research questions. However, a fair part of the analysis tools are common. That is, descriptive analysis, plots, response correlations etc. The data is included in the R-packgage data4consumerscience you get by running the code below. Be aware that you need devtools package to install packages from github, so you need to run both code lines. # install data-package install.packages(&#39;devtools&#39;) devtools::install_github(&#39;mortenarendt/data4consumerscience&#39;) The data is also available as excel sheets, and can be loaded using packages capable of reading from Excel. Before you start you data import, you have to make sure the data set contains all the information you need and the format of the data (columns and rows) is correct. You can import in many different ways. 2.2.2 Importing a csv file If the data is not already an csv file, but an excel file, you need to convert it: Open your Excel file, as it is in xls or xlsx format. Convert this file to csv format. NB: Some data collection tools will provide you with your data in csv and some xlsx/xls format. In Excel, you choose the “save as” and then choose *.csv. Then move in to R, and write: DATASET1 &lt;- read.csv2(file.choose()) The file.choose() function makes you point towards the file you want. You can also simply write the path to the file directly. Actually, by using the file.choose() the first time you import data will prombt the path, and you can simply copy paste this from the console to your script avoiding point and click every time you want to analyse these data. DATASET1 &lt;- read.csv2(&#39;~/path/to/the/data/myfile.csv&#39;) You decide the names/titles of your datasets and models, just do not use other signs than “.” and avoid non-English letters. We called it “DATASET1”. R will open a new window (sometimes hidden behind your other open windows), open the window to choose the wanted csv file. The data set will now also appear in the upper right corner as a line. If you double click a data set in this box, it will open in the editor window You can import any *.csv format dataset, when you try it out. Trouble shooting: * Try new csv format in Excel when saving the file in csv format * Try to write read.csv(file.choose()) instead * Try another import function (see below) 2.2.3 Importing an Excel file/sheet If you have data as excel, you may utilize packages for directly importing, without the need to convert to csv. If your Excel file contains more than one sheet, you have to import each sheet separately. Here we use the package readxl with the function read_excel. If the data is not in the same folder as your script, then include the path to the data, or move the data to the script’s location. The example below imports from an excel file ( DatasetRbook.xlsx) a sheet (named e.g. BuffetData ), positioned in a folder (named data ) in the current position. You can download this dataset from here. When you have to find the path for the file on your computer, you place your cursor within the ’’ in the command and click the tabulator button. Your computer files will appear, and you can find the path for your file easily. If you cannot find the path, try to use the file.choose() command to find the file, and then copy paste the path from the Console (where you find your output). library(readxl) BuffetConsumption &lt;- read_excel(&#39;./data/DatasetRbook.xlsx&#39;, sheet = &#39;PastaBuffet&#39;) BuffetSurvey &lt;- read_excel(&#39;./data/DatasetRbook.xlsx&#39;,sheet = &#39;PastaSurvey&#39;) The first part of the model sentence is what we want to call our dataset, here we chose “BuffetConsumption” in the first line (that is the same as the sheet in the Excel file for simplicity). You decide the names/titles of your datasets and model, just do not use other signs than “.” and avoid non-English letters. Try to import sheets from an excel file. BuffetConsumption is consumption data in grams from a buffet. The data is from 16 different persons, who came on Day 1 and Day 2 to eat Pasta with legumes and/or Pasta with mushroom. In the dataset, there is one line per buffet station per participant per experimental day. BuffetSurvey is survey data collected in SurveyXact. The dataset contains data on liking, motivation, choices etc. linked to the particular buffet data. Survey could also contain demographics for the participants such as age, gender, eating habits etc. These are general and different from the former, in that they have nothing to do with the current buffet. This type of data is not included in the SurveyData. 2.2.4 Clipboard import Last resort is to import via your clipboard. Go to Excel and mark the data you want to import. Make sure there are headings in the data you have marked. Copy the marked data to the clipboard. Go to the Editor and write the following command line: DATASET2 &lt;- read.table(file=&quot;clipboard&quot;, header=TRUE, sep=&quot;\\t&quot;) Meaning read the table you saved in your clipboard and save it as the name “DATASET2” (remember you choose this name). The data has headers and should be separated in cells. Regardless of importation method – the dataset will appear in the upper right corner environment as a line, please check it looks correct. You can import from any Excel dataset, when you try it out. 2.2.5 Looking at the imported elements Have a look at the imported elements to ensure that indeed, they mimic the Excel sheets. Use the functions head(), str() and View() is your tools. They will give you the headlines in your data, how your variables are categorized and open the dataset in a new tab. head(BuffetConsumption) str(BuffetConsumption) View(BuffetConsumption) Try to use the BuffetConsumption dataset. If it does not look as expected, try to import it again using a different method. 2.2.6 Numbers and factors - changing categorisation During the import R will automatically categorise your variables: if they are read as numbers or letters. For instance, if day of the experiment is called 1 and 2 in the data file and is then read as numeric (num). As Day 2 is not double the value of Day 1, we need to change this variable into a factor (Factor) or character (chr). Use the str() function to check your variables before your change them. You transform your variables using as.numeric() or as.character(). BuffetConsumption$Day&lt;-as.factor(BuffetConsumption$Day) Meaning take the variable Day in the dataset you called BuffetConsumption, make it a factor and put it into the same variable name (overwrites it). If you want to have a new variable coded and then keep the old one, simply just give it a new name, e.g. “DayFactor”. The dataset will then be extended with one variable, but sometimes it is nice to have both versions. BuffetConsumption$DayFactor&lt;-as.factor(BuffetConsumption$Day) 2.3 How to edit and merge datasets Sometimes you have to merge two data sets. This is needed if you have for instance consumption data in one Excel sheet and survey data in another Excel sheet. Setup the data in Excel such that they match the below in terms of format. What is important is: First row is used on headings and none of these are repeated. I.e. all unique within a sheet Data comes from row 2 and then on to the right All rows should contain data (NB: empty cell is also data, e.g. an unanswered questions), so all empty rows are removed (not cells) Headings between sheets referring to the same: e.g. participant ID should have exactly similar heading If you have calculated stuff within Excel such as a sum of the numbers in a column, then these should be removed from the sheet. It is not data! We suggest that you keep both the original version of the data as a sheet, and the ready-to-import version as a sheet, so you do not accidentally delete data. 2.3.0.1 Edit using Tidyverse The Consumption data is optimal as is. We have the data as long format with all repsonses in one coloumn and then the next columns clarifying the design, time, type, person etc. However the Survey data is not optimal directly. We need to revert the data to both long and wide format. There are several ways to do this, including editing in Excel. Here we show how it can be done in R using tidyverse. Tidyverse is a larger framework. For introduction see: and maybe visit tidyverse-homepage for resources. library(tidyverse) Surveylong &lt;- BuffetSurvey %&gt;% gather(question,answ, `Pasta with legumes is visually appealing to me. `: `I like the taste of pasta with mushrooms! `) Surveywide &lt;- Surveylong %&gt;% spread(question,answnum) The code above does exactly that, with Surveylong and Surveywide as the resulting data sets. Try to compare BuffetSurvey with Surveywide - did we already have a wide version of the dataset? What is also introduced here is the pipe operator %&gt;%. It originates from the dplyr-package inside the tidyverse-package, and is a handy tool for data manipulation. The way it works is, that whatever is written on the right side of the operator, will be used as the first argument in the function written on the left side of the operator. This means, that x %&gt;% f(y) will result in f(x,y) - or in our case: BuffetSurvey %&gt;% gather(question,answ, Pasta with legumes is visually appealing to me.:I like the taste of pasta with mushrooms!) will be equal to: gather(BuffetSurvey,question,answ, Pasta with legumes is visually appealing to me.: I like the taste of pasta with mushrooms!) The idea is then to “chain” (or “pipe” as it is also known) %&gt;% together line after line, using different functions in a sequence, which makes the code more readable and often shorter as well. gather will lengthen the data, by stacking the columns we specified on top of each other, resulting in each row being one single answer to one of the 4 questions. The answer will be in the answ-column, the question in the question-column, and the rest of the columns can then be used to e.g. group the data. spread will do the opposite as gather, and spread one column in several columns depending on what the columns contains. In our case the question-column in spread into 4 columns, one for each question from the survey, with the numerical values of the answers as their values. You might have also noticed, that when a variable name contains a space, R needs help understanding that this is indeed a variable. Different symbols are added, and while you CAN write everything just the way R know how to read it, there is an easier way to make sure that everything is written correctly. You can call the variables from the data frame that they originate from, using “dataframe$” and then hit TAB. A list of the variables of which the data frame consist will appear, and from this you can choose the right one - always spelled correctly, and the way R knows how the interpret it. 2.3.0.2 Merging datasets For the sake of being able to compare consumption (obtained from buffet data) with liking and motives (obtained from the survey data) these data frames needs to be merged. There are several merge options, here we use left_join() but full_join() and right_join() might more suited in some situations - depending on which data set you want to have appear first, and how you want to merge them. If you feel more comfortable with Excel, you can also merge the two data frames in one Excel sheet before importing it to R. 2.3.0.2.1 Adding survey to buffets Merging should be done such that Person and Day in each separate sheet match. If you additionally have demographic data (gender, age, etc.) then obviously only Person should match, as the data is constant over Days. Buffet_plus_survey &lt;- BuffetConsumption %&gt;% left_join(BuffetSurvey, by = c(&#39;Person&#39;,&#39;Day&#39;)) left_join checks in BuffetSurvey and BuffetConsumption in columns “Person” and “Day”, and will add rows from BuffetSurvey to BuffetConsumption when values in both columns are the same. 2.3.0.2.2 Adding buffet to survey Similarly, merging should be done such that Person and Day match. If you additionally have demographic data (gender, age, etc.) then obviously only Person should match, as the data is constant over Days. Further, we use the long format of the survey data here. Surveylong_buffet &lt;- Surveylong %&gt;% left_join(BuffetConsumption, by = c(&#39;Person&#39;,&#39;Day&#39;)) 2.4 How to save the data Use save.image() to save everything in the Environment (all variables shown in the “Environment”-tab in the upper right corner of RStudio), or use save() to specify which elements to save using the “list”-input. # Saving everything to the folder of your choice save.image(file = &#39;./data/FolderYouWantToSaveYourProjectTo/AllMyData.RData&#39;) # Saving just the specified datasets and other elements to your folder of choice save(file = &#39;./data/FolderYouWantToSaveYourProjectTo/SomeOfMyData.RData&#39;, list = c(&#39;Survey&#39;,&#39;Surveylong_buffet&#39;,&#39;Surveylong&#39;,&#39;Buffet_survey&#39;&#39;)) 2.5 How to export data / results You can export any data frame from R to excel (for instance using the rio package), as well as saving it as .RData for further analysis. This can obviously be used for exporting your data after some modifications. BUT it is also very useful for exporting data frames with results from analysis. When exporting data, it is also important to tell R where to place the exported file. You do this by specifying the path to the desired folder, followed by the name that you choose for the exported file (often it makes sense to choose the same name as the data frame in R). It is also important to specify the file-extension, to ensure that you create the right file type - in this case .xlsx, but rio can also export to other formats such as .txt or .csv. # export one data frame rio::export(Surveylong_buffet,file = &#39;./data/YourFolderForNiceTables/Surveylong_buffet.xlsx&#39;) 2.6 How to load your RData Once you have saved the data, you can simply load the data directly, and you do not need to do the import-setup every time you want to do an analysis on the data. This part is not a part of the data import, but it is a good idea just to check that the data indeed is setup as expected. load(&#39;./data/FolderWhereYourDataAreStored/YourData.RData&#39;) 2.7 How to clear your environment When you want to start a new project or a new analysis, it can be useful to clear the environment for the data that you previously used. This can be done either by the code shown below, or by clicking the brush in the top-right part of the RStudio-window. But be aware - when you clear your environment, you will have to load the data again rm(list = ls()) THE WHY and HOW "],["libraries.html", "Chapter 3 Libraries", " Chapter 3 Libraries R comes with a bit of functionality. However, most of the useful tools in R is distributed as packages. There are +10.000 package for R, so it is a jungle to figure out what the most easy solution to your problem at hand is. However, the teams who have made tidyverse and ggplot2 etc. have made a lot of things much more easy, and we strongly rely on their tools and routines in data analysis. To install packages from CRAN (the main repo where R-packages are distributed) install.packages(&#39;SomePackageName&#39;) To install packages from github (the online place where all the development and general code sharing is distributed) devtools::install_github(&#39;DeveloperName/SomePackageName&#39;) To make packages available within your analysis use library(), or use the package name followed by :: and the function. The library function will activate the installed package. library(ggplot2) ggplot2::qplot(rnorm(100)) # example of a function call without library&#39;ing the package. Tidyverse is a large framework build to handle and manipulate data in a variety of different ways. An example of said manipulation is shown in chapter 1 under Edit using Tidyverse. You can find even more information here tidyverse-homepage. "],["descriptive-statistics.html", "Chapter 4 Descriptive statistics 4.1 Descriptives for a continuous variable 4.2 Distributions of count data 4.3 Aggregate 4.4 Tidyverse", " Chapter 4 Descriptive statistics In this Chapter we will go through the main elements of descriptive statistics. In principle, descriptive statistics is the act of taking a bunch of data and represent them in few numbers, such as mean, median, standard deviation etc. For a more thorough introduction you can check every introductory stats book: The first couple of chapters will cover this. For examples in this chapter we will use two datasets: chili: where and green tea in combination is added to meals and the resulting ad-libitum consumption is recorded, and pasta which is iBuffet data with a survey of preferences for each Person. The data is made available by: library(data4consumerscience) data(chili) data(pasta) # we subset to only have the &quot;Pasta with legumes&quot; data. pasta &lt;- pasta[pasta$StationName==&#39;Pasta with legumes&#39;,] If you do not have imported the data4consumerscience package see: Import data from R-package If you need to import data see: How to import data 4.1 Descriptives for a continuous variable Below we will first explain the different descriptive measures, and then describe calculations of them. Continuous variables could be sensory scores (e.g. 15 cm continuous line scale), consumption data (in gram) or liking scores (e.g. on a scale from 1 to 9). 4.1.1 Mean / median Here you get an introduction to statistics, descriptive statistics and the terms average/mean and median: 4.1.2 Variance Here you get an introduction to variance: 4.1.3 Standard deviation Here you get an introduction to the standard deviation: 4.1.4 Calculations For the calculations, we use the chili dataset. To compute mean, median, variance, standard deviation, etc. there are functions working directly on vectors: mean(chili$Totalg) ## [1] 1699.977 median(chili$Totalg) ## [1] 1740.4 var(chili$Totalg) ## [1] 211259 sd(chili$Totalg) ## [1] 459.6292 IQR(chili$Totalg) ## [1] 660.6 summary(chili$Totalg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 849.6 1289.1 1740.4 1700.0 1949.7 2905.2 4.2 Distributions of count data If the response can take certain values or categories, then the table function is good in getting how many observations there are within a given vector, or combinations of several vectors. table(pasta$I_like_taste_of_pasta_with_legumes) ## ## 2 3 4 5 6 7 ## 1 3 2 5 8 11 table(pasta$I_like_taste_of_pasta_with_legumes, pasta$Did_you_consider_the_proteincontent_of_the_dishes_you_choose) ## ## No Yes ## 2 0 1 ## 3 2 1 ## 4 1 1 ## 5 1 4 ## 6 0 8 ## 7 5 6 You see that most of the answers are in agreement with question, and that there are no observations in the Strongly disagree category. This is a very high level representation, and we usually want to compare means (or other metrics) between different groups. That is to compute descriptive statistics for subsets of the data. There are two ways to do this. Either using the aggregate() function or use the group_by() and summarize() from the tidyverse framework. Below both is shown to characterize Totalg on each of the products 4.3 Aggregate aggregate(chili$Totalg,by = list(chili$Treatment),FUN = mean) ## Group.1 x ## 1 Capsaicin 1716.195 ## 2 Green tea 1664.504 ## 3 CH19 1708.977 ## 4 Capsaicin+ Green tea 1649.495 ## 5 placebo 1759.754 aggregate(chili$Totalg,by = list(chili$Treatment),FUN = sd) ## Group.1 x ## 1 Capsaicin 473.1042 ## 2 Green tea 462.2564 ## 3 CH19 452.1225 ## 4 Capsaicin+ Green tea 450.3940 ## 5 placebo 468.4211 aggregate() will apply a function to a column or data set, using the list provided to group the column or dataset. In our case, the column we want to apply our function to is chili$Totalg, the functions are FUN = mean or FUN = sd (finding the mean and standard deviation, respectively) and the column we use for grouping the data is chili$Treatment. This results in a data frame, where one column shows each unique group in chili$Treatment, and another column shows the mean or standard deviation of chili$Totalg corresponding to each group. 4.4 Tidyverse library(tidyverse) tb &lt;- chili %&gt;% group_by(Treatment) %&gt;% # specify which grouping vector to use summarise(n = n(), # compute n mn = mean(Totalg), # compute mean s = sd(Totalg), # compute s q1 = quantile(Totalg,0.25), # compute lower 25% quartile q3 = quantile(Totalg,0.75)) # compute upper 75% quartile tb ## # A tibble: 5 × 6 ## Treatment n mn s q1 q3 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Capsaicin 55 1716. 473. 1314. 1943. ## 2 Green tea 53 1665. 462. 1213. 1912. ## 3 CH19 54 1709. 452. 1323. 1959. ## 4 Capsaicin+ Green tea 54 1649. 450. 1271. 1906. ## 5 placebo 54 1760. 468. 1302. 1992 group_by will use the column specified to group the data, and the groups are then used when further manipulation is applied to the data frame. summarise will create a summary of the input data, with the columns specified in the code, calculation the statistics also written in the code (e.g. mn = mean(Totalg) will create a column with the mean of Totalg, for each group in Treatment, as specified by group_by()). For more information about data manipulation with tidyverse, see Edit using Tidyverse or check out the tidyverse-homepage. Further, lets print the results in a nice looking table using kable() from the knitr package. library(knitr) kable(tb, caption = &#39;some caption&#39;, digits = 0, format = &#39;simple&#39;) Table 4.1: some caption Treatment n mn s q1 q3 Capsaicin 55 1716 473 1314 1943 Green tea 53 1665 462 1213 1912 CH19 54 1709 452 1323 1959 Capsaicin+ Green tea 54 1649 450 1271 1906 placebo 54 1760 468 1302 1992 … and a plot of it: tb %&gt;% ggplot(data = ., aes(Treatment,mn, ymin = q1, ymax = q3)) + geom_point() + geom_errorbar() More information on plotting will come in a future chapter (see Plotting data). But try and see if you can make sense of the inputs, and what they correspond to in the plot. "],["inferential-statistics.html", "Chapter 5 Inferential statistics 5.1 Intro 5.2 Hypothesis testing 5.3 Confidence intervals 5.4 T-test 5.5 F-test 5.6 Analysis of Variance (ANOVA) 5.7 Introduction to linear and mixed models 5.8 Normal and Mixed models", " Chapter 5 Inferential statistics 5.1 Intro Inferential statistics is a branch of statistics that deals with making predictions or estimates about a population based on a sample of data from that population. The goal of inferential statistics is to use the sample data to draw conclusions about the population as a whole. This approach is quite neat, as it would be rather time-consuming to e.g. measure the height of every person in the whole world (the population), to be able to show the average height of a person. To avoid this, a (more or less) representative sample is used to estimate the average height of a person, while taking into account the uncertainty that arises from not measuring the whole population. And while the process of obtaining a representative sample is a crucial aspect of inferential statistics, it is outside the scope of this book, and will therefore not be addressed further. Inferential statistics use a variety of techniques such as hypothesis testing, confidence intervals or Analysis of Variance (ANOVA), all of which will be introduced in the following sections. 5.2 Hypothesis testing Statistical hypothesis testing starts with a hypothesis about a population parameter (such as the mean or proportion). Then data are collected, after which statistical techniques are used to decide whether the data provide sufficient evidence to support the hypothesis or not. There are two types of hypotheses in hypothesis testing: the null hypothesis and the alternative hypothesis. The null hypothesis is a statement of no effect or no difference between groups. It is typically denoted as \\(H_{0}\\). The alternative hypothesis is a statement of the opposite of the null hypothesis. It is typically denoted as \\(H_{A}\\) or \\(H_{1}\\). The process of hypothesis testing typically involves the following steps: Formulate the null and alternative hypotheses. For example: The average height of men and women is the same. \\(H_{0}: \\mu_{men}=\\mu_{women} \\\\ H_{A}: \\mu_{men}\\neq\\mu_{women}\\) Select a significance level (\\(\\alpha\\)). This is the probability of making a Type I error, which means rejecting the null hypothesis when in fact, it is true (usually, \\(\\alpha = 0.05\\) is used, but this can vary depending on the application). Collect data and compute a test statistic (which test statistic will depend on the desired test - examples will be presented in the following sections). Determine the p-value. The p-value is the probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. Compare the p-value to the significance level. If the p-value is less than the significance level, we reject the null hypothesis. If the p-value is greater than the significance level, we fail to reject the null hypothesis. It’s important to note that hypothesis testing is not about proving the hypothesis to be true or false. Instead, it’s about deciding whether the data provide sufficient evidence to reject the null hypothesis in favor of the alternative hypothesis. A measure to evaluate whether the conclusion of the hypothesis test is valid, is called power, and will be introduced in the following section. 5.2.1 Power Statistical power is a measure of the probability that a statistical test will detect a difference between two groups or treatments if one actually exists. It can also be described as the probability to NOT commit a Type II/\\(\\beta\\)-error. The statistical power of a hypothesis test is influenced by several factors, including the size of the sample, the magnitude of the difference between the groups or treatments being compared, and the level of significance (alpha) that is chosen for the test. A test with high statistical power is more likely to detect a difference between the groups or treatments being compared, while a test with low statistical power is less likely to detect a difference. In practice, power can be hard to calculate, as one needs to know the characteristics of the distribution describing the alternative hypothesis (e.g. mean, standard deviation, etc.). These characteristics are unknown, but are sometimes estimated using previous trials dealing with similar samples to estimate the power of a trial. 5.3 Confidence intervals A confidence interval is a range of values that is calculated from a sample of data, and it is used to estimate the true population parameter. It is called a confidence interval because it provides a level of confidence that the true population parameter falls within the range of values calculated from the sample. The size of the confidence interval depends on the size of the sample, the level of confidence chosen, and the variability of the data. The larger the sample size and the lower the variability, the smaller the confidence interval will be. Confidence intervals are commonly used in statistical analysis to estimate the mean, standard deviation, and other parameters of a population. Below is shown how to calculate the confidence interval of an estimated mean, assuming the the population follows a T-distribution. \\[CI_{\\mu}: \\hat{\\mu} \\pm t_{1-\\alpha/2,df} \\cdot \\hat{\\sigma}/\\sqrt{n}\\] The t-fratcile can be found in a T-table, or using qt(1-\\(\\alpha\\)/2,df) (which usually means qt(0.975, n-1)). \\(\\hat{\\sigma}\\) is the estimated standard deviation, and n is the number of samples used to calculate the mean. The confidence interval can also be used in hypothesis testing. For example, let’s say that the average height of men is 180cm, with a 95% confidence interval of \\(\\pm10cm\\). The null hypothesis is, that men and women have the same average height (\\(\\mu_{men} = \\mu_{women}\\)), whereas the alternative hypothesis is, that their average height is not the same (\\(\\mu_{men} \\neq \\mu_{women}\\)). If the height of women falls outside this confidence interval (meaning that the mean is larger than 190cm or lower than 170cm), one would be able to reject the null hypothesis, and conclude, that men and women do not have the same average height. 5.4 T-test A T-test is a statistical test that is used to determine whether there is a significant difference between the means of two groups. It is commonly used to compare the means of two groups that have been sampled from a larger population, to see if the groups are significantly different from one another. The hypotheses of a T-test: \\[ H_{0}: \\mu_{1}=\\mu_{2} \\\\ H_{A}: \\mu_{1}\\neq\\mu_{2}, \\ \\mu_{1}&gt;\\mu_{2} \\ or \\ \\mu_{1}&lt;\\mu_{2}\\] Which alternative hypothesis to choose depends on the question that one wants answered. There are several types of T-tests, including the independent samples T-test and the paired samples T-test. The independent samples T-test is used to compare the means of two separate groups, while the paired samples T-test is used to compare the means of two related groups, such as before and after measurements. To conduct a T-test in R, you can use the t.test() function. This function takes the following arguments: x and y: These are the two groups that you want to compare. They can be vectors or data frames. alternative: This specifies the alternative hypothesis. You can choose between “two.sided” (the default), “greater”, or “less”. mu: This specifies the hypothesized mean difference between the two groups. By default, it is set to 0. paired: This should be set to TRUE if you are conducting a paired samples t-test. Default is FALSE. var.equal: Can be TRUE or FALSE, depending on whether or not the variances of the two groups can be treated as equal. Default is FALSE. conf.level: The confidence level of choice. Default is 0.95. Here is an example using the BuffetSurvey data set: library(data4consumerscience) data(pasta) t.test(x = pasta$Pasta_with_legumes_is_visually_appealing, y = pasta$Pasta_with_mushrooms_is_visually_appealing) ## ## Welch Two Sample t-test ## ## data: pasta$Pasta_with_legumes_is_visually_appealing and pasta$Pasta_with_mushrooms_is_visually_appealing ## t = -3.0654, df = 103.39, p-value = 0.002774 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.152872 -0.247128 ## sample estimates: ## mean of x mean of y ## 5.5 6.2 # library(readxl) # BuffetSurvey &lt;- read_excel(&#39;~/Dropbox/DataAnalysis_ConsumerScience/dataanalyssisconsumerscience/DatasetRbook.xlsx&#39;, sheet = &#39;BuffetSurvey&#39;) # t.test(BuffetSurvey$`Pasta with legumes is visually appealing to me. `, # BuffetSurvey$`Pasta with mushrooms is visually appealing to me. `) This results in a p-value of 0.036, which is below the chosen \\(\\alpha\\)-level of 0.05. This means, that there is a significant difference between how visually appealing pasta with legumes and pasta with mushrooms perceived. 5.5 F-test An F-test is a statistical test that is used to compare the variance of two populations or samples. It is often used to test whether two groups have the same variance, or whether the variance of one group is significantly larger or smaller than the variance of another group. The hypotheses of an F-test: \\[ H_{0}: var_{1}=var_{2} \\\\ H_{A}: var_{1}\\neq var_{2}, \\ var_{1}&gt;var_{2} \\ or \\ var_{1}&lt;var_{2}\\] To conduct an F-test in R, you can use the var.test() function. This function takes two numeric vectors as input, and returns the F-value and p-value of the test. Here is an example using the BuffetSurvey data set: library(data4consumerscience) data(pasta) var.test(x = pasta$Pasta_with_legumes_is_visually_appealing, y = pasta$Pasta_with_mushrooms_is_visually_appealing) ## ## F test to compare two variances ## ## data: pasta$Pasta_with_legumes_is_visually_appealing and pasta$Pasta_with_mushrooms_is_visually_appealing ## F = 2.2049, num df = 59, denom df = 59, p-value = 0.002828 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 1.317018 3.691228 ## sample estimates: ## ratio of variances ## 2.204861 # library(readxl) # BuffetSurvey &lt;- read_excel(&#39;~/Dropbox/DataAnalysis_ConsumerScience/dataanalyssisconsumerscience/DatasetRbook.xlsx&#39;, sheet = &#39;BuffetSurvey&#39;) # var.test(BuffetSurvey$`Pasta with legumes is visually appealing to me. `, # BuffetSurvey$`Pasta with mushrooms is visually appealing to me. `) The p-value of the F-test is 0.037, which is below the chosen \\(\\alpha\\)-level of 0.05. This means, that the variance of the scores for how visually appealing pasta with legumes is significantly different from variance of the scores for how visually appealing pasta with mushrooms. - This actually confirms, that the correct T-test was used in the section above, since the T-test with unequal variance was used. While it is nice to use the F-test on two groups of samples, another very important statistical method uses the F-test to calculate its p-values - the ANOVA or Analysis of Variance, which will be introduced in the following section. 5.6 Analysis of Variance (ANOVA) ANOVA (Analysis of Variance) is a statistical method used to test the equality of means among more than two groups. But instead of directly comparing the observed means of the groups (which would lead to multiple tests), one can use get away with one test analyzing variance (hence the name). This is done by comparing the variance BETWEEN groups to the variance WITHIN groups. If the variance between the groups is significantly larger than the variance within the groups, we can conclude, that the mean of at least one of the groups is significantly different from the rest. To test whether the variances differ significantly, an F-test is used to compare the variances. If the p-value is below the selected \\(\\alpha\\)-level (often \\(\\alpha\\)=0.05) As hypotheses, it looks like this (k is the number of groups): \\[ H_{0}: \\mu_{1}=\\mu_{2} = ... =\\mu_{k} \\\\ H_{A}: At \\ least \\ one \\ mean \\ is \\ different\\] ANOVA can be performed using one-way ANOVA, multiple-way ANOVA, depending on the application, which can be seen below. 5.6.1 One-way ANOVA In a one-way ANOVA, we analyze the effect of one categorical factor on a response. This could e.g. be if country of origin has an impact on the alcohol content of the wine produced in that country, or if your dietary preference has an impact on your body weight. When conducting a one-way ANOVA, the model looks like this: \\[ Y_{ij} = \\mu + \\alpha(A_{i}) + e_{ij} \\\\ where \\ e_{ij} \\sim \\mathcal{N}(0,\\sigma^{2}) \\ and \\ independent \\\\ for \\ j=1,...,n_{i} \\ and \\ i=1,...,k \\] Here, \\(Y_{ij}\\) represents the jth observation of the ith treatment level (i = 1 to k and j = 1 to \\(n_i\\)).This means, that e.g., \\(Y_{23}\\) represents the 3rd observation of the 2nd factor. \\(\\mu\\) is the grand mean of the dataset, and \\(\\alpha\\) is effect of the i-th level of our factor A (e.g. Argentina or France as wine-producing countries, or pescetarian as your dietary preference). The ANOVA works, when the above-mentioned assumptions are true: The residuals (\\(e_{ij}\\)) are normally distributed around 0, and independent. A way to check the assumptions, is to use the built-in plot-function in R, and look at whether the data are normally distributed using the QQ-plot, and looking at the residuals, to check if the assumptions are viable. To show how to perform a one-way ANOVA in R, here is an example using the beerliking-dataset: library(data4consumerscience) data(&quot;beerliking&quot;) model &lt;- lm(data = beerliking, Liking ~ Beer) plot(model,which = c(1,2)) anova(model) ## Analysis of Variance Table ## ## Response: Liking ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Beer 5 110.41 22.0818 7.3939 8.269e-07 *** ## Residuals 914 2729.64 2.9865 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 First, the model is created using lm, specifying the dataset used, and the dependent and independent variables (Liking and Beer, respectively) The resulting table of the anova-function is called an ANOVA-table, and contains a lot of information about the data. But the column that we are most often looking at is Pr(&gt;F), which is another name for the p-value. If this value is below the chosen \\(\\alpha\\)-level (often \\(\\alpha\\)=0.05, which in this case it is), then we can conclude, that the judges like at least one of the Beers significantly better/worse than the rest. 5.6.2 Two-way ANOVA As the name suggests, the two-way ANOVA includes two categorical factors in the model instead of one in one-way ANOVA, and compares both factors’ (and their interaction) effects on the response. This could be the effect of e.g. both the country of origin as well as the grape variety on the liking of wine. For a two-way ANOVA with factor A with a levels, and factor B with b levels: \\[ Y_{ijk} = \\mu + \\alpha(A_{i}) + \\beta(B_{j}) + \\gamma(A_{i}\\times B_{j}) + e_{ijk} \\\\ where \\ e_{ijk} \\sim \\mathcal{N}(0,\\sigma^{2}) \\ and \\ independent \\\\ for \\ i=1,...,a \\ and \\ j=1,...,b \\ and \\ k=1,...,n_{ij} \\] Here, \\(Y_{ijk}\\) represents the kth observation of the ith level of factor A, and jth level of factor B.This means, that e.g., \\(Y_{234}\\) represents the 4th observation of the 2nd level of A and the 3rd level of B. \\(\\mu\\) is the grand mean of the dataset, \\(\\alpha\\) is the effect of the ith level of factor A (e.g. Argentina or France as wine-producing countries), \\(\\beta\\) is the effect of the jth level of factor B (e.g. Pinot Gris, Chardonnay, Riesling, etc.) and \\(\\gamma\\) is the interaction effect between factor A and B (e.g. French Chardonnay or German Riesling). As with the one-way ANOVA, the same assumptions about normality and independence of the residuals has to hold, and these can again be checked using plot as shown below. However, the anova-function should be used with caution, when model have more than one independent variable. This is the case, because, anova is performing what is commonly referred to as a Type I ANOVA, also called a sequential ANOVA, where the factors are tested in the specified order. Theoretically, this could be what you want, but in most cases we are interested in the effect of a factor, regardless of order. This is called a Type II ANOVA, and in R it can be performed using drop1, as shown below in model2. However, if an interaction effect is present (e.g. if the effect of Riesling is enhanced by the wine originating from Germany), then a Type II ANOVA only returns the effect of this interaction, due to the principle of marginality. This implies, that if an interaction is in fact present, the “simple” main effects (of e.g. country and grape alone) are poor estimators of the response. If the interaction is non-significant, however, it should be removed from the model, and we end up with a Type II model with the main effects. If for some reason we are interested in both the main effects and the interaction (if the interaction effect is significant), we can perform what is called a Type III ANOVA. This will test all effects against a model without said effect. As this will over-parameterise the model, in R, one has to choose a contrasts setting that sums to zero, otherwise the ANOVA analysis will give incorrect results. This is what is done with options. Then model3 is created to also test the interaction, and in drop1 it is specified, that we want all model components to be tested (Type III ANOVA), using .~.. If there is no significant interaction effect, a Type II ANOVA is a stronger test, which is why one should choose it if possible. More on the different types of ANOVA here and here. library(data4consumerscience) data(beerliking) model2 &lt;- lm(data = beerliking, Liking ~ Beer + Age) plot(model2,which = c(1,2)) drop1(model2, test = &#39;F&#39;) ## Single term deletions ## ## Model: ## Liking ~ Beer + Age ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 2692.2 1011.8 ## Beer 5 110.948 2803.1 1039.0 7.4841 6.784e-07 *** ## Age 6 37.493 2729.6 1012.5 2.1076 0.0501 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 options(contrasts = c(&#39;contr.sum&#39;,&#39;contr.poly&#39;)) model3 &lt;- lm(data = beerliking, Liking ~ Beer + Age + Beer:Age) plot(model3,which = c(1,2)) drop1(model3, .~., test = &#39;F&#39;) ## Single term deletions ## ## Model: ## Liking ~ Beer + Age + Beer:Age ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 2604.1 1041.2 ## Beer 5 57.549 2661.6 1051.3 3.8807 0.001745 ** ## Age 6 37.158 2641.2 1042.2 2.0880 0.052306 . ## Beer:Age 30 88.089 2692.2 1011.8 0.9900 0.482521 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It looks like we have no significant interaction effect, since the p-value = 0.48 &gt; \\(\\alpha\\) (0.05). This means, that we can look at the Type II-analysis. Here, it shows that Beer-type is very important for the Liking score, since the p-value = 6.784e-07, which is way lower than \\(\\alpha\\). Age is very close to \\(\\alpha\\), which suggests, that even though the effect is technically non-significant, it might be worth looking into whether all ages like beer the same. 5.6.3 Post hoc test - Tukey’s Honest Significant Difference After the ANOVA, we might have concluded, that at least one of the groups of at least one of the factors is significantly different from the rest. Now you would like to know which one(s) of the groups this significant difference originates from. This is where the Tukey’s Honest Significant Difference (in short, Tukey test) is very useful. To compare more than two groups, one would have to conduct multiple pairwise T-tests. This does, however, not hold up, as probabilities are cumulative, which means that while the first test would yield a p-value lower than \\(\\alpha\\) (often 0.05), when conducting multiple tests, the cumulative p-value could exceed \\(\\alpha\\). A Tukey test corrects for this, and is therefore a better fit when dealing with more than two groups. There are several ways to conduct a Tukey test in R, but the one that works in most usecases is the one shown below, using the multcomp-package and the glht-function. The dataset used here is the beerliking-dataset. library(multcomp) library(data4consumerscience) data(&quot;beerliking&quot;) beerliking$Beer &lt;- as.factor(beerliking$Beer) model &lt;- lm(data = beerliking, Liking ~ Beer) summary(glht(model, linfct = mcp(Beer = &quot;Tukey&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lm(formula = Liking ~ Beer, data = beerliking) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## NY Lager - Brown Ale == 0 -6.399e-01 1.979e-01 -3.233 0.01600 * ## Porse Bock - Brown Ale == 0 -8.364e-01 1.976e-01 -4.233 &lt; 0.001 *** ## Ravnsborg Red - Brown Ale == 0 -2.325e-01 1.976e-01 -1.177 0.84802 ## River Beer - Brown Ale == 0 -8.364e-01 1.976e-01 -4.233 &lt; 0.001 *** ## Wheat IPA - Brown Ale == 0 -9.405e-01 1.979e-01 -4.752 &lt; 0.001 *** ## Porse Bock - NY Lager == 0 -1.965e-01 1.973e-01 -0.996 0.91921 ## Ravnsborg Red - NY Lager == 0 4.074e-01 1.973e-01 2.065 0.30656 ## River Beer - NY Lager == 0 -1.965e-01 1.973e-01 -0.996 0.91920 ## Wheat IPA - NY Lager == 0 -3.007e-01 1.976e-01 -1.522 0.65031 ## Ravnsborg Red - Porse Bock == 0 6.039e-01 1.969e-01 3.066 0.02704 * ## River Beer - Porse Bock == 0 -2.831e-15 1.969e-01 0.000 1.00000 ## Wheat IPA - Porse Bock == 0 -1.042e-01 1.973e-01 -0.528 0.99506 ## River Beer - Ravnsborg Red == 0 -6.039e-01 1.969e-01 -3.066 0.02698 * ## Wheat IPA - Ravnsborg Red == 0 -7.080e-01 1.973e-01 -3.589 0.00476 ** ## Wheat IPA - River Beer == 0 -1.042e-01 1.973e-01 -0.528 0.99506 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) cld(glht(model, linfct = mcp(Beer = &quot;Tukey&quot;))) ## Brown Ale NY Lager Porse Bock Ravnsborg Red River Beer ## &quot;a&quot; &quot;bc&quot; &quot;c&quot; &quot;ab&quot; &quot;c&quot; ## Wheat IPA ## &quot;c&quot; When inserting the glht-object into summary, the individual, pairwise comparisons are shown, with the adjusted p-values. Another (sometimes easier to interpret) way of displaying the pairwise comparison is by the use of letters, as is shown using cld. When groups have different letter, they are significantly different from one another, while groups sharing a letter means no significant difference between the two. Here, we can see, that Brown Ale has scored significantly higher than Wheat IPA, since they have been assigned with c and a respectively. But the Brown Ale is not significantly different from the Ravnsborg Red, as they both have been assigned with c’s. 5.7 Introduction to linear and mixed models Linear models are one of the most used statistical methods. The definition is that the response is linear in the parameters. This means, that it one can see it as an extention of the ANOVA-models already described, but with the independent variable now being continuos rather than categorical. If \\(y\\) is the response, and \\(x\\) is the predictor, then both of the models below are linear models \\[y = a + b\\cdot x + e\\] \\[y = a + b\\cdot x + c\\cdot x^2 + e\\] Here you see that the response is linear in the parameters \\(a,b,c\\). I.e. it has nothing to do with being linear in the predictor. 5.8 Normal and Mixed models 5.8.1 Normal model In a normal linear model such as: \\[y = a + b\\cdot x + e\\] The assumption is that the uncertainty is captured by one entry, namely the residuals (\\(e\\)). For instance, the relation between Hunger and the intake AdLibg of dishes with Capsaicin can be visualized and modeled by: library(tidyverse) library(data4consumerscience) data(&#39;chili&#39;) x &lt;- chili %&gt;% # only include a single treatment filter(Treatment==&#39;Capsaicin&#39;) %&gt;% # only include the first trial for each judge filter(!duplicated(Judge)) ggplot(data = x, aes(x = Hunger, y = AdLibg)) + geom_point() + stat_smooth(method = lm, se = F) Naturally, the more hungry, the higher the intake. A model describing this relation: mdl &lt;- lm(data = x, AdLibg~Hunger) summary(mdl) ## ## Call: ## lm(formula = AdLibg ~ Hunger, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -347.34 -106.69 -18.06 133.88 341.60 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 379.826 73.814 5.146 2.56e-05 *** ## Hunger 3.447 1.078 3.196 0.00375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 190.5 on 25 degrees of freedom ## Multiple R-squared: 0.2901, Adjusted R-squared: 0.2617 ## F-statistic: 10.21 on 1 and 25 DF, p-value: 0.003753 Here we see that consumption increases by \\(3.45g\\) per increase in \\(1\\) hunger scale, and that this slope has a standard error of \\(1.08g\\). Further, at Hunger=0 the intake is \\(379.8g\\). Furhter, we see that this relation is significant \\(p = 0.0036\\). More details on the use of linear models in R and how-to can be viewed in these videos: 5.8.2 Mixed model A mixed model refers to the situation, where more than one part of the model is handling the uncertainty. For instance, in the chili data set there are two instances for each judge, and hence the uncertainty can be split into between judges and within judges. In this plot the intake is shown across products (Treatment) and labelled with the Judge number. For instance, Judge 1 is in general high and 24 generally low. Further, the plot is splitted according to the two test-repetition (First: TestDays = 1,..,5, Second: TestDays = 6,..,10). library(tidyverse) ggplot(data = chili, aes(x = Hunger, y = AdLibg, color = Treatment)) + geom_text(aes(label = Judge)) + stat_smooth(method = lm, se = F) + facet_wrap(~TestDay&gt;5) The above-mentioned structure is encoded in the modelling. lmer is used, when creating a mixed effect model. The judges and the test day are considered random effects, and are assigned to be so by adding a 1 and a horizontal line when creating the model. library(lme4) library(lmerTest) chili$TestDay2 &lt;- factor(chili$TestDay&gt;5) # adding a new testday variable mdlmix &lt;- lmer(data = chili, AdLibg ~ Hunger*Treatment + (1|Judge) + (1|(TestDay2))) summary(mdlmix) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: AdLibg ~ Hunger * Treatment + (1 | Judge) + (1 | (TestDay2)) ## Data: chili ## ## REML criterion at convergence: 3414.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.78687 -0.60988 0.01365 0.51858 2.85224 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Judge (Intercept) 27011 164.35 ## (TestDay2) (Intercept) 2974 54.53 ## Residual 16870 129.89 ## Number of obs: 269, groups: Judge, 27; (TestDay2), 2 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 352.28066 52.75937 3.16773 6.677 0.00576 ** ## Hunger 2.92939 0.24957 238.99649 11.738 &lt; 2e-16 *** ## Treatment1 -18.77367 31.47373 232.74410 -0.596 0.55143 ## Treatment2 5.88641 30.88143 232.74962 0.191 0.84899 ## Treatment3 -11.29847 33.80720 232.71363 -0.334 0.73853 ## Treatment4 -17.75656 29.41156 232.57574 -0.604 0.54661 ## Hunger:Treatment1 0.53290 0.45930 233.05709 1.160 0.24714 ## Hunger:Treatment2 -0.35878 0.44660 233.00610 -0.803 0.42259 ## Hunger:Treatment3 0.03973 0.46252 233.14079 0.086 0.93163 ## Hunger:Treatment4 -0.28247 0.43373 232.88188 -0.651 0.51551 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Hunger Trtmn1 Trtmn2 Trtmn3 Trtmn4 Hng:T1 Hng:T2 Hng:T3 ## Hunger -0.289 ## Treatment1 -0.007 0.002 ## Treatment2 -0.013 0.028 -0.220 ## Treatment3 0.014 -0.018 -0.269 -0.262 ## Treatment4 -0.033 0.081 -0.214 -0.198 -0.238 ## Hngr:Trtmn1 -0.001 0.022 -0.865 0.192 0.229 0.194 ## Hngr:Trtmn2 0.009 -0.012 0.196 -0.856 0.218 0.171 -0.239 ## Hngr:Trtmn3 -0.004 -0.021 0.241 0.231 -0.881 0.208 -0.263 -0.253 ## Hngr:Trtmn4 0.023 -0.041 0.192 0.169 0.193 -0.841 -0.247 -0.222 -0.233 The summary spits out the model estimates, and especially the random effects shows that the within individual residual variation is \\(130g\\) while the between individual variation is larger: \\(164g\\). I.e. the consumption is more depend on the individual than the repetitions. Further, the testday also has a little effect (\\(54g\\)). We can evaluate the systematic effect overall by anova. When anova is used on a model created by lmer, it conducts a Type III ANOVA. anova(mdlmix) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Hunger 2324398 2324398 1 239.00 137.7801 &lt;2e-16 *** ## Treatment 31278 7820 4 232.75 0.4635 0.7625 ## Hunger:Treatment 33035 8259 4 233.05 0.4895 0.7434 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This shows that Hunger indeed will make you eat more, but the slopes and offsets in relation to the different products is non-significant. To learn more and see how to conduct the analysis in R, see here: More on ANOVA and mixed models "],["plotting-data.html", "Chapter 6 Plotting data 6.1 Histograms and boxplots 6.2 Scatter plots 6.3 How to export plots", " Chapter 6 Plotting data Visualizing data is of utmost importance. Especially, looking at the raw data will enable you to point towards outliers and tendencies which may be lost when representing the data with descriptive statistics. ggplot2 is a wide spread library for plotting data and used by a lot of users. Check it out on videos: There is a very condensed cheat-sheet for ggplot2 on RStudios webpage Loading the needed datasets chili and pasta: library(data4consumerscience) data(chili) data(pasta) # we subset to only have the &quot;Pasta with legumes&quot; data. pasta &lt;- pasta[pasta$StationName==&#39;Pasta with legumes&#39;,] 6.1 Histograms and boxplots To show the total consumption a histogram is useful library(ggplot2) ggplot(data = chili, aes(x = AdLibg)) + geom_histogram() The plot reveals a bi-modal distribution with an upper tail. These observations comes from 5 different treatments, 27 different judges and at 10 test days. We can infer this using a boxplot with jittered raw data on top. Per treatment: ggplot(data = chili, aes(x = Treatment, y = AdLibg)) + geom_boxplot() + geom_jitter() Per Judge: ggplot(data = chili, aes(x = factor(Judge), y = AdLibg)) + geom_boxplot() + geom_jitter() None of these explains the bi-modality seen in the histogram. However, we can combine test-day information, which essentially is reflecting whether it is first or second trial splitting at day 5: (TestDay&gt;5) ggplot(data = chili, aes(x = factor(Judge), y = AdLibg, color = factor(TestDay&gt;5))) + geom_boxplot(aes(group = factor(Judge))) + geom_jitter() This coloring shows a clear difference between trials, which explains the bi-modal distribution. It also explains why the two modes of the distribution are overlapping, and why the distribution is skewed. Because while most judges have consumed more during the first trial, some judges did the opposite - higher consumption during the second trial. 6.2 Scatter plots Lets plot the consumption (AdLibg) as a function of LikingAppearance, and add a tendency line: ggplot(data = chili, aes(x = LikingAppearance, y = AdLibg)) + geom_point() + stat_smooth(formula = &#39;y ~ x&#39;) This plot shows AdLibg on the y-axis plotted against LikingApperance on the x-axis, with “stat_smooth()” adding a tendency line. Try adding “method = lm” inside the brackets of stat_smooth(), and see how the line changes. Now, lets funk this up by splitting into test-days and get colors according to product: ggplot(data = chili, aes(x = Hunger, y = AdLibg, color = Treatment)) + geom_point() + stat_smooth(formula = &#39;y ~ x&#39;, se = F, method = lm) + facet_wrap(~ TestDay&gt;5) The plot shows, that the tendency of food intake increasing with increased hunger, is true for all treatments, as well as duriung both trial days. 6.3 How to export plots The plots shown in the Plots pane (lower right of RStudio) can be saved using the Export bottom. You can also save the plots to a file using the ggsave() function. If you run ggsave() without specifying which plot to export, it will use the latest. You can also directly specify the plot to export: myplot &lt;- ggplot(data = chili, aes(Hunger,AdLibg, color = Treatment)) + geom_point() + stat_smooth(se = F, method = lm) + facet_wrap(~ TestDay&gt;5) ggsave(&#39;./data/YourFolderForNicePlots/hunger_vs_consumption.pdf&#39;,myplot) ggsave() supports different formats (.png, .tiff, .pdf,…) and further allows for editing the size (height = , width = ). Just change the .pdf in to the format your need. "],["introduction-to-pca-and-multivariate-data.html", "Chapter 7 Introduction to PCA and multivariate data 7.1 A bit of math 7.2 Interpreting model output", " Chapter 7 Introduction to PCA and multivariate data Multivariate data is defined as a set of (multiple) response variables measured on the same set of samples. In principle these response variables can be of any nature (continuous, ordinal, binary), but mostly PCA is used for analysis of continuous variables. In this book Principal Component Analysis (PCA) is used several times. This chapter will shortly explain the theory behind PCA and the interpretation of relevant plots. PCA is a tool for looking a correlation structure between variables, and groupings of samples. All through visualizations. Check out YouTube on the subject for an introduction. A conceptual introduction is given here: 7.1 A bit of math The multivariate dataset is organized in a matrix \\(\\mathbf{X}\\) with \\(n\\) samples and \\(p\\) variables. This matrix is factorized into so called scores (\\(\\mathbf{T}\\)) and loadings (\\(\\mathbf{P}\\)). \\[ \\mathbf{X} = \\mathbf{T}\\mathbf{P} + \\mathbf{E} \\] This estimation is done such that the residuals (\\(\\mathbf{E}\\)) is minimized in a least squares sense. The upside of using PCA is that we characterize the sample distribution (what is similar and different) using plots of \\(\\mathbf{T}\\), and the correlation of the responses using plots of \\(\\mathbf{P}\\). In addition to PCA there exists a range of methods for factorizing multivariate datasets including Partial Least Squares (PLS), confirmatory factor analysis (CFA), Correspondence Analysis (CA), Redundancy Analysis (RA), and a lot more. Conceptually, all these methods aims to make a latent-factor model just like PCA, and hence understanding the idea and especially how PCA is used, opens up for using a wide range of variants. 7.2 Interpreting model output For calculating a PCA model, we will use the dataset beef. It is a sensory descriptive profile on vacuum packed beef steaks heat treated in water baths at different temperatures for different times (sous vide) In total 12 combinations. The samples are coded as e.g. 56-03, meaning the sample has been treated for 56 degrees for 3 hours. There are 10 assessors in the panel, four sensory repetitions and 22 attrributes (A-Brownsurface to T-Salt). The first 3 columns of the dataset is used for the design. The PCA is computed on the response variables only. These can be chosen using the hard brackets []. The way to chose only a part of a dataset (to subset) is done by specifying the following inside the []: A dataset often has two dimensions, rows and columns. In this case, the [] takes two inputs: which row you want, and which columns you want. These are seperated by a comma. This means, that if you write: dataset[1:4,2:6], the result will be rows 1-4, of columns 2-6. If nothing is written on one side of the comma, all of the rows/columns are chosen. library(data4consumerscience) data(beef) mdlPCA &lt;- prcomp(beef[,4:25], scale. = T, center = T) 7.2.1 Biplot The ggbiplot package (and function) is a needed tool to plot the PCA model. Installation of this is done by: install.packages(&quot;devtools&quot;) devtools::install_github(&#39;vqv/ggbiplot&#39;) ggbiplot::ggbiplot(mdlPCA) The PCA tries to capture the maximum structure of the data in a few dimensions called principal components (PC1, PC2,…). The arrows and labels reflects the correlation structure between the responses. I.e we see that the Bouillon characteristics is correlated and explains PC2, while textural properties like Rubberband, Chewtime is correlated, and oppositely correlated of Tender, and collectively explains PC1. We can decorate the scores with design information, such as beef type as well as judge ggbiplot::ggbiplot(mdlPCA, groups = beef$ProductName, ellipse = T) All though this plot is a bit messy, we see that higher temperature promotes higher position on PC2: I.e. moving from bloody to bouillon. While PC1 to some extend captures trends in cooking time. ggbiplot::ggbiplot(mdlPCA, groups = beef$Assessor, ellipse = T) Coloring according to judge reveal that judge A04 is low in range (variance), and hence do not utilize the entire response scale, A03 tends to rate higher on rubberband, chewtime, bloodmetal, etc. especilly compared to A10, which is higher in bouillon / PC2. This plot indicates how well trained the panel is, as we optimally have only smaller impact of the individual judges. NOTE: If the ggbiplot() needs modification, for instance, if the loading labels are exceeding the boundaries, or are too small, then look into the ggbiplot arguments such as varname.size. Also normal ggplot2 functionallity like xlim(), ylim() or theme() can help to modify the plot. "],["buffet-and-survey-data.html", "Chapter 8 Buffet and survey data 8.1 Buffet data 8.2 Survey data 8.3 Combining consumption and survey data 8.4 PCA on survey answers", " Chapter 8 Buffet and survey data 8.1 Buffet data 8.1.1 Introduction to buffet data In the Meal Systems and Technologies course, we work with data from the iBuffet and data from a survey. You have to merge the two datasets. You can do this in Excel or in R. For the Excel guide, please see the teaching materials of the course in Absalon. For the R guide, please see How to edit and merge datasets This chapter is missing a “Table 1”, as the data is insufficient in this part. Our data analysis process for buffet and survey data is described below for a very simple dataset. Please elaborate plots and models to fit your own data. First, it is a good idea to have a look at the dataset, see what the columns and rows consist of - it could also be a good idea to do some descriptive statistics. All of this is covered in the following chapters: Looking at the imported elements and Descriptive statistics. Often however, it is much easier to understand the data when they are plotted. 8.1.2 Plotting buffet data Below is one way to plot the data of the consumption from the buffet: library(data4consumerscience) library(ggplot2) data(pasta) ggplot(data = pasta, aes(x = factor(Day), y = Consumption)) + geom_boxplot() + geom_point(aes(color = Person)) + geom_line(aes(group = Person, color = Person)) + facet_wrap(~StationName) + xlab(&#39;Day&#39;) The plot shows a boxplot of the consumption for each day and for each serving station, and each point is the consumption of one person from one station. The colors are added to make it easier to see, how the consumption of the same person changes from day to day, as well as for each of the stations. For more information on plotting, see Plotting data. Both the days and the stations look quite similar, but it also looks as if the variation in consumption is dependent on person. To test whether this is the case, we create a mixed model. 8.1.3 Mixed model for buffet data The reason we create a mixed model is that we want to isolate the effect of day and station from the variation caused by which person actually consumed the food. This is done in the example below. For more information on mixed models, see Mixed model. We use the packages lme4 and lmerTest, and use the data called pasta from the data4consumerscience-pacakge. The function lmer will create a mixed model when set up as below. This will yield how Day and StationName affect Consumption. When using ’*’ between Day and StationName, we tell R that we want both the effect of Day and StationName alone, as well as their interaction effect - which could be e.g. if 10 people tasted the pasta with mushrooms on day 1 and decided that they like this station better, and as a result increase consumption on day 2. library(lme4) library(lmerTest) library(data4consumerscience) data(pasta) mdlmix &lt;- lmer(data = pasta, Consumption ~ factor(Day)*StationName + (1|Person)) summary(mdlmix) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Consumption ~ factor(Day) * StationName + (1 | Person) ## Data: pasta ## ## REML criterion at convergence: 663.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4856 -0.6552 -0.1166 0.5527 2.3379 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Person (Intercept) 3263 57.12 ## Residual 4314 65.68 ## Number of obs: 60, groups: Person, 15 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 138.000 17.012 14.000 8.112 1.16e-06 *** ## factor(Day)1 -4.333 8.479 42.000 -0.511 0.612 ## StationName1 6.000 8.479 42.000 0.708 0.483 ## factor(Day)1:StationName1 -7.400 8.479 42.000 -0.873 0.388 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fc(D)1 SttnN1 ## factor(Dy)1 0.000 ## StationNam1 0.000 0.000 ## fct(D)1:SN1 0.000 0.000 0.000 What we often look at in an output like this is the p-values (Pr(&gt;|t|)), which indicate whether or not the variable affects the model. The intercept is irrelevant in this case, as it tests whether or not the intercept is 0, and this is not relevant. What we observe is, that none of the p-values are below 0.05, which is the usual significance level used. This means, that neither day nor station has a significant impact on consumption, even when correcting for the variation caused by different people eating at the buffet. 8.2 Survey data Survey data are (as the name implies) the result of a survey, where the questions can vary dramatically. The dataset used here does not have a lot of questions, so please feel free to elaborate on plots and models when analyzing your own data. The same general advice applies here as in the case of the buffet data - inspect your data, calculate some descriptive statistics, and plot the data, to get a better understanding of the data. 8.2.1 Plotting survey data Below are some examples of plots. First, a barplot showing the whether or not the participants of the survey considered the protein content of the dishes, split between day 1 and 2. Secondly, a histogram of the liking scores for each station, shown as the number of people giving a certain score on the Likert-scale. The histogram can be created by creating each plot separately, assigning each plot to a variable, and then arranging them next to each other using grid.arrange from the gridExtra-package. It can also be created using what is called a ‘pipe operator’ (%&gt;%), which is a function from within the tidyverse-package, and with a little help from the data.table-package. The pipe operator is a nice tool to reduce the amount of code and variable names needed, and can be used for a lot of different stuff. For example, filter can be used to select specific rows, as is done here, where we select rows in column ‘Question’, that include the text ‘like’, and as the questions about liking are the only questions containing this text, these are the rows selected. To learn more about the pipe operator, see Edit using Tidyverse. Here, there is also an explanation of the other functions such as gather. library(ggplot2) library(data4consumerscience) data(pasta) ggplot(data = pasta, aes(x = Did_you_consider_the_proteincontent_of_the_dishes_you_choose)) + geom_bar() + facet_wrap(~factor(Day)) #Arranging to plots together library(gridExtra) legumes &lt;- ggplot(data = pasta, aes(x = I_like_taste_of_pasta_with_legumes)) + geom_histogram() + ggtitle(&#39;Legumes - liking&#39;) mushrooms &lt;- ggplot(data = pasta, aes(x = I_like_taste_of_pasta_with_mushrooms)) + geom_histogram() + ggtitle(&#39;Mushrooms - liking&#39;) grid.arrange(legumes,mushrooms, nrow = 1) #The fancy way: library(tidyverse) library(data.table) pasta %&gt;% gather(Question, Answer, I_like_taste_of_pasta_with_legumes:Pasta_with_mushrooms_is_visually_appealing) %&gt;% filter(., Question %like% &#39;like&#39;) %&gt;% ggplot(data = ., aes(x = Answer)) + geom_histogram() + facet_wrap(~Question) Another nice plot is a bivariate plot, plotting two variables against each other and adding a regression line (using geom_smooth, and specifying ‘lm’ for a linear regression for ‘y ~ x’). This has been done below, to see if there is a correlation between the liking and the rating of the visual apprerance for pasta with legumes: library(ggplot2) ggplot(data = pasta, aes(x = Pasta_with_legumes_is_visually_appealing, y = I_like_taste_of_pasta_with_legumes)) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = &#39;y ~ x&#39;) 8.2.2 Linear model for buffet data You can also create a linear regression model explaining the relationship between liking and visual apperance, as is done below using the lm-function. mdl &lt;- lm(data = pasta, I_like_taste_of_pasta_with_legumes ~ Pasta_with_legumes_is_visually_appealing) summary(mdl) ## ## Call: ## lm(formula = I_like_taste_of_pasta_with_legumes ~ Pasta_with_legumes_is_visually_appealing, ## data = pasta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.2199 -0.5664 0.1265 0.6068 1.6068 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.08609 0.41462 2.62 0.0112 ## Pasta_with_legumes_is_visually_appealing 0.82677 0.07288 11.35 2.36e-16 ## ## (Intercept) * ## Pasta_with_legumes_is_visually_appealing *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8213 on 58 degrees of freedom ## Multiple R-squared: 0.6893, Adjusted R-squared: 0.684 ## F-statistic: 128.7 on 1 and 58 DF, p-value: 2.356e-16 The summary-function will give a lot of information about the model, e.g. the p-value, which is much lower than 0.05 (it is 2.36e-16), showing that the visual rating and the liking are indeed highly correlated. Also, the estimates of the summary are in fact the estimates of the parameters of the regression line, that follows \\(y = a + b\\cdot x\\). The estimate is: \\(\\hat{y}=0.82677 \\cdot x \\ + 1.08609\\). Have a look at the plot and see if it fits with the regression line we created before. To learn more about linear models, see Normal and Mixed models 8.2.3 Post-hoc test for survey data One can also investigate whether two or more discrete groups are different from one another. The model is created the same way as above for the linear model, with the exception that our x is now discrete (‘Why_did_you_consider_the_proteincontent’). A very useful outcome of a model like this is the pairwise comparison between groups - in this case the different considerations about the protein content of the dish. The pairwise comparison compares all the groups (here the considerations), and the result is a letter assigned to each group. If two groups have different letters, the two groups are significantly different from each other, whereas if two groups have the same letter, they are not. library(multcomp) pasta$Why_did_you_consider_the_proteincontent &lt;- as.factor(pasta$Why_did_you_consider_the_proteincontent) model &lt;- lm(data = pasta, I_like_taste_of_pasta_with_legumes ~ Why_did_you_consider_the_proteincontent) cld(glht(model, linfct = mcp(Why_did_you_consider_the_proteincontent = &quot;Tukey&quot;))) ## Because I know that I need protein for my overall health ## &quot;a&quot; ## Because I try to follow the Danish Dietary Guidelines  ## &quot;a&quot; ## I did not consider this  ## &quot;a&quot; In our case, the consideration about protein content did not affect the liking of pasta with legumes, as all groups have been assigned with the same letter. For more information on the post-hoc test, see Post hoc test - Tukey’s Honest Significant Difference. 8.3 Combining consumption and survey data It is of course interesting to investigate how the consumption and the survey correlate. One example of how to look at this can be seen below. First, the data from the ‘Pasta with legumes’-station are plotted in a bivariate plot, where consumption is plotted against liking of pasta with legumes. Next, a linear model is created to test whether the correlation is significant, and to see the estimates. library(ggplot2) #Bivariate plot ggplot(data = pasta[pasta$StationName==&#39;Pasta with legumes&#39;,], aes(x = I_like_taste_of_pasta_with_legumes, y = Consumption)) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = &#39;y ~ x&#39;) #Model library(tidyverse) library(data.table) fullmodel &lt;- pasta %&gt;% filter(.,StationName %like% &#39;legumes&#39;) %&gt;% lm(data = ., Consumption ~ I_like_taste_of_pasta_with_legumes) summary(fullmodel) ## ## Call: ## lm(formula = Consumption ~ I_like_taste_of_pasta_with_legumes, ## data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -108.274 -63.415 -9.345 36.944 187.726 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -46.740 57.942 -0.807 0.42665 ## I_like_taste_of_pasta_with_legumes 33.859 9.961 3.399 0.00205 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 79.05 on 28 degrees of freedom ## Multiple R-squared: 0.2921, Adjusted R-squared: 0.2668 ## F-statistic: 11.55 on 1 and 28 DF, p-value: 0.002048 The model shows, that indeed, the liking of pasta with legumes affects the consumption of pasta with legumes (as the p-value is 0.00205, which is below 0.05). Also shown are the estimates of the regression: \\(\\hat{y}= 33.859\\cdot x \\ -46.740\\) 8.4 PCA on survey answers We use the ggbiplot package for plotting the PCA model (See Biplot for details). library(data4consumerscience) library(tidyverse) library(ggbiplot) 8.4.1 Wrangle data PCA takes numerical data as input, so we use the likert-scales in the form of 1 to 7. Further the yes/no answers are included, and also needs to be changed. x &lt;- pasta %&gt;% mutate(Did_you_take_food_from_both_Dish1_and_Dish2 = Did_you_take_food_from_both_Dish1_and_Dish2 %&gt;% factor %&gt;% as.numeric(), Did_you_consider_the_proteincontent_of_the_dishes_you_choose = Did_you_consider_the_proteincontent_of_the_dishes_you_choose %&gt;% factor() %&gt;% as.numeric()) %&gt;% mutate_if(is.factor, as.numeric) %&gt;% filter(Day==1) %&gt;% # the survey part is the same for both days and both stations. That is what we keep. filter(str_detect(StationName,&#39;leg&#39;)) 8.4.2 Build the model PCAmdl &lt;- prcomp(x[,c(5:6,8:11)],scale. = T) 8.4.3 Bi-plot And a plot of the model ggbiplot(PCAmdl, varname.size = 5) + ylim(c(-4,4)) + xlim(c(-2,5)) What does component 1 (PC1) reflect? What does PC2 reflect? Lets plot the model and color the samples according to the consumption (of legumes) cutted at the median. ggbiplot(PCAmdl, groups = factor(x$Consumption&gt;130), ellipse = T, varname.size = 5) + ylim(c(-4,4)) + xlim(c(-3,5)) 8.4.4 Extract the components and run all associations. We are interested in if any of the likert/survey traits reflected by PCA is correlated with consumption. It is a little complicated, but here goes library(broom) library(broom.mixed) library(lme4) scores &lt;- data.frame(Person = x$Person, PCAmdl$x[,1:2]) # take out the first two components. tbmixed &lt;- pasta %&gt;% left_join(scores, by = &#39;Person&#39;) %&gt;% gather(comp,score,PC1:PC2) %&gt;% group_by(StationName,comp) %&gt;% do(lmer(data = ., Consumption~score + Day + (1|Person)) %&gt;% tidy(conf.int = T)) … Make a table and a plot of the results. library(knitr) tbmixed %&gt;% filter(term==&#39;score&#39;) %&gt;% dplyr::select(-effect,-group) %&gt;% kable(x = .,caption = &#39;Slopes according to components&#39;, digits = 2, format = &#39;simple&#39;) Table 8.1: Slopes according to components StationName comp term estimate std.error statistic df p.value conf.low conf.high Pasta with legumes PC1 score 14.28 13.26 1.08 13 0.30 -14.36 42.93 Pasta with legumes PC2 score -19.47 17.80 -1.09 13 0.29 -57.91 18.98 Pasta with mushroom PC1 score 5.15 10.69 0.48 13 0.64 -17.94 28.25 Pasta with mushroom PC2 score -4.93 14.43 -0.34 13 0.74 -36.11 26.24 tbmixed %&gt;% filter(term==&#39;score&#39;) %&gt;% ggplot(data = ., aes(comp,estimate,ymin = conf.low, ymax = conf.high)) + geom_errorbar(width = 0.1) +geom_point()+ geom_hline(yintercept = 0) + facet_grid(~StationName) + theme(legend.position = &#39;bottom&#39;) Interpret the results. "],["mst-exercises.html", "Chapter 9 MST exercises 9.1 Exercise 1 9.2 Exercise 2 9.3 Exercise 3", " Chapter 9 MST exercises INTRO TEXT 9.1 Exercise 1 SE I EDITOR FRA 2022 + KURSUSMATERIALE 9.2 Exercise 2 9.3 Exercise 3 "],["intro-to-large-survey-data.html", "Chapter 10 Intro to large survey data 10.1 Descriptive statistics 10.2 Plots 10.3 Categorical / Ordinal variables 10.4 Table 1 10.5 The tidyverse way", " Chapter 10 Intro to large survey data Generelt: Der skal skrives forklaringer, andet mangler der ikke her The data used in this chapter are based on surveys conducted during earlier iterations of the courses Food Consumer Research and Meal Consumer Research. Large survey data can be collected using SurveyXact. There are tools (packages for R) which can import SurveyXact outputs directly, while we do not go through these here, googl’ing a bit may be helpful. Alternatively, the SurveyXact files can be exported to .xlsx files and imported as indicated in How to import data. Here we have included the data in the data4consumerscience package ready for use. library(data4consumerscience) data(&quot;fmcrsurvey&quot;) 10.0.1 Looking at the data and checking formating The functions head(), View(), and str() can be used to get an overview of formating. See Looking at the imported elements for details. A fair bunch of the variables are characters (chr) because they are recorded as levels on a likert-scale. However, you may want to also have those as numeric values from \\(1\\) to \\(5\\) or \\(7\\). The code below does the job. The key thing to note here is that if the factor levels are not specified, then they will be ordered according to the alphabet. library(tidyverse) # The coloumns in the dataset which are 7 point likert-scales cols &lt;- colnames(fmcrsurvey)[7:10] # Use the mutate_at() function to make the variables into factors, and setting the levels in the right order. fmcrsurvey &lt;- fmcrsurvey %&gt;% mutate_at(cols, funs(factor(.,ordered = T, levels = c(&#39;Disagree extremely&#39;, &#39;Disagree&#39;, &#39;Disagree slightly&#39;, &#39;Neutral&#39;, &#39;Agree slightly&#39;, &#39;Agree&#39;, &#39;Agree extremely&#39;)))) # The coloumns in the dataset which are 5 point likert-scales cols &lt;- colnames(fmcrsurvey)[11:17] fmcrsurvey &lt;- fmcrsurvey %&gt;% mutate_at(cols, funs(factor(.,ordered = T, levels = c(&#39;Completely disagree&#39;, &#39;Disagree&#39;, &#39;Neither disagree nor agree&#39;, &#39;Agree&#39;, &#39;Completely agree&#39;)))) # Other coloumns which are naturally ordered fmcrsurvey$Freq_Groceries &lt;- factor(fmcrsurvey$Freq_Groceries, ordered = T, levels = c(&#39;1-3 times per month&#39;,&quot;Once a week&quot;,&quot;6-2 times per week&quot;,&quot;Every day&quot;)) fmcrsurvey$Pay_large_water_melon &lt;- factor(fmcrsurvey$Pay_large_water_melon,ordered = T, levels = c(&#39;0-10 DKK&#39;,&#39;10-20 DKK&#39;,&#39;20-30 DKK&#39;,&#39;30-40 DKK&#39;,&#39;40 DKK and above&#39;)) 10.1 Descriptive statistics For reporting the results of a survey it is important to characterize the cohort/population who were used. This, inorder for others to be able to generalize the results. I.e. will certain insights obtained for a group of mostly young females also apply for the general population etc. For this descriptive statistics is used. For more general details see Descriptive statistics In the dataset we have asked for the Age, height, Weight, Gender, WhereGroceries and DietaryPreferences of the consumers. There are different ways to analyse thease background variables depending on the nature of the data. 10.1.1 Numeric values As an example we have Age. It is a numeric variable, and also continuous. We want to calculate mean, median, standard deviation and IQR mean(fmcrsurvey$Age) ## [1] 26.85283 sd(fmcrsurvey$Age) ## [1] 5.247388 median(fmcrsurvey$Age) ## [1] 26 IQR(fmcrsurvey$Age) ## [1] 4 … or simply use the summary() function, to get all the above answers in one go: summary(fmcrsurvey$Age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 20.00 24.00 26.00 26.85 28.00 57.00 Use the above mention codes for all continuous variables. 10.2 Plots It is a good idea to plot data. This is a natural first step to look for distributions and find odd behaving samples / miss-registrations. Below are some relevant examples. To see more, go to Plotting data. ggplot(data = fmcrsurvey, aes(Age, fill = Gender)) + geom_histogram(position = &#39;dodge&#39;) ggplot(data = fmcrsurvey, aes(x= 1, Age)) + geom_boxplot()+ facet_wrap(~Gender) 10.2.1 Within groups of data The flat calculations - across the entire dataset as a single group, is seldom very informative. We usually want to compare numbers between different groups. As an example we use How much you would pay for a banana in reference to the gender: ggplot(data = fmcrsurvey, aes(Gender, Pay_organic_banana)) + geom_boxplot() … If you want to cross with several variables (Try to evaluate the code your self): ggplot(data = fmcrsurvey, aes(Gender, Pay_organic_banana, fill = DietaryPreferences)) + guides(fill=guide_legend(title = &quot;Diet&quot;)) + geom_boxplot() Another useful function is the aggregate-function, which can be used to apply a function (e.g., taking the mean) according to a specified condition. In the below example, the sex of the participants is used. The function then groups rows with the same sex, and calculates the mean for each group. aggregate(fmcrsurvey$Pay_organic_banana, by = list(fmcrsurvey$Gender), mean) ## Group.1 x ## 1 Female 5.102679 ## 2 Male 5.603659 aggregate(fmcrsurvey$Pay_organic_banana, by = list(fmcrsurvey$Gender), sd) ## Group.1 x ## 1 Female 4.633490 ## 2 Male 5.977294 Try to do these for some of the other variables. 10.3 Categorical / Ordinal variables Below are some examples of plots that work well when you work with categorical data. For more info on plotting, see Plotting data. ggplot(data = fmcrsurvey, aes(Gender)) + geom_bar() ggplot(data = fmcrsurvey, aes(Gender, fill = Freq_Groceries)) + geom_bar() ggplot(data = fmcrsurvey, aes(Gender, fill = Freq_Groceries)) + geom_bar(position = &#39;dodge&#39;) 10.3.1 Tables The table() and prop.table() functions are your friends, as they help you get to know you data better. Try and see what the different inputs give as resulting output, and see what happens if you change variables. table(fmcrsurvey$Gender) ## ## Female Male ## 224 41 table(fmcrsurvey$Gender, fmcrsurvey$Freq_Groceries) ## ## 1-3 times per month Once a week 6-2 times per week Every day ## Female 7 47 160 10 ## Male 1 5 34 1 prop.table(table(fmcrsurvey$Gender, fmcrsurvey$Freq_Groceries), 1) ## ## 1-3 times per month Once a week 6-2 times per week Every day ## Female 0.03125000 0.20982143 0.71428571 0.04464286 ## Male 0.02439024 0.12195122 0.82926829 0.02439024 10.4 Table 1 There are some nice packages in R which can do the job for you in terms of organizing tables. An especially nice one is the one called tableone, which will help create tables almost ready for publication, if given the right inputs. Below are some examples of how the package could be used, using the same dataset as the rest of the chapter. Here is how to install the package: install.packages(&#39;tableone&#39;) Below is one way of using tableone, which will give information about the amount of participants in each group. library(tableone) CreateTableOne(data = fmcrsurvey, vars = c(&#39;Gender&#39;,&#39;Age&#39;, &#39;Freq_Groceries&#39;,&#39;I_am_able_to_prepare_a_soup&#39;)) ## ## Overall ## n 265 ## Gender = Male (%) 41 (15.5) ## Age (mean (SD)) 26.85 (5.25) ## Freq_Groceries (%) ## 1-3 times per month 8 ( 3.0) ## Once a week 52 (19.6) ## 6-2 times per week 194 (73.2) ## Every day 11 ( 4.2) ## I_am_able_to_prepare_a_soup (%) ## Completely disagree 4 ( 1.5) ## Disagree 5 ( 1.9) ## Neither disagree nor agree 5 ( 1.9) ## Agree 56 (21.1) ## Completely agree 195 (73.6) … below if split into groups: CreateTableOne(data = fmcrsurvey, vars = c(&#39;Age&#39;, &#39;Freq_Groceries&#39;,&#39;I_am_able_to_prepare_a_soup&#39;), strata = c(&#39;Gender&#39;,&#39;Freq_Groceries&#39;)) ## Stratified by Gender:Freq_Groceries ## Female:1-3 times per month ## n 7 ## Age (mean (SD)) 27.57 (2.07) ## Freq_Groceries (%) ## 1-3 times per month 7 (100.0) ## Once a week 0 ( 0.0) ## 6-2 times per week 0 ( 0.0) ## Every day 0 ( 0.0) ## I_am_able_to_prepare_a_soup (%) ## Completely disagree 0 ( 0.0) ## Disagree 0 ( 0.0) ## Neither disagree nor agree 0 ( 0.0) ## Agree 0 ( 0.0) ## Completely agree 7 (100.0) ## Stratified by Gender:Freq_Groceries ## Male:1-3 times per month Female:Once a week ## n 1 47 ## Age (mean (SD)) 23.00 (NA) 26.09 (4.39) ## Freq_Groceries (%) ## 1-3 times per month 1 (100.0) 0 ( 0.0) ## Once a week 0 ( 0.0) 47 (100.0) ## 6-2 times per week 0 ( 0.0) 0 ( 0.0) ## Every day 0 ( 0.0) 0 ( 0.0) ## I_am_able_to_prepare_a_soup (%) ## Completely disagree 0 ( 0.0) 0 ( 0.0) ## Disagree 0 ( 0.0) 2 ( 4.3) ## Neither disagree nor agree 0 ( 0.0) 0 ( 0.0) ## Agree 0 ( 0.0) 15 ( 31.9) ## Completely agree 1 (100.0) 30 ( 63.8) ## Stratified by Gender:Freq_Groceries ## Male:Once a week Female:6-2 times per week ## n 5 160 ## Age (mean (SD)) 27.40 (2.19) 26.80 (5.48) ## Freq_Groceries (%) ## 1-3 times per month 0 ( 0.0) 0 ( 0.0) ## Once a week 5 (100.0) 0 ( 0.0) ## 6-2 times per week 0 ( 0.0) 160 (100.0) ## Every day 0 ( 0.0) 0 ( 0.0) ## I_am_able_to_prepare_a_soup (%) ## Completely disagree 0 ( 0.0) 3 ( 1.9) ## Disagree 0 ( 0.0) 2 ( 1.2) ## Neither disagree nor agree 0 ( 0.0) 4 ( 2.5) ## Agree 0 ( 0.0) 29 ( 18.1) ## Completely agree 5 (100.0) 122 ( 76.2) ## Stratified by Gender:Freq_Groceries ## Male:6-2 times per week Female:Every day ## n 34 10 ## Age (mean (SD)) 26.65 (2.87) 28.80 (6.78) ## Freq_Groceries (%) ## 1-3 times per month 0 ( 0.0) 0 ( 0.0) ## Once a week 0 ( 0.0) 0 ( 0.0) ## 6-2 times per week 34 (100.0) 0 ( 0.0) ## Every day 0 ( 0.0) 10 (100.0) ## I_am_able_to_prepare_a_soup (%) ## Completely disagree 1 ( 2.9) 0 ( 0.0) ## Disagree 1 ( 2.9) 0 ( 0.0) ## Neither disagree nor agree 1 ( 2.9) 0 ( 0.0) ## Agree 11 ( 32.4) 0 ( 0.0) ## Completely agree 20 ( 58.8) 10 (100.0) ## Stratified by Gender:Freq_Groceries ## Male:Every day p test ## n 1 ## Age (mean (SD)) 55.00 (NA) NA ## Freq_Groceries (%) &lt;0.001 ## 1-3 times per month 0 ( 0.0) ## Once a week 0 ( 0.0) ## 6-2 times per week 0 ( 0.0) ## Every day 1 (100.0) ## I_am_able_to_prepare_a_soup (%) 0.697 ## Completely disagree 0 ( 0.0) ## Disagree 0 ( 0.0) ## Neither disagree nor agree 0 ( 0.0) ## Agree 1 (100.0) ## Completely agree 0 ( 0.0) If you want to do it on a large set of variables then it might be convenient to extract the names without having to type all in: varnames &lt;- colnames(fmcrsurvey)[2:10] CreateTableOne(data = fmcrsurvey, vars = varnames, strata = &#39;Gender&#39;) Try to play around with these codes to see if it makes sense what is presented. 10.5 The tidyverse way If the datasets get to large, or the work feels too tedious, there’s a way around that - the tidyverse-way. tidyverse is a large framework build in R, that lets you do almost everything with data. For example it uses what is called the pipe operator: %&gt;%. For more information, see Edit using Tidyverse. Now, let’s look at all Likert-scale variables in plots and numbers: fmcrsurvey %&gt;% gather(var,y,I_will_only_buy_products_at_a_reduced_price:I_am_able_to_prepare_a_soup) %&gt;% ggplot(data = ., aes(y)) + geom_bar() + facet_wrap(~var) + coord_flip() Some of the levels added doesn’t fit with the columns, which yields the mediocre result above. One will have to either align the levels, or make separate plots for each group of levels. Aligning can be done in different ways, e.g., in R and in Excel, but in our case, since the questions do not have the same number of levels, it seems more appropriate to create two different plots, to ensure that no information is lost. The reorder_levels-function from the rstatix-package ensures, that the levels appear in the desired order on the plot. library(rstatix) fmcrsurvey %&gt;% gather(var,y,I_will_only_buy_products_at_a_reduced_price:I_am_always_updated_on_the_latest_food_trends) %&gt;% reorder_levels(data = ., name = &#39;y&#39;, order = c(&#39;Disagree extremely&#39;, &#39;Disagree&#39;, &#39;Disagree slightly&#39;, &#39;Neutral&#39;, &#39;Agree slightly&#39;, &#39;Agree&#39;, &#39;Agree extremely&#39;)) %&gt;% ggplot(data = ., aes(y,fill = Gender)) + geom_bar() + facet_wrap(~var) + coord_flip() fmcrsurvey %&gt;% gather(var,y,I_am_able_to_bake_bread:I_am_able_to_prepare_a_soup) %&gt;% reorder_levels(data = ., name = &#39;y&#39;, order = c(&#39;Completely disagree&#39;, &#39;Disagree&#39;, &#39;Neither disagree nor agree&#39;, &#39;Agree&#39;, &#39;Completely agree&#39;)) %&gt;% ggplot(data = ., aes(y,fill = Gender)) + geom_bar() + facet_wrap(~var) + coord_flip() "],["consumer-segmentation.html", "Chapter 11 Consumer segmentation 11.1 Segmentation 11.2 Selecting the number of clusters 11.3 Segmentation - example 2", " Chapter 11 Consumer segmentation The data used in the first part of this chapter is from the paper: Verbeke, Wim, Federico JA Pérez-Cueto, and Klaus G. Grunert. “To eat or not to eat pork, how frequently and how varied? Insights from the quantitative Q-PorkChains consumer survey in four European countries.” Meat science 88.4 (2011): 619-626. and can be found in the data4consumerscience-package as pork. library(data4consumerscience) data(pork) 11.1 Segmentation To create the consumer segments, a cluster analysis, based on behavioral data (frequency and variety of consumption), will be carried out. This example shows k-means clustering as the clustering analysis, but there are many more options out there - some more data-driven and some using common sense or domain knowledge. Both types of segmentation can be valid, as long as the scientific reasoning behind makes sense. 11.1.1 K-means K-means is a popular algorithm used for clustering, which is a fancy word for grouping. This means, that we can group un-grouped data, if the different attributes create such groups. The algorithm works by you selecting a value for “k,” which is the number of clusters/groups that the algorithm will attempt to group the data points into. Then, it randomly selects “k” points from the dataset as the initial centroids. These centroids represent the centers of each cluster. Next, the algorithm iteratively assigns each data point to the nearest centroid based on the squared Euclidean distance between the point and the centroid (also known as the “Within Cluster Sum of Squares”(WCSS)). After all data points have been assigned, the centroid for each cluster is recalculated by taking the mean of all the points assigned to that cluster. This step updates the center of each cluster. The algorithm repeats the previous two steps until the centroids no longer move or the maximum number of iterations is reached. The final result is a set of “k” clusters, where each data point belongs to the cluster with the closest centroid. K-means in R is pretty simple - only one line of code is needed to perform the analysis, if the data are ready for analysis. First, we set.seed. As k-means is an iterative algorithm, that starts in a random point, it matters where this point is. Usually, the only thing happening if the seed is changed, is that the order of the clusters change - but this can be annoying enough that you would want to avoid it! To ensure, that we get the same results every time we run the algorithm, we set the seed, and keep it that way throughout the analysis. As a start, we only use two variables (specified in the code as “VarietyTotal” &amp; “TotalPorkWeek”, from the dataset pork), as this is easy to interpret. Below is shown an example with both 2 and 3 clusters (“centers = _“). The “nstart”-input specifies, that the clustering will start at 25 different random points, and selects the best. Doing this will help getting the same result every time, especially if data are less clustered by nature. For details on the algorithm type ?kmeans to get help. Note, the set.seed(123) is only for reproducibility and hence only nice-to-know. set.seed(123) km.cluster3 &lt;- kmeans(pork[, c(&quot;VarietyTotal&quot;, &quot;TotalPorkWeek&quot;)], centers = 3,nstart = 25) km.cluster2 &lt;- kmeans(pork[, c(&quot;VarietyTotal&quot;, &quot;TotalPorkWeek&quot;)], centers = 2,nstart = 25) First, we run a clustering with the function kmeans for the dataset pork. We will use the two variables “VarietyTotal” &amp; “TotalPorkWeek” to calculate the three best clusters, and we will choose the random centers 25 times. You can off course change this number yourself. We save it as km.cluster3 (remember you choose this name). Secondly, we run almost the same clustering, changing only the name and the number of centers. 11.1.2 Internal characterization of clusters Below are some of the outputs, that you can get from the clustering. Here you can see the size of the clusters as well as the positions of their centroids. km.cluster3[[&quot;centers&quot;]] ## VarietyTotal TotalPorkWeek ## 1 27.10061 22.969634 ## 2 22.85949 4.957372 ## 3 27.26288 11.343699 km.cluster3[[&quot;size&quot;]] ## [1] 328 548 757 km.cluster3$size ## [1] 328 548 757 km.cluster2[[&#39;centers&#39;]] ## VarietyTotal TotalPorkWeek ## 1 25.04562 7.571986 ## 2 27.28738 20.140544 km.cluster2[[&#39;size&#39;]] ## [1] 1118 515 km.cluster2$size ## [1] 1118 515 Centers will give you the average values for the variables “VarietyTotal” &amp; “TotalPorkWeek” in the center of each of the three (in km.cluster3) clusters. Size will give you the the number of members per cluster. It is a good idea to check that the clusters are not too uneven in number of members. The third line above is just another way of asking for the size of the clusters in km.cluster3. All command are repeated for the km.cluster2; that is the cluster analysis with two clusters, you made and saved above. 11.1.3 Vizualization It is, however, a lot easier to understand clustering, when it is visualized. Below is shown two different ways of plotting the clusters - one using the build-in plotting function, and one using the package ggplot2. ggplot2 maybe looks a bit more complicated, but has a lot of functions built into it, which means that it can be nice to learn! Try to make the plot yourself, and try to see what happens if you change different inputs. library(ggplot2) #Basic R-plotting plot(TotalPorkWeek ~ VarietyTotal, data = pork, col = km.cluster3[[&quot;cluster&quot;]] + 1) points(km.cluster3[[&quot;centers&quot;]], pch = 20, cex = 1.3) df &lt;- data.frame(km.cluster3[[&quot;centers&quot;]]) df2 &lt;- data.frame(km.cluster2[[&quot;centers&quot;]]) #ggplot 2 ggplot(data = pork, aes(x = VarietyTotal, y = TotalPorkWeek)) + geom_point(aes(color = factor(km.cluster3$cluster))) + guides(color = guide_legend(&#39;Cluster&#39;)) + geom_point(data = df, aes(x = VarietyTotal, y = TotalPorkWeek), color = &#39;black&#39;, size = 2) + theme_linedraw() ggplot(data = pork, aes(x = VarietyTotal, y = TotalPorkWeek)) + geom_point(aes(color = factor(km.cluster2$cluster))) + guides(color = guide_legend(&#39;Cluster&#39;)) + geom_point(data = df2, aes(x = VarietyTotal, y = TotalPorkWeek), color = &#39;black&#39;, size = 2) + theme_linedraw() 11.1.4 K-means splits variation As mentioned before, k-means set the clusters such that the between clusters (sort of differences between centers) are large while the distance within a cluster (from each sample to is cluster-center) is small. R can also display these numbers, as shown below: km.cluster3[[&quot;withinss&quot;]] ## [1] 9293.210 12534.737 9854.921 km.cluster3$withinss ## [1] 9293.210 12534.737 9854.921 km.cluster3[[&quot;tot.withinss&quot;]] ## [1] 31682.87 km.cluster3$tot.withinss ## [1] 31682.87 km.cluster3[[&quot;betweenss&quot;]] ## [1] 73532.81 km.cluster3$betweenss ## [1] 73532.81 11.2 Selecting the number of clusters Usually there is no given set of clusters, so we want to also learn that from data. Here we use the gap-statistics (from the cluster-package) for \\(2\\) up to \\(10\\) clusters to select the optimal number of clusters. Another package able to aid when trying to determine the number of clusters is NbClust. This package runs a variety of different tests, each resulting in an optimal number of clusters. This means, that while you get a lot of opinion on the number of clusters, the NbClust-function might take a while to run. library(cluster) gapStatistic &lt;- clusGap(pork[, c(&quot;VarietyTotal&quot;, &quot;TotalPorkWeek&quot;)], kmeans, 10) plot(gapStatistic, main = &quot;&quot;) library(NbClust) NbClust(pork[, c(&quot;VarietyTotal&quot;, &quot;TotalPorkWeek&quot;)], method = &#39;kmeans&#39; ) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 10 proposed 2 as the best number of clusters ## * 3 proposed 3 as the best number of clusters ## * 6 proposed 4 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## * 2 proposed 11 as the best number of clusters ## * 2 proposed 13 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* ## $All.index ## KL CH Hartigan CCC Scott Marriot TrCovW TraceW ## 2 56.1836 1963.103 811.0747 89.9630 8226.028 2138234314 378913825 47746.762 ## 3 0.0218 1874.052 739.7121 65.3262 9446.926 2277935011 241897284 31888.856 ## 4 12.6137 2061.650 392.7671 61.3300 10690.882 1890559283 120544635 21934.663 ## 5 0.3461 2016.003 286.2602 83.3464 11373.636 1944610296 73584867 17673.433 ## 6 14.6925 1952.442 273.7718 80.9335 11897.567 2031684218 59581315 15030.532 ## 7 0.0348 1945.245 161.7913 79.9313 12432.856 1992463523 44723627 12865.656 ## 8 1.4255 1855.230 155.7371 77.4747 12801.073 2077052106 37486333 11701.342 ## 9 0.4590 1797.264 83.9829 75.7445 13092.654 2198905768 28617679 10677.983 ## 10 0.7411 1688.475 301.0912 72.9275 13247.418 2469234023 24104279 10152.938 ## 11 34.4317 1830.522 143.4684 75.6932 13741.858 2207252667 17205821 8564.156 ## 12 2.4712 1823.222 136.6857 75.2509 14021.502 2213391038 15668068 7868.202 ## 13 0.9641 1822.478 122.9659 74.9888 14288.572 2205744974 14988579 7256.334 ## 14 0.1042 1818.317 100.0471 74.6853 14523.168 2215818816 12409771 6744.402 ## 15 0.3742 1798.810 67.3705 74.0739 14740.176 2227140935 11329442 6351.883 ## Friedman Rubin Cindex DB Silhouette Duda Pseudot2 Beale ## 2 66.0911 29.4371 0.2245 0.8627 0.4909 1.3037 -222.0273 -0.2325 ## 3 83.4537 44.0757 0.1847 0.9375 0.3917 1.8168 -378.5427 -0.4477 ## 4 138.3598 64.0778 0.1814 0.8946 0.4049 1.3178 -136.9828 -0.2399 ## 5 152.7213 79.5275 0.1770 0.9372 0.3651 1.9658 -204.3762 -0.4879 ## 6 185.8257 93.5113 0.1620 0.9253 0.3739 1.2354 -106.1348 -0.1899 ## 7 204.0712 109.2462 0.1540 0.9238 0.3619 1.1836 -54.9170 -0.1544 ## 8 218.4906 120.1165 0.1465 0.9734 0.3411 1.6366 -117.8653 -0.3859 ## 9 240.0631 131.6283 0.1385 0.9393 0.3401 1.8296 -178.1979 -0.4497 ## 10 254.2891 138.4352 0.1368 0.9827 0.3308 1.3959 -80.5506 -0.2812 ## 11 315.9352 164.1171 0.1528 0.9867 0.3455 1.3758 -70.4755 -0.2705 ## 12 340.6775 178.6335 0.1451 0.9451 0.3497 1.3548 -65.9994 -0.2594 ## 13 374.1020 193.6962 0.1499 0.9534 0.3433 1.7624 -104.2523 -0.4281 ## 14 402.0186 208.3987 0.1448 0.9365 0.3478 1.5951 -48.4991 -0.3689 ## 15 416.3872 221.2768 0.1393 0.9234 0.3445 1.4179 -41.8553 -0.2887 ## Ratkowsky Ball Ptbiserial Frey McClain Dunn Hubert SDindex Dindex ## 2 0.3887 23873.3812 0.5687 1.2308 0.3605 0.0024 0 0.5862 4.7142 ## 3 0.4004 10629.6186 0.5382 0.4377 0.6965 0.0024 0 0.5626 3.8855 ## 4 0.4190 5483.6656 0.5467 1.2396 0.8750 0.0017 0 0.5112 3.2072 ## 5 0.3819 3534.6867 0.4947 0.8970 1.1788 0.0028 0 0.5985 2.8671 ## 6 0.3606 2505.0887 0.4616 0.6240 1.4239 0.0023 0 0.6313 2.6517 ## 7 0.3372 1837.9509 0.4379 0.7145 1.6251 0.0024 0 0.6600 2.4445 ## 8 0.3170 1462.6677 0.4216 0.4896 1.7701 0.0014 0 0.7163 2.3324 ## 9 0.3024 1186.4425 0.4101 2.7773 1.8753 0.0024 0 0.7358 2.2326 ## 10 0.2887 1015.2938 0.3896 0.1124 2.0937 0.0034 0 0.9044 2.1659 ## 11 0.2813 778.5597 0.3903 0.7127 2.0591 0.0035 0 0.9345 2.0328 ## 12 0.2708 655.6835 0.3715 0.6366 2.2628 0.0046 0 0.9476 1.9345 ## 13 0.2617 558.1796 0.3590 0.4912 2.4097 0.0050 0 0.9883 1.8688 ## 14 0.2533 481.7430 0.3504 0.6683 2.5089 0.0031 0 1.0249 1.7959 ## 15 0.2451 423.4589 0.3364 1.0617 2.7000 0.0019 0 0.9938 1.7275 ## SDbw ## 2 1.2285 ## 3 0.9080 ## 4 0.5303 ## 5 0.4597 ## 6 0.5171 ## 7 0.5564 ## 8 0.5642 ## 9 0.4591 ## 10 0.4583 ## 11 0.3910 ## 12 0.3725 ## 13 0.2793 ## 14 0.3190 ## 15 0.2906 ## ## $All.CriticalValues ## CritValue_Duda CritValue_PseudoT2 Fvalue_Beale ## 2 0.5727 710.9445 1 ## 3 0.5224 769.8031 1 ## 4 0.5059 554.8207 1 ## 5 0.4768 456.5692 1 ## 6 0.5353 483.6256 1 ## 7 0.5142 334.4922 1 ## 8 0.4610 354.3151 1 ## 9 0.4592 462.8756 1 ## 10 0.4564 338.2412 1 ## 11 0.4397 328.7564 1 ## 12 0.4409 319.6094 1 ## 13 0.4324 316.3443 1 ## 14 0.4230 177.3303 1 ## 15 0.3327 284.8175 1 ## ## $Best.nc ## KL CH Hartigan CCC Scott Marriot TrCovW ## Number_clusters 2.0000 4.00 4.000 2.000 4.000 4 3 ## Value_Index 56.1836 2061.65 346.945 89.963 1243.955 441426740 137016541 ## TraceW Friedman Rubin Cindex DB Silhouette Duda ## Number_clusters 3.000 11.0000 11.0000 10.0000 2.0000 2.0000 2.0000 ## Value_Index 5903.713 61.6461 -11.1654 0.1368 0.8627 0.4909 1.3037 ## PseudoT2 Beale Ratkowsky Ball PtBiserial Frey McClain ## Number_clusters 2.0000 2.0000 4.000 3.00 2.0000 2.0000 2.0000 ## Value_Index -222.0273 -0.2325 0.419 13243.76 0.5687 1.2308 0.3605 ## Dunn Hubert SDindex Dindex SDbw ## Number_clusters 13.000 0 4.0000 0 13.0000 ## Value_Index 0.005 0 0.5112 0 0.2793 ## ## $Best.partition ## [1] 2 2 2 1 1 2 2 1 2 1 2 2 1 2 2 2 2 2 2 2 1 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [38] 2 2 2 2 2 2 2 2 1 1 2 2 1 2 2 1 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 1 2 ## [75] 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 ## [112] 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 ## [149] 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 ## [186] 1 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [223] 2 2 1 1 1 1 2 2 2 2 1 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [260] 2 2 2 2 2 1 2 2 2 1 2 1 2 2 1 2 2 2 2 2 2 2 1 2 2 2 1 2 2 1 2 2 1 1 1 2 2 ## [297] 2 1 1 1 2 1 1 2 2 1 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 ## [334] 1 2 1 2 2 2 2 2 1 2 2 2 2 2 1 2 1 1 2 1 1 2 2 2 2 2 2 1 2 1 2 1 2 2 1 2 2 ## [371] 1 2 2 1 2 2 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 ## [408] 2 2 2 2 1 2 1 2 1 1 2 2 1 2 2 1 2 2 1 1 1 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 ## [445] 2 1 1 2 2 2 2 2 1 2 1 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 ## [482] 2 1 1 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 ## [519] 1 2 2 1 1 1 2 2 2 1 2 2 2 2 2 2 2 1 1 2 1 2 2 2 2 2 2 1 2 2 1 2 2 1 2 1 2 ## [556] 1 1 1 2 2 2 1 2 2 2 2 1 2 2 2 1 2 1 1 2 2 2 1 2 2 2 1 1 1 2 2 1 2 2 2 2 2 ## [593] 2 1 2 2 2 2 2 2 2 2 1 2 1 1 2 2 1 1 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 1 ## [630] 2 2 2 2 2 1 1 2 2 2 1 1 2 2 2 2 2 2 1 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 1 1 ## [667] 2 1 2 2 2 1 2 2 1 2 1 2 1 2 1 2 2 1 2 1 2 2 2 1 1 2 1 1 1 1 2 1 2 1 1 2 2 ## [704] 1 2 1 1 2 2 1 2 2 1 1 2 2 2 2 1 2 1 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 1 2 2 ## [741] 2 2 2 2 2 2 2 1 1 2 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 1 ## [778] 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 ## [815] 2 2 1 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 1 2 2 2 1 1 2 2 1 ## [852] 1 2 2 1 2 1 2 2 2 1 1 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 2 2 1 ## [889] 1 2 2 1 2 2 2 1 1 1 2 2 1 1 2 2 1 2 2 1 1 2 2 1 1 1 2 1 1 2 2 1 1 2 2 2 1 ## [926] 2 1 1 2 1 1 2 2 2 2 1 1 2 2 1 2 1 2 2 1 1 2 2 2 2 1 2 1 1 1 1 1 1 2 1 2 1 ## [963] 2 2 2 1 1 1 2 2 2 1 1 1 1 2 1 1 2 1 1 1 2 1 2 2 1 1 2 2 1 1 1 1 2 2 1 2 1 ## [1000] 1 2 2 1 2 1 2 2 2 1 2 2 1 2 1 2 2 1 2 1 2 1 1 1 2 2 2 2 2 1 1 1 2 2 1 2 2 ## [1037] 2 1 2 1 2 2 2 2 2 2 1 2 2 1 1 2 2 2 1 1 2 2 2 1 2 2 2 1 1 2 1 1 2 1 2 2 1 ## [1074] 1 2 1 1 1 1 2 2 1 1 2 2 1 1 1 1 1 2 2 2 1 1 2 1 2 2 1 1 2 1 2 1 1 2 1 1 1 ## [1111] 1 2 2 2 1 1 2 1 2 1 1 2 1 1 2 1 1 2 2 2 1 2 1 2 1 2 1 1 2 2 2 1 2 1 2 1 1 ## [1148] 1 1 1 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 2 1 1 2 2 1 1 2 1 2 2 2 2 1 2 2 1 2 2 ## [1185] 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 1 1 2 1 2 2 1 2 2 2 2 1 1 1 1 1 2 2 2 2 ## [1222] 2 2 2 1 2 2 1 2 2 2 1 2 2 2 2 2 1 1 2 1 2 2 2 2 1 2 1 2 2 2 1 2 2 2 2 1 1 ## [1259] 1 1 2 1 1 1 1 2 2 2 1 2 2 1 2 1 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 1 1 2 ## [1296] 1 2 2 2 2 2 2 1 2 1 2 1 2 1 2 2 1 2 2 1 1 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 ## [1333] 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 2 2 1 2 1 2 1 2 1 1 2 2 2 2 2 2 2 1 2 ## [1370] 1 2 2 1 1 1 1 1 2 1 2 1 2 2 2 2 1 2 2 2 1 2 1 1 2 2 2 1 1 2 2 2 1 2 2 2 2 ## [1407] 1 2 1 2 1 1 1 1 2 2 2 1 1 2 2 1 2 1 1 2 2 1 2 2 2 1 2 2 1 1 2 2 2 1 1 1 2 ## [1444] 2 2 1 1 1 1 1 2 1 2 2 2 1 1 2 2 2 2 1 2 1 1 2 2 2 2 2 2 2 1 2 2 2 2 1 1 2 ## [1481] 2 2 1 2 2 2 1 2 2 2 2 1 1 2 1 1 2 2 1 2 1 2 1 2 2 2 2 2 2 2 1 2 1 2 1 2 2 ## [1518] 2 2 1 1 2 1 2 2 1 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 ## [1555] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 1 1 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 ## [1592] 1 2 2 2 2 1 2 1 1 2 2 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 1 2 1 2 1 2 2 ## [1629] 2 2 2 2 2 The gap-statistics seems to suggest k=3, whereas NbClust suggests k=2-4, which shows how important it is, that you know your data enough to be able to make the right decision, such as number of clusters. The rule of thumb is to use these data driven approaches together with interpretability of the resulting clusters, such that the solution is meaningful. 11.3 Segmentation - example 2 The data plantbaseddiet constitute data from the following study, and can be found in the data4consumerscience-package: Reipurth, Malou FS, Lasse Hørby, Charlotte G. Gregersen, Astrid Bonke, and Federico JA Perez Cueto. “Barriers and facilitators towards adopting a more plant-based diet in a sample of Danish consumers.” Food quality and preference 73 (2019): 288-292. These we will use as a second example for cluster analysis. library(data4consumerscience) data(plantbaseddiet) 11.3.1 Cluster analysis In this example, 3 variables are included in the clustering: ‘a_meat’, ‘a_dairy’ and ‘a_eggs’. Otherwise, the same procedure applies: Finding the appropriate amount of clusters, and running the analysis. library(cluster) library(NbClust) set.seed(123) gapStatistic &lt;- clusGap(plantbaseddiet[,c(&#39;a_meat&#39;, &#39;a_dairy&#39;,&#39;a_eggs&#39;)], kmeans, 10) plot(gapStatistic, main = &quot;&quot;) NbClust(plantbaseddiet[,c(&#39;a_meat&#39;, &#39;a_dairy&#39;,&#39;a_eggs&#39;)],method = &#39;kmeans&#39; ) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 4 proposed 2 as the best number of clusters ## * 2 proposed 3 as the best number of clusters ## * 8 proposed 4 as the best number of clusters ## * 1 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 2 proposed 11 as the best number of clusters ## * 1 proposed 12 as the best number of clusters ## * 3 proposed 14 as the best number of clusters ## * 1 proposed 15 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 4 ## ## ## ******************************************************************* ## $All.index ## KL CH Hartigan CCC Scott Marriot TrCovW TraceW ## 2 0.9252 311.7057 182.2207 11.7654 1149.509 5.488072e+12 156679680 36862.471 ## 3 0.2586 308.0304 285.4003 11.7339 1675.645 3.953831e+12 44523036 26403.285 ## 4 7.1975 427.2404 49.5727 17.7814 2234.681 2.095967e+12 23640671 16280.364 ## 5 0.2894 366.7036 153.9497 15.1320 2442.315 2.089400e+12 19737933 14690.323 ## 6 3.2405 422.0525 22.2209 18.7139 2743.294 1.568391e+12 13180805 10988.593 ## 7 0.5040 371.7357 101.6624 20.5406 2812.700 1.836977e+12 12153549 10478.001 ## 8 343.2438 403.4580 36.9155 23.5175 3048.082 1.441522e+12 7337648 8564.420 ## 9 0.0135 385.4943 45.8019 23.0734 3216.600 1.266818e+12 6981343 7920.400 ## 10 0.1817 381.5523 111.5743 23.4753 3303.003 1.297204e+12 5401575 7193.119 ## 11 30.8298 438.3487 29.5864 27.7794 3782.521 5.559440e+11 4470955 5769.052 ## 12 0.0246 426.3833 -62.7616 27.6000 3845.865 5.768499e+11 3880497 5413.891 ## 13 1.1235 330.3733 115.2435 21.3517 3483.329 1.483823e+12 3995422 6291.346 ## 14 1.2601 391.2247 -42.0818 26.2890 3982.228 5.844825e+11 3332476 5006.375 ## 15 0.5880 325.4223 -67.2994 21.8516 3648.917 1.380450e+12 3227640 5525.390 ## Friedman Rubin Cindex DB Silhouette Duda Pseudot2 Beale Ratkowsky ## 2 11.1925 4.2926 0.2408 1.1390 0.4242 1.0316 -8.2299 -0.0519 0.3618 ## 3 23.0223 5.9930 0.2045 0.9780 0.4798 2.7601 -183.6544 -1.0807 0.3504 ## 4 32.8452 9.7194 0.1738 0.9088 0.5576 1.0816 -13.1957 -0.1265 0.3672 ## 5 44.0329 10.7714 0.1851 0.9134 0.5535 3.3473 -94.6685 -1.1827 0.3302 ## 6 41.3374 14.3999 0.2038 1.0512 0.4393 0.8229 26.2617 0.3635 0.3569 ## 7 42.1340 15.1016 0.1929 1.4036 0.3636 0.7647 26.4662 0.5160 0.3350 ## 8 52.5762 18.4759 0.1710 0.9051 0.4696 4.3917 -102.7154 -1.2963 0.3199 ## 9 53.4843 19.9782 0.1632 0.8835 0.4736 0.4539 56.5376 2.0113 0.3087 ## 10 66.8519 21.9981 0.1723 0.8750 0.4953 0.8440 17.1893 0.3075 0.2916 ## 11 144.1779 27.4283 0.1317 0.8836 0.4774 0.3838 85.0828 2.6823 0.2804 ## 12 146.9496 29.2276 0.1266 0.8611 0.4921 0.7735 19.6242 0.4919 0.2693 ## 13 70.9156 25.1512 0.2079 0.9648 0.4956 8.1807 -88.6539 -1.4476 0.2594 ## 14 167.8458 31.6067 0.1428 0.9407 0.4541 9.9679 -42.2849 -1.4295 0.2512 ## 15 80.6944 28.6378 0.1689 0.8187 0.5513 2.7370 -60.9257 -0.9604 0.2430 ## Ball Ptbiserial Frey McClain Dunn Hubert SDindex Dindex SDbw ## 2 18431.2357 0.4744 -0.0609 0.5672 0.0472 0 0.5957 7.3721 0.7241 ## 3 8801.0948 0.5864 0.0248 0.6604 0.2590 0 0.4208 6.1734 0.4075 ## 4 4070.0910 0.6834 0.5076 0.6790 0.2997 0 0.4083 4.8901 0.3497 ## 5 2938.0647 0.6826 2.6401 0.7024 0.3360 0 0.4447 4.5644 0.3380 ## 6 1831.4322 0.5574 1.5315 1.1764 0.0583 0 0.6780 4.1481 0.2968 ## 7 1496.8572 0.5127 0.2105 1.4380 0.0583 0 0.7699 3.9974 0.2660 ## 8 1070.5525 0.5120 0.1956 1.4505 0.0729 0 0.6013 3.5977 0.2825 ## 9 880.0444 0.5118 -0.4712 1.4471 0.0729 0 0.6727 3.5194 0.2775 ## 10 719.3119 0.5259 0.4604 1.3555 0.1278 0 0.6765 3.3753 0.2292 ## 11 524.4593 0.4970 0.5527 1.4692 0.1166 0 0.6530 2.9324 0.2096 ## 12 451.1575 0.4885 -0.0373 1.5052 0.1166 0 0.6509 2.7838 0.1839 ## 13 483.9497 0.4734 -0.1262 1.6879 0.1650 0 0.7505 3.0000 0.1904 ## 14 357.5982 0.4918 -0.9123 1.4990 0.0639 0 0.8799 2.7325 0.1630 ## 15 368.3593 0.4587 -0.5293 1.7779 0.0714 0 0.9286 2.7439 0.1742 ## ## $All.CriticalValues ## CritValue_Duda CritValue_PseudoT2 Fvalue_Beale ## 2 0.6409 150.7003 1.0000 ## 3 0.6373 163.9017 1.0000 ## 4 0.5210 160.8866 1.0000 ## 5 0.5730 100.5973 1.0000 ## 6 0.5873 85.7137 0.7794 ## 7 0.5151 80.9573 0.6718 ## 8 0.5247 120.4586 1.0000 ## 9 0.4921 48.4998 0.1144 ## 10 0.4551 111.3709 0.8199 ## 11 0.4868 55.8698 0.0487 ## 12 0.5301 59.4019 0.6883 ## 13 0.3992 151.9816 1.0000 ## 14 0.2298 157.5488 1.0000 ## 15 0.0819 1075.4679 1.0000 ## ## $Best.nc ## KL CH Hartigan CCC Scott Marriot ## Number_clusters 8.0000 11.0000 4.0000 11.0000 4.0000 4.000000e+00 ## Value_Index 343.2438 438.3487 235.8276 27.7794 559.0357 1.851296e+12 ## TrCovW TraceW Friedman Rubin Cindex DB Silhouette ## Number_clusters 3 4.00 14.0000 14.0000 12.0000 15.0000 4.0000 ## Value_Index 112156644 8532.88 96.9302 -9.4244 0.1266 0.8187 0.5576 ## Duda PseudoT2 Beale Ratkowsky Ball PtBiserial Frey ## Number_clusters 2.0000 2.0000 2.0000 4.0000 3.000 4.0000 1 ## Value_Index 1.0316 -8.2299 -0.0519 0.3672 9630.141 0.6834 NA ## McClain Dunn Hubert SDindex Dindex SDbw ## Number_clusters 2.0000 5.000 0 4.0000 0 14.000 ## Value_Index 0.5672 0.336 0 0.4083 0 0.163 ## ## $Best.partition ## [1] 3 4 4 4 2 2 4 2 4 4 2 3 2 2 4 4 4 4 4 3 3 4 1 2 3 4 4 4 4 1 1 1 2 4 1 4 3 ## [38] 2 4 4 1 3 4 2 3 2 2 4 4 2 2 4 2 4 2 3 1 4 4 4 4 4 3 2 3 1 1 3 2 4 2 4 1 4 ## [75] 2 2 1 4 4 1 4 4 4 4 3 4 2 4 2 4 4 4 4 4 3 2 1 4 4 1 3 4 2 2 2 4 1 2 4 3 3 ## [112] 4 3 4 2 3 2 3 4 2 4 4 2 4 1 4 4 4 4 2 1 1 2 4 4 1 2 4 4 2 2 3 4 2 2 4 4 2 ## [149] 4 1 4 2 3 4 4 1 4 3 2 4 4 3 4 4 3 2 2 2 1 1 4 2 4 4 2 2 4 4 4 2 2 1 1 3 2 ## [186] 1 1 1 1 4 4 4 4 1 2 1 4 2 4 4 2 4 4 4 2 2 1 1 4 4 4 2 4 4 2 4 1 4 4 4 1 3 ## [223] 1 2 4 1 1 1 4 4 2 1 4 4 3 2 4 4 4 4 2 2 1 4 1 1 4 4 4 4 3 2 4 2 3 4 4 4 3 ## [260] 4 1 4 4 1 4 3 2 2 1 4 4 4 2 4 4 4 4 4 2 4 4 4 4 4 4 2 2 1 1 2 1 2 4 2 2 4 ## [297] 1 2 1 2 4 1 4 1 2 2 3 3 4 1 2 2 4 4 4 2 4 2 1 4 4 4 4 4 2 3 4 4 4 2 3 4 2 ## [334] 3 2 2 2 4 4 2 2 4 4 2 4 3 4 4 4 4 2 2 4 2 2 4 3 3 2 1 4 4 4 3 4 1 1 2 4 1 ## [371] 4 4 2 4 4 3 4 4 4 2 4 4 2 4 2 3 4 1 4 4 1 4 3 4 4 4 4 3 4 1 3 4 2 1 4 4 4 ## [408] 4 2 2 1 3 4 2 4 4 2 2 4 1 2 4 2 1 2 2 4 4 2 4 4 4 4 4 2 2 1 2 2 3 1 4 2 4 ## [445] 1 2 4 4 2 4 2 4 4 2 4 4 2 4 4 1 2 4 res &lt;- kmeans(plantbaseddiet[,c(&#39;a_meat&#39;, &#39;a_dairy&#39;,&#39;a_eggs&#39;)], centers = 4, nstart = 25) 11.3.2 Plot the model using PCA The thing is, that now that we are dealing with more than 2 dimensions, it can be harder to visualize, how well the clustering has worked. One way though, is to use Principal Component Analysis (PCA), as PCA aims to describe the most variance from the data as possible, in the fewest dimensions as possible. This means, that if indeed the clusters are a result of variation in the data, a PCA should show it. For more information on PCA, see Introduction to PCA and multivariate data. A PCA with the same data as the clustering is created (using the prcomp) and plotted (using ggbiplot) library(ggbiplot) set.seed(123) PCAmdl &lt;- prcomp(plantbaseddiet[,c(&#39;a_meat&#39;, &#39;a_dairy&#39;,&#39;a_eggs&#39;)], center = T, scale. = F) ggbiplot(PCAmdl, groups = factor(res$cluster), ellipse = T) 11.3.3 Add to dataset As the clustering is a characteristic for each sample, similar to age, gender, name,… we can simply add it to the dataset. Here the interpretation from above gives rise to some intuitive names. These can be used directly. Here is how to add it to the dataset: #First, check the centriods, to see if they make sense. res$centers ## a_meat a_dairy a_eggs ## 1 4.688508 18.854839 3.601815 ## 2 3.719886 5.167045 2.915909 ## 3 19.150000 21.850000 7.678571 ## 4 18.812500 5.729167 4.492188 #Then matched the label names, according to the plot above. plantbaseddiet$clusters &lt;- factor(res$cluster,labels = c(&#39;High Meat&#39;,&#39;Low All&#39;,&#39;High Dairy&#39;,&#39;High ALl&#39;)) #And here the plot, now with the right labels. ggbiplot(PCAmdl, groups = plantbaseddiet$clusters, ellipse = T) 11.3.4 Comments Try to investigate if the number of clusters using gab-statistics and extract between and within variances. Plot the different versions using PCA. "],["profiling-segments.html", "Chapter 12 Profiling segments 12.1 Table 1 as a profiling tool 12.2 Visualization of Consumer Segments 12.3 Creating the contingency table 12.4 Plot the numbers 12.5 Contingency table 12.6 Correspondence Analysis", " Chapter 12 Profiling segments There are many ways of profiling consumer segments. Some of the most widely used are shown in this chapter, as well as in the chapter Logistic Regression. The data in this chapter constitute data from the following study. Reipurth, Malou FS, Lasse Hørby, Charlotte G. Gregersen, Astrid Bonke, and Federico JA Perez Cueto. “Barriers and facilitators towards adopting a more plant-based diet in a sample of Danish consumers.” Food quality and preference 73 (2019): 288-292. and can be found in the data4consumerscience-package as plantbaseddiet. The same order of operation regarding clustering will be followed here as in Consumer segmentation. library(data4consumerscience) data(plantbaseddiet) set.seed(123) res &lt;- kmeans(plantbaseddiet[,c(&#39;a_meat&#39;, &#39;a_dairy&#39;,&#39;a_eggs&#39;)],centers = 4) res$centers ## a_meat a_dairy a_eggs ## 1 18.812500 5.729167 4.492188 ## 2 3.719886 5.167045 2.915909 ## 3 4.688508 18.854839 3.601815 ## 4 19.150000 21.850000 7.678571 plantbaseddiet$clusters &lt;- factor(res$cluster, labels = c(&#39;High Meat&#39;,&#39;Low all&#39;,&#39;High Dairy&#39;,&#39;High All&#39;)) The dataset from the data4consumerscience package is loaded into the R environment. set.seed(123) sets a seed for generating random numbers, resulting in the k-means cluster analysis being reproducible, that is when running the k-means cluster analysis multiple times R will produce the same results. The kmeans() function performs the k-means cluster analysis, using the variables a_meat, a_dairy and a_eggs as input, while “centers” specifies the number of centers to be created. res$centers retrieves the cluster centers and the values represent the average values of the variables for each cluster e.g. Cluster 1 has an average value of 18.81 for a_meat. The last line adds a new variable “clusters” to the “plantbaseddiet” dataset. The “res$cluster” contains the cluster assignment for each observation (in this case participant). The factor is used to convert the cluster assignments into factor levels, corresponding to the labels “High Meat”, “Low all”, “High Dairy”, “High All”. Meaning cluster “1” will be assigned “High Meat”, in alignment with the result from earlier. 12.1 Table 1 as a profiling tool Table 1 in this study is a descriptive representation of the clusters, and hence is indeed a profiling of the segments, as it shows a nice overview of the socio-demographic distribution between clusters. Recall the tableone-package from Table 1. Below is code, that can lead you on the right track to mimic the table in the study. What variables are missing from the Table 1 of the study? library(tableone) tb1 &lt;- CreateTableOne(data = plantbaseddiet, strata = &#39;clusters&#39;, vars = c(&#39;gender&#39;,&#39;age&#39;)) print(tb1, cramVars = &#39;gender&#39;, nonnormal = &#39;age&#39;) ## Stratified by clusters ## High Meat Low all ## n 48 220 ## gender = Female/Male (%) 29/19 (60.4/39.6) 168/52 (76.4/23.6) ## age (median [IQR]) 29.00 [25.00, 35.25] 29.00 [25.00, 39.00] ## Stratified by clusters ## High Dairy High All p ## n 124 70 ## gender = Female/Male (%) 99/25 (79.8/20.2) 44/26 (62.9/37.1) 0.008 ## age (median [IQR]) 32.00 [26.00, 45.25] 31.50 [26.00, 40.75] 0.104 ## Stratified by clusters ## test ## n ## gender = Female/Male (%) ## age (median [IQR]) nonnorm 12.2 Visualization of Consumer Segments We use Correspondence Analysis. In principle just a PCA model for count data. You do not need to know all the details behind Correspondence Analysis. But it is a rather useful tool as a graphical supplement to the tabular characterization. 12.3 Creating the contingency table library(tidyverse) X &lt;- plantbaseddiet %&gt;% mutate(agegrp = cut(age, breaks = quantile(age), include.lowest = T)) %&gt;% gather(demo,demoval, gender,agegrp,city:income) %&gt;% mutate(demoval = paste(demo,demoval,sep = &#39;=&#39;)) conttb &lt;- table(X$demoval,X$clusters) conttb ## ## High Meat Low all High Dairy High All ## agegrp=(26,30] 9 46 16 15 ## agegrp=(30,40.8] 13 45 35 18 ## agegrp=(40.8,76] 10 52 36 18 ## agegrp=[15,26] 16 77 37 19 ## children=1 35 165 81 40 ## children=2 5 27 23 13 ## children=3 6 25 17 13 ## children=4 2 2 2 3 ## children=5 0 1 1 1 ## city=1 35 179 97 51 ## city=2 13 41 27 19 ## education=2 3 7 2 2 ## education=3 14 52 28 25 ## education=4 17 90 53 27 ## education=5 14 71 41 16 ## gender=Female 29 168 99 44 ## gender=Male 19 52 25 26 ## household=1 5 52 27 11 ## household=2 28 110 53 29 ## household=3 7 27 24 13 ## household=4 7 26 16 13 ## household=5 1 5 4 4 ## income=1 3 25 11 7 ## income=2 7 55 34 10 ## income=3 12 63 27 9 ## income=4 12 38 23 12 ## income=5 14 39 29 32 The package “tidyverse” is used to create a contingency table from the “plantbaseddiet” dataset. The first use of the “mutate” function is used to create a new variable “agegrp”. The “cut” function is used to create age groups by dividing the age into quantiles breaks. The “include.lowest” is set to “TRUE” to include the lowest value in the first age group. The “gather” function is then used to gather the variables “gender”, “agegrp” and variables from “city” to “income” into two new variables “demo” and “demoval”. This reshapes the data from a wide format to a long format, resulting in the variable “demo” containing the variable names and the variable “demoval” containing the corresponding values. The second use of the “mutate” function combines the values in “demo” and “demoval” seperated by a “=” into the variable “demoval”. Meaning if “demo” is gender and “demoval” is Male. The resulting “demoval” would be “gender=Male” The contingency table is then created using the “Table” function. In this case “demoval” is used as rows, and “clusters” as columns. If you consider the row for children=1, Based on the contingency table results provided above, you can see that out of the participants who have 1 child, 35 participants are assigned to the cluster “High Meat”, 165 participants are assigned to the cluster “Low All”, and so on. Similarly, for other categories such as children=2, children=3, etc. 12.4 Plot the numbers Here is a simple visual representation of the numbers library(&quot;gplots&quot;) # 1. convert the data as a table dt &lt;- as.table(as.matrix(conttb)) # 2. Graph balloonplot(t(dt), main =&quot;Demographics of Meat/Dairy segments&quot;, xlab =&quot;&quot;, ylab=&quot;&quot;, label = F, show.margins = F) To further visualize the results from the contingency table, one can use a balloonplot. First the results from the contingency table has to be converted into a table. The package “gplots” is used for creating the balloonplot. In the balloonplot, the dataframe dt is transposed to satisfy the needs of the balloonplot function. The arguments “label” and “show.margins” are set to “FALSE” to hide labels and margins for a cleaner plot. The interpretation is straight forward. The bigger the circle, the higher the frequencies. e.g. as for the previous section participants with 1 child, the circle is smaller for the cluster “High Meat” corresponding to 35 in the contingency table compared to the cluster “Low All” with a much bigger circle corresponding to 165 in the contingency table. 12.5 Contingency table There is another option for assessing differences between 2x2 categorical data using Chi-squared tests or Fisher’s exact test. Although these methods essentially answers the same question: Name if there is differences in cluster membership given a binary explanatory variable. The methods are less general, as we can not account for confounders. Here we show the computation of those test using gender as explanatory variable on cluster 1 membership using a Chi-square test or Fisher’s exact test. library(data4consumerscience) data(pork) table2x2 &lt;- table(pork$Gender, pork$cluster1) table2x2 ## ## 0 1 ## 1 532 326 ## 2 562 208 prop.table(table2x2, margin = 1) ## ## 0 1 ## 1 0.6200466 0.3799534 ## 2 0.7298701 0.2701299 # 1 stands for row-wise percentages 12.5.1 Pearson Chi-square test Pearson’s Chi-square test applied to the contingency table. This test is used here to answer whether there are differences in distribution of counts between several clusters. For instance, the gender distribution as seen in the figure above indicates a higher relative proportion of females in the Low All cluster. This observation can be tested. conttb[16:17,] ## ## High Meat Low all High Dairy High All ## gender=Female 29 168 99 44 ## gender=Male 19 52 25 26 chisq.test(conttb[16:17,],correct = F) ## ## Pearson&#39;s Chi-squared test ## ## data: conttb[16:17, ] ## X-squared = 11.798, df = 3, p-value = 0.008109 The results indicates that gender indeed affects the cluster membership. 12.6 Correspondence Analysis In order to globally extract relations between clusters and demographics, correspondence analysis (CA) is a useful tool by which major trends in the data are visualized. Interpretation follows how biplots of PCA models are interpreted. See Interpreting model output in the chapter on PCA. library(FactoMineR) library(factoextra) res.ca &lt;- CA(conttb, graph = FALSE) fviz_ca_biplot(res.ca, repel = TRUE) Reading the CA plot is pretty simple. Those demographic characteristics in the direction of position of the cluster is positively associated with that cluster / segment. E.g. High all is associated with children 4 and 5, household 5, income 5 and age from \\(30\\) - \\(40.8\\) * Low all is children 1, education 5, females etc. "],["logistic-regression.html", "Chapter 13 Logistic Regression 13.1 Segmentation/Clustering 13.2 Fitting the logistic regression-model 13.3 Probabilities of segment membership: 13.4 Odds ratios 13.5 ORs and Probs 13.6 Effect of Age 13.7 Multivariate analysis 13.8 Segment 2 and 3 13.9 A new set of data 13.10 Logistic regression for demographic characterization 13.11 TASK 13.12 The tidyverse way 13.13 Comment", " Chapter 13 Logistic Regression Logistic regression is regression towards a binary response. In short regression were developed towards continuous responses, and is what is supported in R using the lm() function. However, there exits other type of responses, including binary, counts, etc. where the aim of the statistical analysis is still to use a regression type of model. This has given rise to the use of generalized linear models, where several response types can be analysis in a regression framework. In R this is done using the glm() function, extended with a specification of the response type using the family argument. Logistic regression aims to model the probability of observing case (versus control) given the input. The outputs from such a model is odds ratios. This video takes you through some basics. If you want to dive directly at logistic regression, then dive in from 6.53. In this chapter, we are going to use logistic regression for characterizing clusters. This is simply done by constructing binary classes based on the clusters as belong to cluster k versus not belong to cluster k. Even with more than two clusters binary endpoints can be constructed in this way. Alternatively, if there is one of the clusters which naturally is a reference cluster, then comparing the individual clusters to this one is also a way forward. The data used in this chapter is from the paper: Verbeke, Wim, Federico JA Pérez-Cueto, and Klaus G. Grunert. “To eat or not to eat pork, how frequently and how varied? Insights from the quantitative Q-PorkChains consumer survey in four European countries.” Meat science 88.4 (2011): 619-626. and can be found in the data4consumerscience-package as pork. library(data4consumerscience) data(pork) 13.1 Segmentation/Clustering In the dataset there are three clusters encoded as binary membership variables: cluster1, cluster2 and cluster3. They are mutually exclusive. These clusters are based on another clustering procedure than k-means, but in principle any clustering can be used to generate clusters… as long as they are meaningful. Here we just plot them to see how they appear on the two variables used for their construction. library(ggplot2) library(tableone) pork$clusters &lt;- pork$cluster1*1 + pork$cluster2*2 + pork$cluster3*3 plot(TotalPorkWeek ~ VarietyTotal, data = pork, col = pork[[&quot;clusters&quot;]] + 1) #Or using ggplot2: ggplot(data = pork, aes(x = VarietyTotal, y = TotalPorkWeek,color = factor(clusters))) + geom_point() + guides(color = guide_legend(&#39;Cluster&#39;)) + theme_linedraw() 13.2 Fitting the logistic regression-model The question we are going to adress here is is gender related to the being in cluster 1?. Other tools for this is covered in Profiling segments, but here we will see how logistic regression can be used for this. pork[[&quot;Gender&quot;]] &lt;- as.factor(pork[[&quot;Gender&quot;]]) # 1: Male, 2: Female ## Fitting logistic model logreg.1.gender &lt;- glm(cluster1 ~ Gender, data = pork, family = binomial) summary(logreg.1.gender) ## ## Call: ## glm(formula = cluster1 ~ Gender, family = binomial, data = pork) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9777 -0.9777 -0.7936 1.3912 1.6179 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7419 0.0537 -13.815 &lt; 2e-16 *** ## Gender1 0.2521 0.0537 4.695 2.67e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2060.3 on 1627 degrees of freedom ## Residual deviance: 2037.9 on 1626 degrees of freedom ## (5 observations deleted due to missingness) ## AIC: 2041.9 ## ## Number of Fisher Scoring iterations: 4 This model indicates Gender is related to cluster1 as the p-value is highly significant. 13.3 Probabilities of segment membership: One way to get the direction is to interpret the coefficients from the summary() output, but alternatively, simply using the model to predict membership probabilities will give similar insight. Here the probability of beloging to cluster 1 is predicted for the situation where Gender is \\(1\\) as well as \\(2\\). Be aware that such predictions should match with the data source used for building the model. Here that means, that if Gender is encoded with numbers (1 or 2), then the predictions are likewise done usign those. If the Gender is encoded with male and female, then that it the inputs when doing predictions. predict(logreg.1.gender, data.frame(Gender = &quot;1&quot;), type = &quot;response&quot;) ## 1 ## 0.3799534 predict(logreg.1.gender, data.frame(Gender = &quot;2&quot;), type = &quot;response&quot;) ## 1 ## 0.2701299 What we see it that the probablity of belonging to cluster1 is higher if the Gender is \\(1\\) (in reference to \\(2\\)). This is obviously in line with the results in Contingency table and Pearson Chi-square test in the former chapter. 13.4 Odds ratios Odds ratio is the terminology used for interpreting coefficients from a logistic regression model. In the direct read out of coefficients from summary() returns the natural logarithm of the oddsratio, and hence to get odds ratios the exponential function is used. exp(coef(summary(logreg.1.gender))[2, 1]) ## [1] 1.286736 exp(confint(logreg.1.gender)[2, ]) ## 2.5 % 97.5 % ## 1.158605 1.430157 Here is it seen that odds of belonging to cluster1 is in favor of gender \\(1\\) as both the estimate and its confidence bounds are above 1. 13.5 ORs and Probs The odds ratio for a binary predictor is given by: \\[ OR = \\frac{p_1(1-p_2)}{(1-p_1)p2}\\] where \\(p_1\\) and \\(p_2\\) are probabilities of females and males respectively. Here we use the observed probability predictions for females and males (see Probabilities of segment membership above) to calculate the OR given the formula: p1 &lt;- 0.38 p2 &lt;- 0.27 OR &lt;-(p1*(1-p2))/((1-p1)*p2) # put in the ratio-calculation here OR Which is exactly the same as the output from the exponential of the coeficient in the model. 13.6 Effect of Age Logistic regression can, as normal regression, also be used with continous predictors, such as Age. head(pork[, c(&quot;Age&quot;, &quot;cluster1&quot;)]) ## Age cluster1 ## 1 56 0 ## 2 28 0 ## 3 63 0 ## 4 52 1 ## 5 64 1 ## 6 53 0 logreg.1.age &lt;- glm(cluster1 ~ Age, data = pork, family = binomial) # as log(OR) coef(summary(logreg.1.age)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.843015043 0.191718815 -4.3971430 1.096851e-05 ## Age 0.002828027 0.004135367 0.6838636 4.940613e-01 # and as OR (only for the Age) exp(coef(summary(logreg.1.age))[2,1]) ## [1] 1.002832 The coefficient indicates a very weak tendency towards, that cluster1 has older individuals (the estimate is positive), but the inference for this results is very non-significant. 13.7 Multivariate analysis Evaluating the effect of country while adjusting for age and gender. 13.7.1 Descriptives First we look at the crude percentages of cluster1 membership for the different countries. tb &lt;- table(pork$COUNTRY, pork$cluster1) prop.table(tb,1) ## ## 0 1 ## 1 0.7820823 0.2179177 ## 3 0.7222222 0.2777778 ## 4 0.5289673 0.4710327 ## 6 0.6485149 0.3514851 It seems as if especially country \\(4\\) is higher in cluster 1 compared to the other. 13.7.2 Two nested models In order to investigate the effect of country, we make a model with country, and a model without country (But appart from that similar!). Then we compare the drop in ability to fit data using anova. This indicates whether country were an important factor. pork[[&quot;COUNTRY&quot;]] &lt;- as.factor(pork[[&quot;COUNTRY&quot;]]) # Model with country logreg.1.cntr_1 &lt;- glm(cluster1 ~ COUNTRY + Age + Gender, data = pork, family = binomial) # Model without country logreg.1.cntr_0 &lt;- glm(cluster1 ~ Age + Gender, data = pork, family = binomial) #Bedre/nemnmere end at lave to modeller? Bare at droppe den ene af dem? Det giver i hvert fald samme resultat... drop1(logreg.1.cntr_1, test = &quot;Chisq&quot;) ## Single term deletions ## ## Model: ## cluster1 ~ COUNTRY + Age + Gender ## Df Deviance AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 1970.0 1982.0 ## COUNTRY 3 2037.6 2043.6 67.577 1.409e-14 *** ## Age 1 1970.1 1980.1 0.089 0.7657 ## Gender 1 1992.7 2002.7 22.660 1.934e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #Eller er det bedre for forståelsen, at man laver begge? # Comparison anova(logreg.1.cntr_1,logreg.1.cntr_0, test = &#39;Chisq&#39;) ## Analysis of Deviance Table ## ## Model 1: cluster1 ~ COUNTRY + Age + Gender ## Model 2: cluster1 ~ Age + Gender ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 1622 1970.0 ## 2 1625 2037.6 -3 -67.577 1.409e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So the effect of country is strongly significant. 13.7.3 Coefficients Lets look at how similar/different the levels of education is. coef(summary(logreg.1.cntr_1)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.826120904 0.203377024 -4.0620169 4.865056e-05 ## COUNTRY1 -0.573279550 0.101338371 -5.6570827 1.539675e-08 ## COUNTRY2 -0.202964661 0.095555639 -2.1240469 3.366622e-02 ## COUNTRY3 0.635372034 0.090915771 6.9885789 2.776846e-12 ## Age 0.001316609 0.004418472 0.2979782 7.657198e-01 ## Gender1 0.268673119 0.056871908 4.7241798 2.310460e-06 The estimates for first level of country (1) is within (Intercept). The estimates for the remaining three levels are all IN CONTRAST to the intercept, and hence country 1. So it seems as if country 3,4 and 6 are different from country 1, but we have no idea on whether say country 3 is different from country 6 and so forth. 13.7.4 Re-level In order to get other pairs of contrast we can re-level the factor country, and repeat the model Here we re-level to the third level of country which is country==4. pork[[&#39;COUNTRY&#39;]] &lt;- relevel(pork[[&#39;COUNTRY&#39;]],3) logreg.1.cntr_1a &lt;- glm(cluster1 ~ COUNTRY + Age + Gender, data = pork, family = binomial) coef(summary(logreg.1.cntr_1a)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.826120904 0.203377024 -4.0620169 4.865056e-05 ## COUNTRY1 0.635372034 0.090915771 6.9885789 2.776846e-12 ## COUNTRY2 -0.573279550 0.101338371 -5.6570827 1.539675e-08 ## COUNTRY3 -0.202964661 0.095555639 -2.1240469 3.366622e-02 ## Age 0.001316609 0.004418472 0.2979782 7.657198e-01 ## Gender1 0.268673119 0.056871908 4.7241798 2.310460e-06 This can be repeated setting all levels as reference. But there is a more fair and easy solution. 13.7.5 All pairwise comparisons We want to compare all pairs of the \\(4\\) country levels. This is a multiple comparison task, and can be undertaken by the glht() function from the multcomp package. library(multcomp) summary(glht(logreg.1.cntr_1, linfct = mcp(COUNTRY = &quot;Tukey&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: glm(formula = cluster1 ~ COUNTRY + Age + Gender, family = binomial, ## data = pork) ## ## Linear Hypotheses: ## Estimate Std. Error z value Pr(&gt;|z|) ## 3 - 1 == 0 0.3703 0.1634 2.266 0.10568 ## 4 - 1 == 0 1.2087 0.1582 7.641 &lt; 0.001 *** ## 6 - 1 == 0 0.7142 0.1599 4.467 &lt; 0.001 *** ## 4 - 3 == 0 0.8383 0.1508 5.559 &lt; 0.001 *** ## 6 - 3 == 0 0.3438 0.1526 2.254 0.10860 ## 6 - 4 == 0 -0.4945 0.1466 -3.372 0.00416 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) cld(glht(logreg.1.cntr_1, linfct = mcp(COUNTRY = &quot;Tukey&quot;))) ## 1 3 4 6 ## &quot;a&quot; &quot;ab&quot; &quot;c&quot; &quot;b&quot; Here we get all \\(6\\) pairs of pairwise comparisons. And it appears as country 1 and 3 are not statistically different, as well as country 3 and 6 (\\(p&gt;0.1\\)). However, country 1 and 6. Does this compare with the percentages from the descriptive analysis? 13.8 Segment 2 and 3 Use the code above to conduct the same analysis for segment/cluster 2 and 3, and reveal Gender differences Age effect Country differences 13.9 A new set of data The data plantbaseddiet, found in the data4consumerscience-package constitute data from the following study: Reipurth, Malou FS, Lasse Hørby, Charlotte G. Gregersen, Astrid Bonke, and Federico JA Perez Cueto. “Barriers and facilitators towards adopting a more plant-based diet in a sample of Danish consumers.” Food quality and preference 73 (2019): 288-292. Here we use the clusters from the consumer segmentation analysis as provided in Segmentation - example 2 rm(list =ls()) library(data4consumerscience) data(&quot;plantbaseddiet&quot;) set.seed(123) res &lt;- kmeans(plantbaseddiet[,c(&#39;a_meat&#39;, &#39;a_dairy&#39;,&#39;a_eggs&#39;)], 4) res$centers ## a_meat a_dairy a_eggs ## 1 18.812500 5.729167 4.492188 ## 2 3.719886 5.167045 2.915909 ## 3 4.688508 18.854839 3.601815 ## 4 19.150000 21.850000 7.678571 plantbaseddiet$clusters &lt;- factor(res$cluster,labels = c(&#39;High Meat&#39;,&#39;Low all&#39;,&#39;High Dairy&#39;,&#39;High All&#39;)) # add as binary columns plantbaseddiet$cluster_highmeat &lt;- as.numeric(plantbaseddiet$clusters==&#39;High Meat&#39;) plantbaseddiet$cluster_lowall &lt;- as.numeric(plantbaseddiet$clusters==&#39;Low all&#39;) plantbaseddiet$cluster_highdairy &lt;- as.numeric(plantbaseddiet$clusters==&#39;High Dairy&#39;) plantbaseddiet$cluster_highall &lt;- as.numeric(plantbaseddiet$clusters==&#39;High All&#39;) 13.10 Logistic regression for demographic characterization 13.10.1 Age library(ggplot2) ggplot(data = plantbaseddiet, aes(clusters,age)) + geom_boxplot() mdl_age &lt;- glm(data = plantbaseddiet, clusters==&#39;High Meat&#39; ~ age) # This is essentially the same as: # res &lt;- glm(data = plantbaseddiet, cluster_highmeat ~ age) summary(mdl_age) ## ## Call: ## glm(formula = clusters == &quot;High Meat&quot; ~ age, data = plantbaseddiet) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.12297 -0.11221 -0.10634 -0.08971 0.93669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.137638 0.042711 3.223 0.00136 ** ## age -0.000978 0.001167 -0.838 0.40260 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.09336404) ## ## Null deviance: 43.013 on 461 degrees of freedom ## Residual deviance: 42.947 on 460 degrees of freedom ## AIC: 219.58 ## ## Number of Fisher Scoring iterations: 2 13.10.2 Gender ggplot(data = plantbaseddiet, aes(clusters,fill = gender)) + geom_bar(position = &#39;dodge&#39;) tb2 &lt;- table(plantbaseddiet$clusters,plantbaseddiet$gender) tb2 ## ## Female Male ## High Meat 29 19 ## Low all 168 52 ## High Dairy 99 25 ## High All 44 26 prop.table(tb2,margin = 1) ## ## Female Male ## High Meat 0.6041667 0.3958333 ## Low all 0.7636364 0.2363636 ## High Dairy 0.7983871 0.2016129 ## High All 0.6285714 0.3714286 mdl_gender &lt;- glm(data = plantbaseddiet, clusters==&#39;High Meat&#39; ~ gender) summary(mdl_gender) ## ## Call: ## glm(formula = clusters == &quot;High Meat&quot; ~ gender, data = plantbaseddiet) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.15574 -0.08529 -0.08529 -0.08529 0.91471 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.12052 0.01605 7.508 3.15e-13 *** ## gender1 -0.03522 0.01605 -2.194 0.0287 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.09253794) ## ## Null deviance: 43.013 on 461 degrees of freedom ## Residual deviance: 42.567 on 460 degrees of freedom ## AIC: 215.47 ## ## Number of Fisher Scoring iterations: 2 exp(coef(summary(mdl_gender))[2, 1]) ## [1] 0.9653913 13.11 TASK Try to exchange which cluster you are looking at to see relation to age and gender Try to look at some other characteristics in the dataset. Can you compare clusters against the variables a_meat, a_dairy or a_eggs using logistic regression? Why should you be cautios with interpreting these results? 13.12 The tidyverse way We want to produce the results from the Table 4 in the paper. In principle, this table is \\(11\\) survey likert scale answers against \\(4\\) clusters… I.e. \\(44\\) logistic regression models. If you do this in the traditional way it takes up tons of code, and even small changes will be time-consuming to implement. For instance, the first element of Table 4 is calculated as: mdl &lt;- glm(data = plantbaseddiet, clusters==&#39;High Dairy&#39; ~ o_prepar + age + gender + factor(education), family = binomial) exp(coef(summary(mdl))) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.1233580 1.606315 0.01208922 1.000010 ## o_prepar 1.0350811 1.112082 1.38342790 2.107518 ## age 1.0194420 1.008492 9.74805146 1.023044 ## gender1 1.2520173 1.137510 5.72266605 1.084461 ## factor(education)1 0.5138224 1.803580 0.32334359 1.295480 ## factor(education)2 1.0871876 1.300013 1.37521044 2.117052 ## factor(education)3 1.3334406 1.274791 3.27156968 1.266061 exp(confint(mdl)) ## 2.5 % 97.5 % ## (Intercept) 0.04731725 0.305777 ## o_prepar 0.83959291 1.274398 ## age 1.00255942 1.036456 ## gender1 0.97841139 1.624150 ## factor(education)1 0.12451725 1.417995 ## factor(education)2 0.66391303 1.912630 ## factor(education)3 0.85310374 2.280131 But we want all! So: Tidyverse and broom for the rescue! library(tidyverse) library(broom) tb &lt;- plantbaseddiet %&gt;% mutate(id = 1) %&gt;% # make a vector of 1&#39;s spread(clusters,id, fill = 0) %&gt;% # distribute the clustering into 4 new coloumns and make sure that the binary cluster vectors are 1 and 0. gather(plant_plantbaseddiet,answ,o_prepar:o_family) %&gt;% # long format for survey questions gather(cluster_type,cluster,`High Meat`:`High All`) %&gt;% # long format for clusters group_by(plant_plantbaseddiet,cluster_type) %&gt;% # loop over all questions and all clusters do(glm(data = ., cluster==1 ~ answ + factor(education) + age + gender, family = binomial) %&gt;% tidy(exponentiate = T,conf.int = T)) # perform logistic regression tb %&gt;% head() ## # A tibble: 6 × 9 ## # Groups: plant_plantbaseddiet, cluster_type [1] ## plant_plantbaseddiet cluster_type term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 o_easy_a High All (Inter… 0.0981 0.627 -3.70 2.14e-4 ## 2 o_easy_a High All answ 1.20 0.133 1.39 1.64e-1 ## 3 o_easy_a High All factor… 0.911 0.586 -0.158 8.74e-1 ## 4 o_easy_a High All factor… 1.49 0.271 1.46 1.44e-1 ## 5 o_easy_a High All factor… 1.01 0.265 0.0398 9.68e-1 ## 6 o_easy_a High All age 1.00 0.0109 0.307 7.59e-1 ## # ℹ 2 more variables: conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt; This table (tb) has all the results, with estimates being OR and with confidence intervals. However, there are too much information. For instance are the intercept as well as the covariates reported. We can tidy it even more: tb4 &lt;- tb %&gt;% ungroup %&gt;% filter(term==&#39;answ&#39;) %&gt;% mutate(OR = paste(round(estimate,2), &#39; (&#39;, round(conf.low,2),&#39;-&#39;, round(conf.high,2),&#39;)&#39;, sep = &#39;&#39;)) %&gt;% dplyr::select(plant_plantbaseddiet,cluster_type,OR) %&gt;% spread(cluster_type,OR) tb4 ## # A tibble: 11 × 5 ## plant_plantbaseddiet `High All` `High Dairy` `High Meat` `Low all` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 o_easy_a 1.2 (0.93-1.57) 1.07 (0.87-1.32) 1.07 (0.79-… 0.83 (0.… ## 2 o_family 1.14 (0.91-1.42) 0.96 (0.8-1.15) 1.18 (0.9-1… 0.91 (0.… ## 3 o_fulnes 1.48 (1.16-1.9) 0.91 (0.73-1.12) 1.66 (1.24-… 0.71 (0.… ## 4 o_health 0.69 (0.52-0.92) 1.21 (0.95-1.56) 0.82 (0.59-… 1.14 (0.… ## 5 o_prepar 1.28 (0.99-1.65) 1.04 (0.84-1.27) 1.32 (0.98-… 0.77 (0.… ## 6 o_price 1.11 (0.87-1.42) 1.01 (0.83-1.24) 1.19 (0.89-… 0.88 (0.… ## 7 o_protei 1.93 (1.48-2.55) 0.84 (0.68-1.04) 1.74 (1.29-… 0.65 (0.… ## 8 o_restau 0.88 (0.65-1.18) 1.06 (0.84-1.34) 0.99 (0.7-1… 1.02 (0.… ## 9 o_social 0.98 (0.75-1.28) 0.94 (0.75-1.16) 1.39 (1.01-… 0.94 (0.… ## 10 o_sustai 0.69 (0.52-0.91) 1.15 (0.91-1.47) 0.8 (0.58-1… 1.21 (0.… ## 11 o_taste 0.82 (0.63-1.08) 1.09 (0.87-1.39) 0.59 (0.43-… 1.29 (1.… Now this just needs to be exported to excel (use e.g. export() from the rio package), and a bit of love to be ready for a publication. 13.13 Comment Try to understand each of the commands in the pipeline, what they do and why they make sense. You can put in a hash-tag (#) before the the pipe sign (%&gt;%) to block the pipeline and see the structure of the output at this point. "],["cata-data-check-all-that-apply.html", "Chapter 14 CATA data (Check-All-That-Apply) 14.1 Importing and looking at the beer data 14.2 Two versions of the data 14.3 Cochran’s Q test 14.4 PCA on CATA data", " Chapter 14 CATA data (Check-All-That-Apply) Check-All-That-Apply (CATA) data is in its raw form binary indicating whether a participant finds a product to have the attribute (1) or not (0). Usually, such data is organized in a matrix where each row corresponds to the evaluation of one product by one respondent (sensory panelist, consumer or other). The columns describe the product ID/properties, respondent number and the attributes. Say you for instance have 26 participants and 4 products, and further that all products are evaluated by all respondents once on 13 attributes. Your data matrix would then have 104 rows and 13 columns (with responses) and additionally columns indicating respondent, product, record id, date, etc. In the CATA section of this book, we will use a data set with Beer. The data originates from Giacalone, Bredie, and Frøst (2013). They consist of evaluation of six different commercial beers from Danish craft brewers evaluated by \\(160\\) consumers on a range of different questions: Sensory properties: the consumers’ response to 27 sensory descriptors (s_descriptor), some of which are super-ordinate and others more detailed. Includes information about beer and respondent. One line for each beer (6) x consumer (160) . This dataset is called beercata. Background information: a range of questions about consumers’ demography, food neophobia, beer knowledge and use, also including appropriateness ratings (a descriptor) for 27 sensory descriptors on a 7-points scale (e.g. how appropriate do you think it is for a beer to be bitter?). The two semantic anchors were 1 = not at all appropriate and 7 = extremely appropriate. This dataset is called beerbackground. Hedonics: Their hedonic responses to the beer on a 7-point hedonic scale (1-7). This dataset is called beerliking. 14.1 Importing and looking at the beer data The data appear as a part of the data4consumerscience package (see Import data from R-package) We first have to import or load the data. Here is the import, remember to change the path for the Excel file to match your own settings: library(readxl) beercata &lt;- read_excel(&#39;DatasetRbook.xlsx&#39;,sheet = &#39;BeerCATA&#39;) Beerbackground &lt;- read_excel(&#39;DatasetRbook.xlsx&#39;,sheet = &#39;BeerBackground&#39;) beerliking &lt;- read_excel(&#39;DatasetRbook.xlsx&#39;,sheet = &#39;BeerLiking&#39;) The packages you need to run the analyses are activated with the library function and the package name. If the package is not installed, please do this first: library(data4consumerscience) library(tidyverse) data(&quot;beercata&quot;) beercata %&gt;% head() ## Consumer.ID Beer S_Flowers S_Beans S_Intense berries S_Caramel ## 1 a01 Brown Ale 0 0 0 0 ## 2 a01 NY Lager 0 1 1 0 ## 3 a01 Porse Bock 0 0 0 0 ## 4 a01 Ravnsborg Red 0 0 0 0 ## 5 a01 River Beer 0 0 1 0 ## 6 a01 Wheat IPA 0 0 0 0 ## S_Nuts S_Savoury spices S_Dessert spices S_Regional spices S_Herbs ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 0 0 1 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 ## 6 1 0 0 0 1 ## S_Citrus fruit S_Berries S_Fruit S_Dried fruit S_Liquor S_Bitter S_Sparkling ## 1 0 0 1 0 0 0 0 ## 2 0 0 0 0 1 1 0 ## 3 0 0 0 0 0 1 0 ## 4 0 0 0 0 1 1 0 ## 5 0 0 0 0 1 1 1 ## 6 0 0 0 0 0 1 0 ## S_Refreshing S_Fruity S_Aromatic S_Pungent S_Still S_Smoked S_Foamy S_Sour ## 1 1 0 1 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 ## 3 0 0 1 0 0 0 0 1 ## 4 0 0 1 0 0 0 0 1 ## 5 0 0 0 0 0 0 0 1 ## 6 0 0 1 1 0 0 0 0 ## S_Sweet S_Warming S_Vinous ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 str(beercata) ## &#39;data.frame&#39;: 960 obs. of 29 variables: ## $ Consumer.ID : chr &quot;a01&quot; &quot;a01&quot; &quot;a01&quot; &quot;a01&quot; ... ## $ Beer : chr &quot;Brown Ale&quot; &quot;NY Lager&quot; &quot;Porse Bock&quot; &quot;Ravnsborg Red&quot; ... ## $ S_Flowers : num 0 0 0 0 0 0 0 0 0 1 ... ## $ S_Beans : num 0 1 0 0 0 0 0 0 0 0 ... ## $ S_Intense berries: num 0 1 0 0 1 0 1 0 0 0 ... ## $ S_Caramel : num 0 0 0 0 0 0 1 0 0 0 ... ## $ S_Nuts : num 0 0 0 0 0 1 0 0 0 0 ... ## $ S_Savoury spices : num 0 0 0 0 0 0 0 0 0 0 ... ## $ S_Dessert spices : num 0 0 0 0 0 0 1 0 0 0 ... ## $ S_Regional spices: num 0 0 0 0 0 0 0 0 1 0 ... ## $ S_Herbs : num 0 0 1 0 0 1 0 0 0 0 ... ## $ S_Citrus fruit : num 0 0 0 0 0 0 0 1 0 0 ... ## $ S_Berries : num 0 0 0 0 0 0 0 0 0 0 ... ## $ S_Fruit : num 1 0 0 0 0 0 0 1 0 0 ... ## $ S_Dried fruit : num 0 0 0 0 0 0 0 0 0 0 ... ## $ S_Liquor : num 0 1 0 1 1 0 0 0 0 0 ... ## $ S_Bitter : num 0 1 1 1 1 1 0 0 0 1 ... ## $ S_Sparkling : num 0 0 0 0 1 0 0 0 0 0 ... ## $ S_Refreshing : num 1 0 0 0 0 0 0 0 0 0 ... ## $ S_Fruity : num 0 0 0 0 0 0 1 1 0 0 ... ## $ S_Aromatic : num 1 0 1 1 0 1 1 1 0 1 ... ## $ S_Pungent : num 0 0 0 0 0 1 0 0 0 0 ... ## $ S_Still : num 0 0 0 0 0 0 0 0 0 0 ... ## $ S_Smoked : num 0 0 0 0 0 0 1 0 0 0 ... ## $ S_Foamy : num 0 0 0 0 0 0 1 1 0 0 ... ## $ S_Sour : num 0 0 1 1 1 0 1 0 0 1 ... ## $ S_Sweet : num 0 0 0 0 0 0 0 1 0 0 ... ## $ S_Warming : num 0 0 0 0 0 0 0 0 1 0 ... ## $ S_Vinous : num 0 0 0 0 0 0 0 0 0 0 ... table(beercata$Beer) ## ## Brown Ale NY Lager Porse Bock Ravnsborg Red River Beer ## 160 160 160 160 160 ## Wheat IPA ## 160 length(unique(beercata$Consumer.ID)) ## [1] 160 From the above functions, you can see the data structure. Using the str() function will give you all the variable names. The lentgh() function is counting the number of participants as we have asked for the Consumer.ID variable in the dataset beercata. 14.2 Two versions of the data For analyses of CATA data, we need to versions of the data: Raw data (binary, 0/1) with each row being responses from one evaluation (beercata dataset) Agglomerated to counts, with each row being one product The agglomerated version of the counts is computed by: beercatasum &lt;- beercata %&gt;% gather(attrib, val, S_Flowers:S_Vinous) %&gt;% group_by(Beer,attrib) %&gt;% dplyr::summarize(n = sum(val)) %&gt;% spread(attrib,n) beercatasum ## # A tibble: 6 × 28 ## # Groups: Beer [6] ## Beer S_Aromatic S_Beans S_Berries S_Bitter S_Caramel `S_Citrus fruit` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brown Ale 57 54 6 63 52 10 ## 2 NY Lager 37 10 5 69 16 30 ## 3 Porse Bock 20 2 5 67 7 34 ## 4 Ravnsborg Red 52 25 6 71 35 11 ## 5 River Beer 22 12 4 65 3 29 ## 6 Wheat IPA 29 9 9 57 9 30 ## # ℹ 21 more variables: `S_Dessert spices` &lt;dbl&gt;, `S_Dried fruit` &lt;dbl&gt;, ## # S_Flowers &lt;dbl&gt;, S_Foamy &lt;dbl&gt;, S_Fruit &lt;dbl&gt;, S_Fruity &lt;dbl&gt;, ## # S_Herbs &lt;dbl&gt;, `S_Intense berries` &lt;dbl&gt;, S_Liquor &lt;dbl&gt;, S_Nuts &lt;dbl&gt;, ## # S_Pungent &lt;dbl&gt;, S_Refreshing &lt;dbl&gt;, `S_Regional spices` &lt;dbl&gt;, ## # `S_Savoury spices` &lt;dbl&gt;, S_Smoked &lt;dbl&gt;, S_Sour &lt;dbl&gt;, S_Sparkling &lt;dbl&gt;, ## # S_Still &lt;dbl&gt;, S_Sweet &lt;dbl&gt;, S_Vinous &lt;dbl&gt;, S_Warming &lt;dbl&gt; We call our new dataset beercatasum. Gather all the variables from S_Flower to S_Vinous and call them attrib. Group all data by the Beer variable (sample name column) and attrib (all of our CATA variables), then sum up the values (val) and call them n. Make a table of attrib and n. Save it all as the new name beercatasum. Then finally shown us the new data set. … and visualized by for instance a barplot. # summary counts over attribute beercatasum %&gt;% gather(attrib, n, S_Aromatic:S_Warming) %&gt;% ggplot(data = ., aes(x = attrib, y = n, fill = Beer)) + geom_bar(stat = &#39;identity&#39;, position = position_dodge()) + coord_flip() To plot all attributes, we need a long data format (discussed in more detail in chapter 1 in the section Edit using Tidyverse). gather creates the long format, which is then added as the first input in the plot (represented by “.”). The different attributes are depicted on the x-axis, with the summed up values grouped by beer-type on the y-axis. The last line (coord_flip()) flips the plot, to make the attributes more readable. For more plot types go to the Chapter on Plotting data. 14.3 Cochran’s Q test Cochran’s Q test is a statistical test for the comparison of several products, where the response is binary, and there is repeated responses across several judges. We need the package RVAideMemoire.The data needs to be structured as the beercata is. We can only run the model independently for one variable at a time. For one response variable: S_Flowers library(RVAideMemoire) m &lt;- cochran.qtest(S_Flowers ~ Beer | Consumer.ID, data = beercata) m ## ## Cochran&#39;s Q test ## ## data: S_Flowers by Beer, block = Consumer.ID ## Q = 63.252, df = 5, p-value = 2.581e-12 ## alternative hypothesis: true difference in probabilities is not equal to 0 ## sample estimates: ## proba in group &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 0.02500 0.28750 0.16875 0.10625 0.13750 ## &lt;NA&gt; ## 0.28125 The p.value is strongly significant, indicating that we cannot assume the same level of S_Flower in all beers. I.e. the beers seems different based on this characteristics. This is in agreement with the barplot above, where S_Flower is high in NY Lager and really low for Brown ale. But in reality we only know that the beers are different overall, not which specific beers that are different. For this we need at post hoc test. 14.3.1 Post hoc test As we observe differences based on this attribute, we pursue the question on which products stick out? And are there products which are similar? This is done by pairwise comparisons, for this we need the package rcompanion: library(rcompanion) PT &lt;- pairwiseMcnemar(S_Flowers ~ Beer | Consumer.ID, data = beercata, test = &quot;permutation&quot;, method = &quot;fdr&quot;, digits = 3) PT$Pairwise %&gt;% arrange(-abs(as.numeric(Z))) %&gt;% data.frame() ## Comparison Z p.value p.adjust ## 1 Brown Ale - NY Lager = 0 -6.48 9.13e-11 1.37e-09 ## 2 Brown Ale - Wheat IPA = 0 -5.86 4.71e-09 3.53e-08 ## 3 Brown Ale - Porse Bock = 0 -4.27 1.95e-05 9.75e-05 ## 4 NY Lager - Ravnsborg Red = 0 4.14 3.43e-05 1.29e-04 ## 5 Ravnsborg Red - Wheat IPA = 0 -3.96 7.5e-05 2.25e-04 ## 6 NY Lager - River Beer = 0 3.54 0.000402 8.48e-04 ## 7 Brown Ale - River Beer = 0 -3.53 0.000415 8.48e-04 ## 8 River Beer - Wheat IPA = 0 -3.51 0.000452 8.48e-04 ## 9 Brown Ale - Ravnsborg Red = 0 -2.98 0.00286 4.77e-03 ## 10 NY Lager - Porse Bock = 0 2.52 0.0118 1.77e-02 ## 11 Porse Bock - Wheat IPA = 0 -2.36 0.0181 2.47e-02 ## 12 Porse Bock - Ravnsborg Red = 0 1.54 0.123 1.54e-01 ## 13 Porse Bock - River Beer = 0 0.845 0.398 4.26e-01 ## 14 Ravnsborg Red - River Beer = 0 -0.845 0.398 4.26e-01 ## 15 NY Lager - Wheat IPA = 0 0.135 0.893 8.93e-01 The first part of the code is conducting the Post hoc test, whereas the last part of the code sorts the table from highest to lowest numerical value of Z. This is done to ease the interpretation of the table. Most products are significantly different, while Porse Bock and Ravnsborg Red are fairly alike. This is determined by looking at the adjusted p-value, and checking whether it is exceed the desired \\[\\alpha\\]-level. The adjusted p-values are used, as they have been adjusted to compensate for the fact that multiple pairwise comparisons are conducted. 14.3.2 For all attributes in one run (nice to know) We use the packages tidyverse and broom for this, but need a function capable of handling Cochran’s Q-test outputs. library(broom) tidy.RVtest &lt;- function(m){ r &lt;- data.frame(statistic = m$statistic,df = m$parameter, p.value= m$p.value, method = m$method.test) return(r) } tb_cochran &lt;- beercata %&gt;% gather(attrib, val, S_Flowers:S_Vinous) %&gt;% group_by(attrib) %&gt;% do(cochran.qtest(val ~ Beer | Consumer.ID, data = .) %&gt;% tidy) tb_cochran %&gt;% arrange(p.value) ## # A tibble: 27 × 5 ## # Groups: attrib [27] ## attrib statistic df p.value method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 S_Beans 114. 5 7.19e-23 Cochran&#39;s Q test ## 2 S_Caramel 108. 5 1.08e-21 Cochran&#39;s Q test ## 3 S_Flowers 63.3 5 2.58e-12 Cochran&#39;s Q test ## 4 S_Aromatic 45.8 5 9.90e- 9 Cochran&#39;s Q test ## 5 S_Sweet 36.3 5 8.24e- 7 Cochran&#39;s Q test ## 6 S_Warming 35.9 5 1.01e- 6 Cochran&#39;s Q test ## 7 S_Smoked 33.9 5 2.47e- 6 Cochran&#39;s Q test ## 8 S_Liquor 33.9 5 2.55e- 6 Cochran&#39;s Q test ## 9 S_Citrus fruit 29.4 5 1.96e- 5 Cochran&#39;s Q test ## 10 S_Dried fruit 26.0 5 8.81e- 5 Cochran&#39;s Q test ## # ℹ 17 more rows Again the table is sorted with the most sigficant at the top, and the least significant at the bottom. This output indicates that S_Beans is the most discriminatory attribute, while S_Pungent is the least. For the pairwise comparisons, please apply the code for the Tukey test above per attribute. 14.4 PCA on CATA data For an introduction PCA, please go to the Chapter Introduction to PCA and multivariate data in the first part of the book. A PCA on the summed CATA counts will reveal the attributes associated with the individual products: mdlPCA &lt;- prcomp(beercatasum[,-1], scale. = T) ggbiplot::ggbiplot(mdlPCA, labels = beercatasum$Beer) The figure shows the scores (beers, in black) and loadings (sensory descriptors, dark red) from the PCA’s first two components. These two components describe 45.5% + 27.2% = 72.7% of the total variance in the summed data. From the positions of the beers it is clear that the beers are similar in three pairs – Brown Ale and Ravnsborg Red, left side of panel: Porse Bock and River Beer, upper right side; NY Lager and Wheat IPA, lower right part of panel. The sensory descriptors S_Beans, Dried fruit, Caramel, Warming, Aromatic etc. are all is associated to the beer Brown ale, while Berries, Herb_Savory, Dessert, Pungent etc. is characteristic for Wheat IPA, and to some degree also NY Lager. Sour and Sparkling are the two descriptors that are most frequently mentioned for River Beer and also Porse Bock. You can check the analysis by cross checking with the summed CATA table. Descriptors that are located very close to each other show very similar patterns of response, i.e. they are highly correlated. An example is Caramel and Warming, which do not have same summed scores but the same pattern of high and low across samples. From the figure we can extract the overall conclusion of similarity and differences among samples, but we need Cochran’s Q to describe which descriptors that can be used to statistically separate the samples, and which samples are different for said descriptor. "],["cata-and-hedonics.html", "Chapter 15 CATA and Hedonics 15.1 Individual attributes and liking 15.2 PCA on CATA and Liking", " Chapter 15 CATA and Hedonics Certain attributes like floral is wanted in beer, while others may be unwanted, and hence detection of these attributes can have hedonic impact. This analysis can be evaluated for each attribute separately or all collectively using PCA. These data appears as two separate dataset, and hence need to be joint: library(data4consumerscience) library(tidyverse) data(&quot;beercata&quot;) data(&quot;beerliking&quot;) xbeer &lt;- beerliking %&gt;% left_join(beercata, by = c(&#39;Beer&#39;,&#39;Consumer.ID&#39;)) 15.1 Individual attributes and liking 15.1.1 An example with Refreshing ggplot(data = xbeer, aes(Beer, Liking, fill = factor(S_Refreshing))) + geom_boxplot() From this plot it seems as if checking of the attribute Refreshing leads to higher liking score. Further, this is even more so for light-colored beers like Wheat IPA and Porse Bock. A model capturing this phenomena could be a linear model with interactions: library(lme4) library(lmerTest) mdl &lt;- lmer(data = xbeer, Liking~factor(S_Refreshing)*Beer + (1|Consumer.ID)) anova(mdl) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## factor(S_Refreshing) 316.30 316.30 1 901.60 141.9478 &lt; 2.2e-16 *** ## Beer 75.61 15.12 5 783.91 6.7864 3.309e-06 *** ## factor(S_Refreshing):Beer 48.91 9.78 5 877.05 4.3894 0.0005933 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All terms are significant and we are interested in the individual beer effects. We re-parametrice the model by removing the intercept (adding -1 at the end of the formula) and not including the main effect of S_Refreshing (change * to : in interaction, and add Beer as main effect). mdl &lt;- lmer(data = xbeer, Liking~Beer + factor(S_Refreshing):Beer + (1|Consumer.ID) - 1) summary(mdl) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Liking ~ Beer + factor(S_Refreshing):Beer + (1 | Consumer.ID) - 1 ## Data: xbeer ## ## REML criterion at convergence: 3446.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.87114 -0.67445 0.08872 0.70059 2.46378 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Consumer.ID (Intercept) 0.2706 0.5202 ## Residual 2.2283 1.4928 ## Number of obs: 920, groups: Consumer.ID, 155 ## ## Fixed effects: ## Estimate Std. Error df t value ## BeerBrown Ale 5.1235 0.1713 905.0570 29.905 ## BeerNY Lager 4.5212 0.1471 891.2430 30.733 ## BeerPorse Bock 4.2747 0.1322 869.6115 32.335 ## BeerRavnsborg Red 4.9078 0.1350 874.8096 36.349 ## BeerRiver Beer 4.2451 0.1332 872.0429 31.864 ## BeerWheat IPA 4.2981 0.1329 871.0139 32.332 ## BeerBrown Ale:factor(S_Refreshing)1 -0.3340 0.1694 883.0668 -1.972 ## BeerNY Lager:factor(S_Refreshing)1 -0.5176 0.1449 882.6727 -3.573 ## BeerPorse Bock:factor(S_Refreshing)1 -0.7714 0.1297 883.4564 -5.947 ## BeerRavnsborg Red:factor(S_Refreshing)1 -0.7095 0.1326 883.8535 -5.352 ## BeerRiver Beer:factor(S_Refreshing)1 -0.6040 0.1308 884.3471 -4.618 ## BeerWheat IPA:factor(S_Refreshing)1 -1.2025 0.1304 882.8483 -9.219 ## Pr(&gt;|t|) ## BeerBrown Ale &lt; 2e-16 *** ## BeerNY Lager &lt; 2e-16 *** ## BeerPorse Bock &lt; 2e-16 *** ## BeerRavnsborg Red &lt; 2e-16 *** ## BeerRiver Beer &lt; 2e-16 *** ## BeerWheat IPA &lt; 2e-16 *** ## BeerBrown Ale:factor(S_Refreshing)1 0.048964 * ## BeerNY Lager:factor(S_Refreshing)1 0.000372 *** ## BeerPorse Bock:factor(S_Refreshing)1 3.92e-09 *** ## BeerRavnsborg Red:factor(S_Refreshing)1 1.11e-07 *** ## BeerRiver Beer:factor(S_Refreshing)1 4.44e-06 *** ## BeerWheat IPA:factor(S_Refreshing)1 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## BrBrwA BrNYLg BrPrsB BrRvnR BrRvrB BrWIPA BBA:(S BNYL:( BPB:(S ## BeerNYLager 0.069 ## BeerPorsBck 0.080 0.090 ## BrRvnsbrgRd 0.079 0.089 0.099 ## BeerRiverBr 0.076 0.090 0.099 0.096 ## BeerWhetIPA 0.078 0.091 0.100 0.099 0.099 ## BrBA:(S_R)1 -0.664 0.000 -0.004 -0.005 0.000 -0.001 ## BNYL:(S_R)1 0.001 -0.496 0.001 -0.002 -0.001 -0.002 0.000 ## BrPB:(S_R)1 -0.009 0.002 -0.268 -0.003 -0.002 0.000 0.014 -0.005 ## BrRR:(S_R)1 -0.009 -0.003 -0.003 -0.332 0.003 -0.004 0.014 0.006 0.010 ## BrRB:(S_R)1 0.003 -0.002 -0.001 0.004 -0.293 -0.001 -0.001 0.004 0.008 ## BWIPA:(S_R) -0.002 -0.003 0.001 -0.004 -0.001 -0.276 0.003 0.006 -0.002 ## BRR:(S BRB:(S ## BeerNYLager ## BeerPorsBck ## BrRvnsbrgRd ## BeerRiverBr ## BeerWhetIPA ## BrBA:(S_R)1 ## BNYL:(S_R)1 ## BrPB:(S_R)1 ## BrRR:(S_R)1 ## BrRB:(S_R)1 -0.009 ## BWIPA:(S_R) 0.012 0.004 Here we see that the attribute will increase the liking by \\(0.7\\) points for Brown Ale and up to \\(2.4\\) points for Wheat IPA. 15.1.2 All attributes We can visualize for all attributes: g1 &lt;- xbeer %&gt;% gather(cata, val,S_Flowers:S_Vinous) %&gt;% ggplot(data = ., aes(Beer, Liking, fill = factor(val))) + geom_boxplot() + theme(axis.text.x = element_text(angle = 79, hjust = 1), legend.position = &#39;bottom&#39;) + facet_wrap(~cata, ncol = 4) g1 For each attribute a interaction model will be used to qualify further analysis. library(broom) library(broom.mixed) tb &lt;- xbeer %&gt;% gather(cata, val,S_Flowers:S_Vinous) %&gt;% group_by(cata) %&gt;% do(lmer(data = ., Liking~factor(val)*Beer + (1|Consumer.ID)) %&gt;% anova %&gt;% tidy) This table has both main effect of the beer and cata-attribute as well their interaction. This print out shows the attributes related to liking - overall. tb %&gt;% filter(term==&#39;factor(val)&#39;) %&gt;% arrange(p.value) ## # A tibble: 27 × 8 ## # Groups: cata [27] ## cata term sumsq meansq NumDF DenDF statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 S_Refreshing factor(val) 316. 316. 1 902. 142. 1.70e-30 ## 2 S_Sour factor(val) 129. 129. 1 905. 51.2 1.73e-12 ## 3 S_Caramel factor(val) 23.6 23.6 1 899. 8.92 2.90e- 3 ## 4 S_Herbs factor(val) 23.3 23.3 1 905. 8.84 3.02e- 3 ## 5 S_Sweet factor(val) 21.8 21.8 1 897. 8.22 4.25e- 3 ## 6 S_Regional spices factor(val) 19.4 19.4 1 903. 7.43 6.54e- 3 ## 7 S_Sparkling factor(val) 19.3 19.3 1 865. 7.33 6.90e- 3 ## 8 S_Bitter factor(val) 17.9 17.9 1 907. 6.76 9.45e- 3 ## 9 S_Dessert spices factor(val) 16.5 16.5 1 908. 6.28 1.24e- 2 ## 10 S_Liquor factor(val) 16.0 16.0 1 908. 6.00 1.45e- 2 ## # ℹ 17 more rows The strongest ones are S_Refreshing, S_Sour, etc. while the presence of S_Warming, S_Pungent etc. has no effect on liking. tb %&gt;% filter(term==&#39;factor(val):Beer&#39;) %&gt;% arrange(p.value) %&gt;% head(6) ## # A tibble: 6 × 8 ## # Groups: cata [6] ## cata term sumsq meansq NumDF DenDF statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 S_Refreshing factor(val):Beer 48.9 9.78 5 877. 4.39 0.000593 ## 2 S_Fruity factor(val):Beer 55.8 11.2 5 873. 4.30 0.000711 ## 3 S_Nuts factor(val):Beer 32.2 6.44 5 872. 2.44 0.0329 ## 4 S_Smoked factor(val):Beer 30.2 6.05 5 876. 2.29 0.0441 ## 5 S_Citrus fruit factor(val):Beer 29.2 5.84 5 879. 2.21 0.0510 ## 6 S_Regional spices factor(val):Beer 27.8 5.57 5 873. 2.13 0.0595 This table shows that S_Refreshing, S_Fruity, etc. has differential impact on the liking dependent on the beer it is detected in. A deep dive into the effect can be done using the setup with a single variable above using the tidyverse and broom framework. tb2 &lt;- xbeer %&gt;% gather(cata, val,S_Flowers:S_Vinous) %&gt;% group_by(cata) %&gt;% do(lmer(data = ., Liking~Beer + factor(val):Beer + (1|Consumer.ID) - 1) %&gt;% tidy(conf.int = T)) tb2 %&gt;% filter(str_detect(term,&#39;factor&#39;)) %&gt;% # filter to get individual beer differences. filter(cata %in% tb$cata[tb$term==&#39;factor(val):Beer&#39; &amp; tb$p.value&lt;0.05]) %&gt;% # include only interesting ones. ggplot(data = ., aes(substr(term,5,14), estimate, ymin = conf.low, ymax = conf.high)) + geom_point() + geom_errorbar() + facet_wrap(~cata) + geom_hline(yintercept = 0) + theme(axis.text.x = element_text(angle = 79, hjust = 1)) + xlab(&#39;Beer&#39;) The interpretation is that detecting Smoked in e.g. Ravnsborg Red tends to be positive, while it is pretty bad in Wheat IPA. Similarly, Fruity is related to higher liking in Wheat IPA and Porse Bock but a bad thing in River Beer. Nutty only matters on liking in Ravnsborg Red and River beer. 15.2 PCA on CATA and Liking A PCA on the agglomerated CATA counts including the liking will reveal the attributes associated with the individual products, and which attributes are correlated with the liking. xbeeragglom &lt;- xbeer %&gt;% gather(cata, val,S_Flowers:S_Vinous, Liking) %&gt;% #looong format for all variables group_by(Beer,cata) %&gt;% # summarize for each beer type dplyr::summarise(yy = mean(val, na.rm = T)) %&gt;% spread(cata,yy) # wide format mdlPCAcataliking &lt;- prcomp(xbeeragglom[,-1], scale. = T) ggbiplot::ggbiplot(mdlPCAcataliking, labels = xbeeragglom$Beer) The attributes Bean, Caramel, Warming, Aromatic etc is associated to the beer Brown ale, while Berrie, Dessert, Pungent, etc. is characteristic of Wheat IPA. Further, Liking is associated with all attributes on the first component, such as Aromatic, Warming, etc. 15.2.1 A beer centric model The above PCA shows general good- and bad attributes in terms of liking. However, the univariate analysis indicated that for some beers e.g. Smoked was a good thing while for others not so much. For that reason we can build a PCA for each beer to see which attributes that drives liking xbeer &lt;- xbeer[complete.cases(xbeer),] PCAmdl_RR &lt;- prcomp(xbeer[xbeer$Beer==&#39;Ravnsborg Red&#39;,14:41], scale. = T) ggbiplot::ggbiplot(PCAmdl_RR) Caramel, Savoury spices and Reefreshing is promoting liking, while Sour is not. "],["preference-mapping.html", "Chapter 16 Preference Mapping 16.1 Example of Preference Mapping 16.2 PCA 16.3 Analysis by PLS 16.4 L-PLS [For the future…]", " Chapter 16 Preference Mapping [some narrative] 16.1 Example of Preference Mapping [Which???] 16.2 PCA PCA is a nice tool to get overview of structure in data. Here we explicitly are interested in hedonic liking of the 6 beer types, and whether there are certain beer-drinker profiles, such as some prefer dark beer, while others like wheat or pilsner. The liking data is in long format, and as we want to see correlation between different beers we need to wrap the liking into wide format, this can be done using spread from tidyverse. Further, there is incomplete liking data, and here we only sustain hedonic answers from consumers with all 6 liking answers. This filter can be computed in different ways, here drop_na() is used. library(data4consumerscience) library(tidyverse) data(&quot;beerliking&quot;) xbeerliking &lt;- beerliking %&gt;% spread(Beer,Liking) %&gt;% # make into wide format drop_na() PCA is computed on the liking columns of this matrix mdlPCA &lt;- prcomp(xbeerliking[,13:18]) ggbiplot::ggbiplot(mdlPCA) Those who like Ravnsborg red also likes NY Lager and to some extend Brown ale, while Porse Bock and Wheat IPA also attracts the same consumers. In general there is a trend towards all liking score being positively correlated, meaning, that costumers overall like (or dis like) beer. This can both be a real phenomena, but also an artifact of the consumers not using the scale in a similar fashion. It is a very common phenomena for sensory and hedonic data. We can glue on demographic characteristics, such as age, gender, etc., as well as questions on interest in food and beer on this figure to understand the consumer population. ggbiplot::ggbiplot(mdlPCA, groups = xbeerliking$Gender, ellipse = T) ggbiplot::ggbiplot(mdlPCA, groups = factor(xbeerliking$`Beer knowledge`), ellipse = T) In general, the classical demographics do not relate to liking patterns, as shown by gender above. Try the others to confirm. For interest in food and beer there are patterns. One example is the Beer knowledge with higher liking scores for more beer knowledge. Similar intuitive patterns can be seen for some of the other characteristics. 16.3 Analysis by PLS Predictors can be objective characteristics of the products or CATA type data, while response is hedonik liking data. [minimum 5 samples X = CATA (Beer_XYZmatrix.xlsx, sheet = X CATA (coll.)+Y liking (aver.)), Y = Living average Y2 = Liking for each consumer = t(Y (long thin)) Objective = Visualize to get patterns related to liking, and which deescriptors are merely irrelevant. 16.4 L-PLS [For the future…] "],["hedonic-rating-e.g.-liking-scores.html", "Chapter 17 Hedonic rating (e.g. liking scores) 17.1 Plotting liking scores 17.2 Simple mixed models 17.3 Multivariable models", " Chapter 17 Hedonic rating (e.g. liking scores) An evaluation of how much we like a food or drink is a hedonic response. Often this is given as a number on a scale, or by checking a category box with a description, such as the 9-point hedonic scale. For data analysis, the boxes are made numerical, with the assumption the there is equidistance between the meaning of the category labels, i.e dislike extremely and dislike very much is as different as dislike a little and neutral. The original English language anchors for the 9-point hedonic scale was develop and validated on tests using approximately 5500 American soldiers in the beginning of the 1950’s (Lawless &amp; Heymann, Chapter 7 Scaling, 2010). There are a number of validated scales, including visual smiley scales for children. Dependent on the respondent 5, 7 or 9 point scales are used. Most often we want to know if the hedonic response is significantly different (not just different by chance) depending on the samples. We might also want to know if other factors have an influence on the hedonic rating. e.g. household income or sex. We also want to know what are the actual differences are in numerical size. All this requires different statistical estimates. 17.1 Plotting liking scores For the beer data we have the liking in a long matrix. library(data4consumerscience) library(ggplot2) data(beerliking) A histogram of the likings shows that some are symmetric ( NY Lager, River Beer and to some extend Porse Bock), while Brown Ale and Ravnsborg Red is skeew, and Wheat IPA is uniform. ggplot(data = beerliking, aes(Liking)) + geom_histogram() + facet_wrap(~Beer) 17.2 Simple mixed models In the following plot each liking score is connected within consumer across the beer types. The facet (according to Age) is just to avoid overplotting. Here, there is a trend towards, if you rate one beer high, the other likings within that consumer will also be high. ggplot(data = beerliking, aes(Beer,Liking, group = Consumer.ID, color = Consumer.ID)) + geom_point() + geom_line() + theme(legend.position = &#39;none&#39;) + facet_wrap(~Age) Mixed models are used when there is repetitions in the response due to (here) the person tasting more than one product. library(lmerTest) library(lme4) mdl &lt;- lmer(data = beerliking, Liking ~ Beer + (1|Consumer.ID)) summary(mdl) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Liking ~ Beer + (1 | Consumer.ID) ## Data: beerliking ## ## REML criterion at convergence: 3606 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.52447 -0.73529 0.08622 0.74232 2.01132 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Consumer.ID (Intercept) 0.3242 0.5694 ## Residual 2.6615 1.6314 ## Number of obs: 920, groups: Consumer.ID, 155 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 4.32012 0.07067 154.58384 61.128 &lt; 2e-16 *** ## Beer1 0.58163 0.12075 763.97341 4.817 1.76e-06 *** ## Beer2 -0.05964 0.12040 762.72306 -0.495 0.62048 ## Beer3 -0.25523 0.12007 762.19954 -2.126 0.03385 * ## Beer4 0.34866 0.12007 762.19954 2.904 0.00379 ** ## Beer5 -0.25512 0.12013 764.89744 -2.124 0.03401 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Beer1 Beer2 Beer3 Beer4 ## Beer1 0.003 ## Beer2 0.001 -0.201 ## Beer3 -0.001 -0.200 -0.200 ## Beer4 -0.001 -0.200 -0.200 -0.199 ## Beer5 -0.003 -0.201 -0.200 -0.199 -0.199 Here we see that the residual uncertainty is \\(1.63\\) on the 1-7 likert scale, while the uncertainty between consumers is \\(0.57\\). This implies that the uncertainty between two ratings is higher when from two different consumers, compared to two ratings from the same consumer. We can use this model to evaluate the effect of the different beers. anova(mdl) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Beer 110.52 22.105 5 763.12 8.3054 1.175e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It seems as the average liking is different between beers. 17.2.1 Post hoc test The overall anova result implies that the 6 beers are NOT equal in terms of liking, but some may be similar. This can be investigated by computing pairwise contrasts using for instance the emmeans package or the multcomp package. Both are shown below library(emmeans) mdlemm &lt;- emmeans(mdl,&#39;Beer&#39;) contrast(mdlemm,&#39;tukey&#39;) ## contrast estimate SE df t.ratio p.value ## Brown Ale - NY Lager 0.641267 0.187 762 3.431 0.0083 ## Brown Ale - Porse Bock 0.836859 0.187 762 4.485 0.0001 ## Brown Ale - Ravnsborg Red 0.232963 0.187 762 1.249 0.8126 ## Brown Ale - River Beer 0.836742 0.187 763 4.483 0.0001 ## Brown Ale - Wheat IPA 0.941921 0.187 762 5.040 &lt;.0001 ## NY Lager - Porse Bock 0.195592 0.186 761 1.050 0.9006 ## NY Lager - Ravnsborg Red -0.408304 0.186 761 -2.192 0.2425 ## NY Lager - River Beer 0.195475 0.186 763 1.049 0.9010 ## NY Lager - Wheat IPA 0.300654 0.187 761 1.612 0.5909 ## Porse Bock - Ravnsborg Red -0.603896 0.186 761 -3.248 0.0153 ## Porse Bock - River Beer -0.000117 0.186 762 -0.001 1.0000 ## Porse Bock - Wheat IPA 0.105061 0.186 761 0.564 0.9933 ## Ravnsborg Red - River Beer 0.603779 0.186 762 3.247 0.0154 ## Ravnsborg Red - Wheat IPA 0.708957 0.186 761 3.807 0.0021 ## River Beer - Wheat IPA 0.105178 0.186 763 0.565 0.9932 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 6 estimates To calculate pairwise comparisons between e.g. samples and find letter-based representation you need a the package multcomp. library(multcomp) cld(glht(mdl, linfct = mcp(Beer = &quot;Tukey&quot;))) ## Brown Ale NY Lager Porse Bock Ravnsborg Red River Beer ## &quot;a&quot; &quot;bc&quot; &quot;c&quot; &quot;ab&quot; &quot;c&quot; ## Wheat IPA ## &quot;c&quot; The letters are based on the Post Hoc test Tukey, and they are calculated on the basis of the model mdl. Samples with the same letters are not significantly different. You can only use this function on factors. The resulst indicate Brown Ale and Ravnsborg Red is the most likable beers, with River Beer, Wheat IPA and Porse Bock as the least likable ones. These three at the bottom is also not significantly different. 17.3 Multivariable models In the example above, only the impact of the different beers is evaluated, however, the liking scores may also depend on background information such as gender, age, … as well as attitude towards beer and food. Further, is there any of the background variables that attenuate or make the differences between the beers stronger? These questions can be investigated by models including several explanatory variables as well as interaction terms. 17.3.1 Additive models This can be included in the models as a sequence of explanatory variables. Given a set of possible explanatory variables, there is two ways to include them in the model. Forward step-wise Selection and Backward Step-wise Elimination. For both, the principle is simple and intuitive. In the forward procedure each variable is added to the model, and the strongest one (in terms of the lowest p-value) is kept. In the backward procedure all variables are added to the model and the least important one (in terms of the largest p-value) is removed. Both procedures stop when the model is not going to improve by adding or eliminating explanatory variables, and the final model will only contain the significant variables. In the beer dataset we would like to know which of the explanatory variables that is most related to the liking. A large model with all explanatory variables is constructed mdl_be &lt;- lmer(data = beerliking, Liking ~ Beer + Gender + Age + Income + Householdsize + `Beer types/month` + `Interest in food` + neophilia + `Interest in beer` + `Beer knowledge` + `Ingredients/labels` + `Future interest in beer` + (1|Consumer.ID)) anova(mdl_be) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Beer 110.799 22.1597 5 762.75 8.3223 1.133e-07 *** ## Gender 0.093 0.0932 1 125.57 0.0350 0.851886 ## Age 20.946 3.4911 6 129.73 1.3111 0.256695 ## Income 47.126 7.8544 6 126.31 2.9498 0.009962 ** ## Householdsize 7.385 1.2309 6 127.84 0.4623 0.835121 ## `Beer types/month` 17.519 4.3797 4 125.10 1.6448 0.167136 ## `Interest in food` 0.015 0.0149 1 127.62 0.0056 0.940561 ## neophilia 2.730 2.7305 1 125.27 1.0255 0.313179 ## `Interest in beer` 1.092 1.0923 1 125.69 0.4102 0.523029 ## `Beer knowledge` 1.019 1.0186 1 127.44 0.3825 0.537350 ## `Ingredients/labels` 1.742 1.7424 1 130.36 0.6544 0.420024 ## `Future interest in beer` 8.480 8.4798 1 128.45 3.1847 0.076692 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From this, Interest in food is the least significant one, and is hence removed. A sequential removal of the non-significant variables at a \\(p &gt; 0.1\\) level leads to the following model: mdl_be_red &lt;- lmer(data = beerliking, Liking ~ Beer + Age + Income + `Beer types/month` + `Beer knowledge` + `Future interest in beer` + (1|Consumer.ID)) anova(mdl_be_red) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Beer 110.397 22.079 5 763.19 8.2936 1.206e-07 *** ## Age 31.657 5.276 6 140.10 1.9819 0.0721622 . ## Income 53.130 8.855 6 136.92 3.3261 0.0043400 ** ## `Beer types/month` 22.526 5.631 4 135.80 2.1153 0.0822411 . ## `Beer knowledge` 7.624 7.624 1 136.44 2.8638 0.0928735 . ## `Future interest in beer` 32.101 32.101 1 136.94 12.0579 0.0006906 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results can be interpreted from the estimates: summary(mdl_be_red) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## Liking ~ Beer + Age + Income + `Beer types/month` + `Beer knowledge` + ## `Future interest in beer` + (1 | Consumer.ID) ## Data: beerliking ## ## REML criterion at convergence: 3579.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6308 -0.7514 0.1098 0.7695 2.3071 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Consumer.ID (Intercept) 0.1282 0.358 ## Residual 2.6622 1.632 ## Number of obs: 920, groups: Consumer.ID, 155 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.44033 0.35039 140.52292 9.818 &lt; 2e-16 *** ## Beer1 0.58190 0.12075 764.14349 4.819 1.74e-06 *** ## Beer2 -0.06093 0.12041 762.70817 -0.506 0.613000 ## Beer3 -0.25622 0.12008 762.14804 -2.134 0.033190 * ## Beer4 0.34768 0.12008 762.14804 2.895 0.003897 ** ## Beer5 -0.25074 0.12016 765.10670 -2.087 0.037242 * ## Age1 -0.48478 0.16837 139.73875 -2.879 0.004614 ** ## Age2 0.20761 0.18210 143.88934 1.140 0.256129 ## Age3 0.19424 0.19982 138.26638 0.972 0.332724 ## Age4 0.19042 0.16837 142.38351 1.131 0.259972 ## Age5 0.08476 0.16478 141.12124 0.514 0.607780 ## Age6 -0.06256 0.20524 137.42692 -0.305 0.760960 ## Income1 0.67731 0.16773 135.36220 4.038 8.98e-05 *** ## Income2 0.06662 0.14326 138.02740 0.465 0.642649 ## Income3 -0.27765 0.18078 138.05336 -1.536 0.126853 ## Income4 -0.28362 0.19231 137.91719 -1.475 0.142539 ## Income5 -0.09937 0.17441 136.93000 -0.570 0.569795 ## Income6 -0.23446 0.17040 135.40859 -1.376 0.171111 ## `Beer types/month`1 -0.30772 0.18388 135.92255 -1.673 0.096541 . ## `Beer types/month`2 -0.44067 0.16874 135.47736 -2.611 0.010032 * ## `Beer types/month`3 -0.03544 0.19971 135.69780 -0.177 0.859396 ## `Beer types/month`4 0.11330 0.39541 135.36362 0.287 0.774902 ## `Beer knowledge` 0.08520 0.05034 136.44151 1.692 0.092873 . ## `Future interest in beer` 0.16456 0.04739 136.93585 3.472 0.000691 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Some notes: Liking is higher in the lowest Income group, Liking is lower in the lowest age group, and liking is higher with higher Future interest in beer. 17.3.2 Effect modification and Interactions It could be nice to calculate if the liking of specific sample are affected by the degree of future interest in beer. This can be visualized as scatterplots and modelled using interactions. ggplot(data = beerliking, aes(`Future interest in beer`, Liking, color = Beer)) + geom_point() + stat_smooth(se = F, method = lm) mdl_interaction &lt;- lmer(data = beerliking, Liking ~ Beer*`Future interest in beer` + (1|Consumer.ID)) anova(mdl_interaction) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Beer 14.431 2.886 5 758.29 1.0898 0.3645 ## `Future interest in beer` 65.599 65.599 1 153.35 24.7694 1.718e-06 ## Beer:`Future interest in beer` 23.230 4.646 5 758.06 1.7543 0.1200 ## ## Beer ## `Future interest in beer` *** ## Beer:`Future interest in beer` ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Although the slopes appear a bit different between the beers this is not significant (\\(p = 0.12\\)). Be aware that it appears as the main effect of beer is not significant. However, this value should not be interpreted, as there are interaction terms in the model including beer. Remember you choose your own “name” for the model “+” between variables defines (additive) main effects “:” between variables defines an interaction “*” between variables defines a parameterization with both interaction and main effect terms For model selection here, you start by removing the interaction with the hight p-value and then recalculate the model. You cannot remove a single variable if an interaction including it is significant. "],["projective-mapping.html", "Chapter 18 Projective mapping 18.1 Example from mapping of XX 18.2 A Collated version of the data 18.3 PCA on Collated data 18.4 ", " Chapter 18 Projective mapping [An image of a PM] 18.1 Example from mapping of XX library(data4consumerscience) data(&quot;tempetotemperature&quot;) tempetotemperature ## Productname Product Sousvidetemperature_C Sousvidetime_days id Assessor ## 1 45C-2D Sample 1 45 2 903 1 ## 2 45C-3D Sample 2 45 3 714 1 ## 3 45C-4D Sample 3 45 4 31 1 ## 4 50C-2D Sample 4 50 2 487 1 ## 5 50C-3D Sample 5 50 3 133 1 ## 6 50C-4D Sample 6 50 4 827 1 ## 7 55C-2D Sample 7 55 2 538 1 ## 8 55C-3D Sample 8 55 3 215 1 ## 9 55C-4D Sample 9 55 4 341 1 ## 10 45C-2D Sample 1 45 2 903 2 ## 11 45C-3D Sample 2 45 3 714 2 ## 12 45C-4D Sample 3 45 4 31 2 ## 13 50C-2D Sample 4 50 2 487 2 ## 14 50C-3D Sample 5 50 3 133 2 ## 15 50C-4D Sample 6 50 4 827 2 ## 16 55C-2D Sample 7 55 2 538 2 ## 17 55C-3D Sample 8 55 3 215 2 ## 18 55C-4D Sample 9 55 4 341 2 ## 19 45C-2D Sample 1 45 2 903 3 ## 20 45C-3D Sample 2 45 3 714 3 ## 21 45C-4D Sample 3 45 4 31 3 ## 22 50C-2D Sample 4 50 2 487 3 ## 23 50C-3D Sample 5 50 3 133 3 ## 24 50C-4D Sample 6 50 4 827 3 ## 25 55C-2D Sample 7 55 2 538 3 ## 26 55C-3D Sample 8 55 3 215 3 ## 27 55C-4D Sample 9 55 4 341 3 ## 28 45C-2D Sample 1 45 2 903 4 ## 29 45C-3D Sample 2 45 3 714 4 ## 30 45C-4D Sample 3 45 4 31 4 ## 31 50C-2D Sample 4 50 2 487 4 ## 32 50C-3D Sample 5 50 3 133 4 ## 33 50C-4D Sample 6 50 4 827 4 ## 34 55C-2D Sample 7 55 2 538 4 ## 35 55C-3D Sample 8 55 3 215 4 ## 36 55C-4D Sample 9 55 4 341 4 ## 37 45C-2D Sample 1 45 2 903 5 ## 38 45C-3D Sample 2 45 3 714 5 ## 39 45C-4D Sample 3 45 4 31 5 ## 40 50C-2D Sample 4 50 2 487 5 ## 41 50C-3D Sample 5 50 3 133 5 ## 42 50C-4D Sample 6 50 4 827 5 ## 43 55C-2D Sample 7 55 2 538 5 ## 44 55C-3D Sample 8 55 3 215 5 ## 45 55C-4D Sample 9 55 4 341 5 ## 46 45C-2D Sample 1 45 2 903 6 ## 47 45C-3D Sample 2 45 3 714 6 ## 48 45C-4D Sample 3 45 4 31 6 ## 49 50C-2D Sample 4 50 2 487 6 ## 50 50C-3D Sample 5 50 3 133 6 ## 51 50C-4D Sample 6 50 4 827 6 ## 52 55C-2D Sample 7 55 2 538 6 ## 53 55C-3D Sample 8 55 3 215 6 ## 54 55C-4D Sample 9 55 4 341 6 ## 55 45C-2D Sample 1 45 2 903 7 ## 56 45C-3D Sample 2 45 3 714 7 ## 57 45C-4D Sample 3 45 4 31 7 ## 58 50C-2D Sample 4 50 2 487 7 ## 59 50C-3D Sample 5 50 3 133 7 ## 60 50C-4D Sample 6 50 4 827 7 ## 61 55C-2D Sample 7 55 2 538 7 ## 62 55C-3D Sample 8 55 3 215 7 ## 63 55C-4D Sample 9 55 4 341 7 ## 64 45C-2D Sample 1 45 2 903 8 ## 65 45C-3D Sample 2 45 3 714 8 ## 66 45C-4D Sample 3 45 4 31 8 ## 67 50C-2D Sample 4 50 2 487 8 ## 68 50C-3D Sample 5 50 3 133 8 ## 69 50C-4D Sample 6 50 4 827 8 ## 70 55C-2D Sample 7 55 2 538 8 ## 71 55C-3D Sample 8 55 3 215 8 ## 72 55C-4D Sample 9 55 4 341 8 ## 73 45C-2D Sample 1 45 2 903 9 ## 74 45C-3D Sample 2 45 3 714 9 ## 75 45C-4D Sample 3 45 4 31 9 ## 76 50C-2D Sample 4 50 2 487 9 ## 77 50C-3D Sample 5 50 3 133 9 ## 78 50C-4D Sample 6 50 4 827 9 ## 79 55C-2D Sample 7 55 2 538 9 ## 80 55C-3D Sample 8 55 3 215 9 ## 81 55C-4D Sample 9 55 4 341 9 ## 82 45C-2D Sample 1 45 2 903 10 ## 83 45C-3D Sample 2 45 3 714 10 ## 84 45C-4D Sample 3 45 4 31 10 ## 85 50C-2D Sample 4 50 2 487 10 ## 86 50C-3D Sample 5 50 3 133 10 ## 87 50C-4D Sample 6 50 4 827 10 ## 88 55C-2D Sample 7 55 2 538 10 ## 89 55C-3D Sample 8 55 3 215 10 ## 90 55C-4D Sample 9 55 4 341 10 ## 91 45C-2D Sample 1 45 2 903 11 ## 92 45C-3D Sample 2 45 3 714 11 ## 93 45C-4D Sample 3 45 4 31 11 ## 94 50C-2D Sample 4 50 2 487 11 ## 95 50C-3D Sample 5 50 3 133 11 ## 96 50C-4D Sample 6 50 4 827 11 ## 97 55C-2D Sample 7 55 2 538 11 ## 98 55C-3D Sample 8 55 3 215 11 ## 99 55C-4D Sample 9 55 4 341 11 ## X1 Y1 X2 Y2 X3 Y3 X4 Y4 X5 Y5 X6 Y6 X7 Y7 X8 ## 1 32.2 34.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 2 15.1 23.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 3 6.6 34.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 4 30.1 16.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 5 42.8 20.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 6 38.6 8.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 7 3.8 17.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 8 44.8 12.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 9 45.4 4.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 10 0.0 0.0 41.8 20.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 11 0.0 0.0 42.5 36.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 12 0.0 0.0 23.3 25.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 13 0.0 0.0 32.6 23.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 14 0.0 0.0 26.5 30.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 15 0.0 0.0 12.2 27.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 16 0.0 0.0 36.9 14.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 17 0.0 0.0 11.9 14.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 18 0.0 0.0 1.5 18.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 19 0.0 0.0 0.0 0.0 1.4 1.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 20 0.0 0.0 0.0 0.0 10.3 12.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 21 0.0 0.0 0.0 0.0 26.6 14.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 22 0.0 0.0 0.0 0.0 16.6 4.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 23 0.0 0.0 0.0 0.0 34.5 16.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 24 0.0 0.0 0.0 0.0 43.8 31.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 25 0.0 0.0 0.0 0.0 27.6 23.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 26 0.0 0.0 0.0 0.0 49.9 24.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 27 0.0 0.0 0.0 0.0 58.8 38.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 28 0.0 0.0 0.0 0.0 0.0 0.0 14.3 36.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 29 0.0 0.0 0.0 0.0 0.0 0.0 4.2 36.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 30 0.0 0.0 0.0 0.0 0.0 0.0 9.2 28.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 31 0.0 0.0 0.0 0.0 0.0 0.0 48.3 32.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 32 0.0 0.0 0.0 0.0 0.0 0.0 29.7 21.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 33 0.0 0.0 0.0 0.0 0.0 0.0 46.1 11.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 34 0.0 0.0 0.0 0.0 0.0 0.0 29.1 29.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 35 0.0 0.0 0.0 0.0 0.0 0.0 48.7 17.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 36 0.0 0.0 0.0 0.0 0.0 0.0 53.4 9.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 37 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.4 10.6 0.0 0.0 0.0 0.0 0.0 ## 38 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.3 38.4 0.0 0.0 0.0 0.0 0.0 ## 39 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.6 30.9 0.0 0.0 0.0 0.0 0.0 ## 40 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 8.2 22.7 0.0 0.0 0.0 0.0 0.0 ## 41 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 42.3 22.9 0.0 0.0 0.0 0.0 0.0 ## 42 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 52.2 15.8 0.0 0.0 0.0 0.0 0.0 ## 43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 28.5 36.7 0.0 0.0 0.0 0.0 0.0 ## 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 43.3 35.9 0.0 0.0 0.0 0.0 0.0 ## 45 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 55.0 35.5 0.0 0.0 0.0 0.0 0.0 ## 46 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 45.8 31.8 0.0 0.0 0.0 ## 47 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 55.9 37.2 0.0 0.0 0.0 ## 48 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 37.9 26.2 0.0 0.0 0.0 ## 49 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 54.7 32.2 0.0 0.0 0.0 ## 50 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 32.9 16.1 0.0 0.0 0.0 ## 51 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 13.8 12.0 0.0 0.0 0.0 ## 52 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 22.7 23.4 0.0 0.0 0.0 ## 53 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 9.1 23.0 0.0 0.0 0.0 ## 54 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.7 8.9 0.0 0.0 0.0 ## 55 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.9 7.6 0.0 ## 56 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 38.5 0.0 ## 57 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 18.4 32.2 0.0 ## 58 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 22.4 21.1 0.0 ## 59 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 44.3 17.0 0.0 ## 60 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 42.2 31.8 0.0 ## 61 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 26.1 6.8 0.0 ## 62 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 43.7 7.4 0.0 ## 63 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 51.9 38.6 0.0 ## 64 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 56.3 ## 65 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 45.2 ## 66 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 45.6 ## 67 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 56.1 ## 68 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 32.7 ## 69 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 15.1 ## 70 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 28.0 ## 71 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.5 ## 72 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6.6 ## 73 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 74 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 75 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 76 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 77 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 78 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 79 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 80 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 81 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 82 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 83 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 84 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 85 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 86 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 87 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 88 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 89 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 90 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 91 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 92 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 93 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 94 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 95 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 96 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 97 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 98 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## 99 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ## Y8 X9 Y9 X10 Y10 X11 Y11 Miso Soft Bitter Sweet Sour Mild Caramel ## 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 1 1 0 ## 3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1 1 1 1 1 0 ## 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 1 1 0 0 ## 5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 1 0 0 1 1 ## 6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 1 0 ## 7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 1 0 ## 8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 0 ## 9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 0 ## 10 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 11 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 13 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 14 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 16 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 18 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0 0 0 1 0 0 ## 19 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 1 0 0 ## 20 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 21 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 22 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 23 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 24 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 1 ## 25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 26 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 1 ## 28 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 29 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 30 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 1 ## 31 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 32 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 33 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 1 ## 34 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 1 ## 35 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 1 ## 36 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 37 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 0 0 0 ## 38 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 39 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 1 0 1 0 0 ## 40 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 1 1 0 0 ## 41 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 42 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 1 0 0 ## 43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 45 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 46 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 1 0 0 ## 47 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 48 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 1 0 0 ## 49 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 50 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 1 0 0 0 0 0 ## 51 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 52 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 0 0 0 ## 53 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 54 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 55 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 56 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 57 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 58 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 59 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 60 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 0 0 0 ## 61 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 0 1 0 0 ## 62 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 63 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 64 30.7 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 65 1.3 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 66 11.9 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 67 15.3 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 68 10.1 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 69 13.8 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 70 26.6 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 71 24.7 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 1 0 0 1 ## 72 14.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 73 0.0 53.4 35.0 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 74 0.0 6.8 34.4 0.0 0.0 0.0 0.0 0 0 0 1 0 1 0 ## 75 0.0 12.0 21.6 0.0 0.0 0.0 0.0 0 0 0 1 0 0 0 ## 76 0.0 54.6 12.7 0.0 0.0 0.0 0.0 0 0 0 0 1 0 0 ## 77 0.0 38.9 13.7 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 78 0.0 9.7 7.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 ## 79 0.0 49.8 28.3 0.0 0.0 0.0 0.0 0 0 0 1 1 0 0 ## 80 0.0 30.0 27.5 0.0 0.0 0.0 0.0 0 0 1 1 1 0 0 ## 81 0.0 6.0 9.2 0.0 0.0 0.0 0.0 0 0 0 1 0 0 0 ## 82 0.0 0.0 0.0 5.5 7.6 0.0 0.0 0 0 0 0 0 0 0 ## 83 0.0 0.0 0.0 49.5 35.5 0.0 0.0 0 0 0 0 0 0 0 ## 84 0.0 0.0 0.0 31.0 12.9 0.0 0.0 0 0 0 1 0 0 0 ## 85 0.0 0.0 0.0 21.1 21.8 0.0 0.0 0 0 0 0 0 0 0 ## 86 0.0 0.0 0.0 53.8 18.9 0.0 0.0 0 0 0 0 0 0 0 ## 87 0.0 0.0 0.0 40.6 27.8 0.0 0.0 0 0 0 1 0 0 0 ## 88 0.0 0.0 0.0 8.9 33.2 0.0 0.0 0 0 0 0 0 0 0 ## 89 0.0 0.0 0.0 46.6 8.8 0.0 0.0 0 0 0 1 0 0 0 ## 90 0.0 0.0 0.0 56.6 8.2 0.0 0.0 0 0 0 0 0 0 0 ## 91 0.0 0.0 0.0 0.0 0.0 18.3 12.5 0 0 0 1 0 0 0 ## 92 0.0 0.0 0.0 0.0 0.0 4.6 26.2 0 0 1 0 0 0 0 ## 93 0.0 0.0 0.0 0.0 0.0 30.2 4.6 0 0 0 0 0 0 0 ## 94 0.0 0.0 0.0 0.0 0.0 16.8 22.0 0 0 0 0 0 1 0 ## 95 0.0 0.0 0.0 0.0 0.0 45.3 31.3 0 0 0 0 0 0 0 ## 96 0.0 0.0 0.0 0.0 0.0 27.2 24.3 0 0 0 0 0 1 0 ## 97 0.0 0.0 0.0 0.0 0.0 19.2 31.2 0 0 0 0 0 0 0 ## 98 0.0 0.0 0.0 0.0 0.0 29.0 34.2 0 0 0 0 0 0 0 ## 99 0.0 0.0 0.0 0.0 0.0 34.6 17.9 0 0 0 0 0 0 0 ## Umami Deep Balanced Nutty Hard Dry Roasted Strong Fish sauce Dark Chocolate ## 1 1 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 1 1 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 1 0 0 0 0 0 ## 5 0 0 0 0 0 0 1 0 0 0 0 ## 6 0 0 0 0 0 0 0 1 0 0 0 ## 7 0 0 0 0 1 1 0 0 0 0 0 ## 8 0 0 0 0 0 1 0 0 1 1 1 ## 9 0 0 0 0 0 1 1 1 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 0 0 ## 12 0 0 0 0 0 0 0 0 0 1 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 ## 15 0 0 0 0 0 0 0 0 0 1 0 ## 16 1 0 0 0 0 1 0 0 0 0 0 ## 17 0 0 0 0 0 1 0 0 0 0 0 ## 18 0 0 0 0 0 0 0 0 0 1 0 ## 19 0 0 0 0 0 0 0 0 0 0 0 ## 20 1 0 0 0 0 0 0 0 0 0 0 ## 21 1 0 0 0 0 0 0 0 0 1 0 ## 22 1 0 0 0 0 0 0 0 0 0 0 ## 23 1 0 0 0 0 0 0 0 0 1 0 ## 24 1 0 0 0 0 1 0 0 1 1 0 ## 25 1 0 0 0 0 0 0 0 0 0 0 ## 26 1 0 0 0 0 0 0 0 0 1 0 ## 27 1 0 0 0 0 1 0 0 0 1 0 ## 28 0 0 0 0 0 0 0 0 0 0 0 ## 29 0 0 0 0 0 0 0 0 0 0 0 ## 30 0 0 0 0 0 0 0 0 0 0 0 ## 31 0 0 0 0 0 0 0 0 0 0 0 ## 32 0 0 1 0 0 0 0 0 0 0 0 ## 33 0 0 0 0 0 0 0 0 0 1 0 ## 34 0 0 1 1 0 0 0 0 0 0 0 ## 35 0 0 0 0 0 0 0 0 0 0 0 ## 36 0 0 0 0 0 0 0 0 0 1 0 ## 37 0 0 0 0 0 0 0 0 0 0 0 ## 38 0 0 0 0 0 0 0 1 0 0 0 ## 39 0 0 0 0 0 0 0 0 0 0 0 ## 40 0 0 0 0 0 0 0 0 0 0 0 ## 41 0 0 0 0 0 0 0 0 0 0 0 ## 42 0 0 0 0 0 0 0 0 0 1 0 ## 43 0 0 0 0 0 1 0 0 0 0 0 ## 44 0 0 0 0 0 1 0 1 0 1 0 ## 45 0 0 0 0 0 1 0 1 0 1 0 ## 46 0 0 0 0 0 0 0 0 0 0 0 ## 47 0 0 0 0 0 0 0 0 0 0 0 ## 48 0 0 0 0 0 0 0 0 0 0 0 ## 49 0 0 0 0 0 0 0 0 0 0 0 ## 50 0 0 0 0 0 0 0 0 0 0 0 ## 51 0 0 0 0 0 0 0 0 0 1 0 ## 52 0 0 0 0 0 0 0 0 0 0 0 ## 53 0 0 0 0 0 0 0 0 0 1 0 ## 54 0 0 0 0 0 0 0 0 0 1 0 ## 55 0 0 1 0 0 0 0 0 0 0 0 ## 56 0 0 1 0 0 0 0 0 0 0 0 ## 57 0 0 1 0 0 0 0 0 0 0 0 ## 58 0 0 0 0 0 0 0 0 0 0 0 ## 59 0 0 1 0 0 0 0 0 0 0 0 ## 60 0 0 0 0 0 0 0 0 0 0 0 ## 61 0 0 0 0 0 0 0 0 0 0 0 ## 62 0 0 0 0 0 0 1 0 0 1 0 ## 63 0 0 0 0 0 0 0 0 0 0 1 ## 64 0 0 0 0 0 0 0 0 0 0 0 ## 65 0 0 0 0 0 0 0 0 0 0 0 ## 66 0 0 0 0 0 0 0 0 0 0 0 ## 67 0 0 0 0 0 0 0 0 0 0 0 ## 68 1 0 0 0 0 0 0 0 0 0 0 ## 69 0 0 0 0 0 0 0 0 0 0 0 ## 70 0 0 0 0 0 0 0 0 0 0 0 ## 71 0 0 0 0 0 0 0 0 0 0 0 ## 72 0 0 0 0 0 0 0 0 0 0 0 ## 73 0 0 0 0 0 0 0 0 0 0 0 ## 74 0 0 0 0 0 0 0 0 0 0 0 ## 75 0 0 0 0 0 0 0 0 0 0 0 ## 76 0 0 0 0 0 0 0 0 0 0 0 ## 77 0 0 0 0 0 0 0 0 0 0 0 ## 78 0 0 0 0 0 0 0 0 0 0 0 ## 79 0 0 0 0 0 1 0 0 0 0 0 ## 80 0 0 0 0 0 0 0 0 0 0 0 ## 81 0 0 0 0 0 0 0 0 0 0 0 ## 82 0 0 0 0 0 0 0 0 0 0 0 ## 83 0 1 0 0 0 0 0 0 0 0 0 ## 84 0 0 0 0 0 0 0 0 0 0 0 ## 85 0 0 0 0 0 0 0 0 0 0 0 ## 86 0 0 0 0 0 0 0 0 0 0 0 ## 87 0 0 0 0 0 0 0 0 0 0 0 ## 88 0 0 0 0 0 0 0 0 0 0 0 ## 89 0 0 0 0 0 0 0 0 0 0 0 ## 90 0 1 0 0 0 0 0 0 0 0 0 ## 91 0 0 0 0 0 0 0 0 0 0 0 ## 92 0 0 0 0 0 0 0 0 0 0 0 ## 93 0 0 0 0 0 0 0 0 0 0 0 ## 94 0 0 0 0 0 0 0 0 0 0 0 ## 95 0 0 0 0 0 0 0 0 0 0 0 ## 96 0 0 0 0 0 0 0 0 0 0 0 ## 97 0 0 0 0 0 0 0 0 0 0 0 ## 98 0 0 0 0 0 0 0 0 0 0 0 ## 99 0 0 0 0 1 0 0 0 0 0 0 ## Moisty Brown Dried fruits Beany Smooth Dense Alcoholic Firm Grainy Soy sauce ## 1 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 ## 10 1 0 0 0 0 0 0 0 0 0 ## 11 1 0 0 0 0 0 0 0 0 0 ## 12 1 0 0 0 0 0 0 0 0 0 ## 13 1 0 0 0 0 0 0 0 0 0 ## 14 1 0 0 0 0 0 0 0 0 0 ## 15 1 0 0 0 0 0 0 0 0 0 ## 16 0 0 0 0 0 0 0 0 0 0 ## 17 0 1 0 0 0 0 0 0 0 0 ## 18 1 0 0 0 0 0 0 0 0 0 ## 19 0 0 1 0 0 0 0 0 0 0 ## 20 0 0 1 0 0 0 0 0 0 0 ## 21 0 0 1 0 0 0 1 0 0 0 ## 22 0 0 1 1 1 1 0 0 0 0 ## 23 0 0 0 1 0 0 0 0 0 0 ## 24 0 0 0 1 0 1 0 0 0 1 ## 25 0 0 0 1 0 0 0 1 1 0 ## 26 0 0 0 0 0 0 0 0 0 1 ## 27 0 0 0 1 0 0 0 0 1 1 ## 28 0 0 0 0 0 0 0 0 0 0 ## 29 0 1 0 0 0 1 0 0 0 0 ## 30 0 0 0 0 1 1 0 0 0 0 ## 31 0 0 0 0 1 0 0 1 0 0 ## 32 0 1 0 0 1 0 0 1 0 0 ## 33 0 0 0 0 0 0 1 0 0 0 ## 34 0 1 0 0 0 0 0 0 0 0 ## 35 0 1 0 1 0 0 0 1 1 0 ## 36 0 0 0 0 0 0 0 1 1 0 ## 37 0 0 0 0 0 0 0 0 0 0 ## 38 0 0 0 0 0 0 0 0 0 0 ## 39 0 0 0 0 0 0 0 0 0 0 ## 40 0 0 0 0 0 0 0 0 0 0 ## 41 0 0 0 0 0 0 0 0 0 0 ## 42 0 0 0 0 0 0 0 0 0 0 ## 43 0 0 0 0 0 0 0 0 0 0 ## 44 0 0 0 0 0 0 0 0 0 0 ## 45 0 0 0 0 0 0 0 0 0 0 ## 46 0 1 0 0 0 0 0 0 0 0 ## 47 0 1 0 0 0 0 0 0 0 0 ## 48 0 1 0 0 0 0 0 0 0 0 ## 49 0 1 0 0 0 0 0 0 0 0 ## 50 0 1 0 0 0 0 0 0 0 0 ## 51 0 0 0 1 0 0 0 0 0 0 ## 52 0 1 0 0 0 0 0 0 0 0 ## 53 0 0 0 0 0 0 0 0 1 0 ## 54 0 0 0 1 0 0 0 0 1 0 ## 55 1 0 0 0 0 0 1 0 0 0 ## 56 1 0 0 0 0 0 1 0 0 0 ## 57 0 0 0 0 0 0 0 0 0 0 ## 58 0 0 0 0 0 0 0 0 0 0 ## 59 0 1 0 0 0 0 1 0 0 0 ## 60 0 0 0 0 0 0 0 0 0 0 ## 61 0 0 0 0 0 0 0 0 1 0 ## 62 0 0 0 0 0 0 0 0 1 0 ## 63 0 0 0 0 0 0 0 0 0 0 ## 64 0 0 0 0 0 0 0 0 0 0 ## 65 0 0 0 0 0 0 0 0 0 0 ## 66 0 0 0 0 0 0 0 0 0 0 ## 67 0 0 0 0 0 0 0 0 0 0 ## 68 0 0 0 0 0 0 0 0 0 0 ## 69 0 0 0 0 0 0 0 0 0 1 ## 70 0 0 0 0 0 1 0 0 0 0 ## 71 0 0 0 0 0 0 0 0 0 1 ## 72 0 0 0 0 0 0 0 0 0 1 ## 73 0 0 0 0 0 0 1 0 0 0 ## 74 0 0 0 0 0 0 0 0 0 0 ## 75 0 0 0 0 0 0 0 0 0 0 ## 76 0 0 0 0 0 0 0 0 0 0 ## 77 0 0 0 0 0 0 0 0 0 0 ## 78 0 0 0 0 0 0 0 0 0 1 ## 79 0 0 0 0 0 0 1 0 0 0 ## 80 0 0 0 0 0 0 0 0 1 0 ## 81 0 0 1 0 0 0 0 0 0 1 ## 82 0 0 0 0 0 0 0 0 0 0 ## 83 0 0 0 0 0 0 0 0 0 0 ## 84 0 0 0 0 0 0 0 0 0 0 ## 85 0 0 0 0 0 0 0 0 0 0 ## 86 0 0 0 0 0 0 0 0 0 0 ## 87 0 0 0 0 0 0 0 0 0 0 ## 88 0 0 0 0 0 0 0 0 0 0 ## 89 0 0 0 0 0 0 0 0 0 0 ## 90 0 0 0 0 0 0 0 0 0 0 ## 91 0 0 0 0 0 0 0 0 0 0 ## 92 0 0 0 0 0 0 0 0 0 0 ## 93 0 0 0 0 0 0 0 0 0 0 ## 94 0 0 0 0 0 0 0 0 0 0 ## 95 0 0 0 0 0 0 0 0 1 0 ## 96 0 0 0 0 0 0 0 0 0 0 ## 97 0 0 0 0 0 0 0 0 0 0 ## 98 0 0 0 0 0 0 0 0 1 0 ## 99 0 0 0 0 0 0 0 0 1 0 ## Chewy Yeasty Fruity Wine Gummy Malty Liquorise Bread Salty Meaty Creamy ## 1 0 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 0 0 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 ## 11 0 0 0 0 0 0 0 0 0 0 0 ## 12 0 0 0 0 0 0 0 0 0 0 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 0 0 ## 15 0 0 0 0 0 0 0 0 0 0 0 ## 16 0 0 0 0 0 0 0 0 0 0 0 ## 17 0 0 0 0 0 0 0 0 0 0 0 ## 18 0 0 0 0 0 0 0 0 0 0 0 ## 19 0 0 0 0 0 0 0 0 0 0 0 ## 20 0 0 0 0 0 0 0 0 0 0 0 ## 21 0 0 0 0 0 0 0 0 0 0 0 ## 22 0 0 0 0 0 0 0 0 0 0 0 ## 23 0 0 0 0 0 0 0 0 0 0 0 ## 24 0 0 0 0 0 0 0 0 0 0 0 ## 25 0 0 0 0 0 0 0 0 0 0 0 ## 26 1 0 0 0 0 0 0 0 0 0 0 ## 27 0 0 0 0 0 0 0 0 0 0 0 ## 28 0 0 1 0 0 0 0 0 0 0 0 ## 29 1 1 1 0 0 0 0 0 0 0 0 ## 30 1 0 1 0 0 0 0 0 0 0 0 ## 31 0 0 0 0 0 0 0 0 0 0 0 ## 32 0 0 0 0 1 0 0 0 0 0 0 ## 33 0 0 0 0 0 0 0 0 0 0 0 ## 34 0 0 0 1 0 0 0 0 0 0 0 ## 35 0 0 1 0 0 0 0 0 0 0 0 ## 36 0 0 0 0 0 0 0 0 0 0 0 ## 37 0 0 1 0 0 0 0 0 0 0 0 ## 38 0 0 0 0 0 0 0 0 0 0 0 ## 39 0 0 0 0 0 0 0 0 0 0 0 ## 40 0 0 0 0 0 0 0 0 0 0 0 ## 41 0 0 0 0 0 1 0 0 0 0 0 ## 42 0 0 0 0 0 1 0 0 0 0 0 ## 43 0 0 0 0 0 1 0 0 0 0 0 ## 44 0 0 0 0 0 1 0 0 0 0 0 ## 45 0 0 0 0 0 1 0 0 0 0 0 ## 46 0 0 0 0 0 0 0 0 0 0 0 ## 47 0 0 1 0 0 0 0 0 0 0 0 ## 48 0 0 0 0 0 0 0 0 0 0 1 ## 49 0 0 1 0 0 0 0 0 0 0 0 ## 50 0 1 0 0 0 0 0 0 0 0 0 ## 51 0 0 0 0 0 0 0 0 0 0 0 ## 52 0 1 0 0 0 0 0 0 0 0 0 ## 53 0 1 0 0 0 0 0 0 0 0 0 ## 54 0 0 0 0 0 1 1 0 0 0 0 ## 55 0 0 0 0 0 0 0 0 0 0 0 ## 56 0 0 0 0 0 0 0 0 0 0 0 ## 57 0 0 0 0 0 0 0 0 0 0 0 ## 58 0 0 0 0 0 0 0 0 0 0 0 ## 59 0 0 0 0 0 0 0 0 0 0 0 ## 60 0 0 0 0 0 1 0 0 0 0 0 ## 61 0 0 0 0 0 0 0 0 0 0 0 ## 62 0 0 0 0 0 0 0 0 0 0 0 ## 63 0 0 0 0 0 0 1 1 0 0 0 ## 64 0 0 0 0 1 0 0 1 0 0 0 ## 65 0 0 0 0 0 0 0 0 0 0 0 ## 66 0 0 0 0 0 0 0 0 0 0 0 ## 67 0 0 0 0 0 0 0 1 0 0 0 ## 68 0 0 0 0 0 0 0 0 0 0 0 ## 69 0 0 0 0 0 0 0 0 1 0 0 ## 70 0 0 1 0 0 0 0 0 0 0 0 ## 71 0 0 0 0 0 0 0 0 0 0 0 ## 72 0 0 0 0 0 0 0 0 1 0 0 ## 73 0 0 0 0 0 0 0 0 0 0 0 ## 74 0 0 0 0 0 0 0 0 0 0 0 ## 75 0 0 0 0 0 0 0 0 1 1 0 ## 76 0 0 0 0 0 0 0 0 0 0 0 ## 77 0 0 0 0 0 0 0 0 0 0 0 ## 78 0 0 0 0 0 0 0 0 0 1 0 ## 79 0 0 0 0 0 0 0 0 0 0 0 ## 80 0 0 0 0 0 0 0 0 0 0 0 ## 81 0 0 0 0 0 0 0 0 0 0 0 ## 82 0 0 0 0 0 0 0 0 0 0 0 ## 83 0 1 0 0 0 0 0 0 0 0 0 ## 84 0 0 0 0 0 0 0 0 0 0 0 ## 85 0 1 0 0 0 0 0 0 0 0 0 ## 86 0 0 0 1 0 0 0 0 0 0 0 ## 87 0 0 0 0 0 0 0 0 0 0 0 ## 88 0 0 0 0 0 0 0 0 0 0 0 ## 89 0 0 0 0 0 0 0 0 0 0 0 ## 90 0 0 0 0 0 0 0 0 0 1 0 ## 91 0 0 1 0 0 0 0 0 0 0 0 ## 92 0 0 0 0 0 0 0 0 0 0 1 ## 93 0 0 0 0 0 0 0 0 0 0 1 ## 94 0 0 0 0 0 0 0 0 0 0 0 ## 95 0 0 0 0 0 0 0 0 0 0 0 ## 96 0 0 0 0 0 0 0 0 0 0 1 ## 97 0 0 0 0 0 0 0 0 0 0 0 ## 98 0 0 0 0 0 1 0 0 0 0 0 ## 99 0 0 0 0 0 1 0 0 0 0 0 ## Stout ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 0 ## 10 0 ## 11 0 ## 12 0 ## 13 0 ## 14 0 ## 15 0 ## 16 0 ## 17 0 ## 18 0 ## 19 0 ## 20 0 ## 21 0 ## 22 0 ## 23 0 ## 24 0 ## 25 0 ## 26 0 ## 27 0 ## 28 0 ## 29 0 ## 30 0 ## 31 0 ## 32 0 ## 33 0 ## 34 0 ## 35 0 ## 36 0 ## 37 0 ## 38 0 ## 39 0 ## 40 0 ## 41 0 ## 42 0 ## 43 0 ## 44 0 ## 45 0 ## 46 0 ## 47 0 ## 48 0 ## 49 0 ## 50 0 ## 51 1 ## 52 0 ## 53 0 ## 54 1 ## 55 0 ## 56 0 ## 57 0 ## 58 0 ## 59 0 ## 60 0 ## 61 0 ## 62 0 ## 63 0 ## 64 0 ## 65 0 ## 66 0 ## 67 0 ## 68 0 ## 69 0 ## 70 0 ## 71 0 ## 72 0 ## 73 0 ## 74 0 ## 75 0 ## 76 0 ## 77 0 ## 78 0 ## 79 0 ## 80 0 ## 81 0 ## 82 0 ## 83 0 ## 84 0 ## 85 0 ## 86 0 ## 87 0 ## 88 0 ## 89 1 ## 90 0 ## 91 0 ## 92 0 ## 93 0 ## 94 0 ## 95 0 ## 96 0 ## 97 0 ## 98 0 ## 99 0 table(tempetotemperature$Productname) ## ## 45C-2D 45C-3D 45C-4D 50C-2D 50C-3D 50C-4D 55C-2D 55C-3D 55C-4D ## 11 11 11 11 11 11 11 11 11 table(tempetotemperature$Assessor) ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 9 9 9 9 9 9 9 9 9 9 9 This dataset consists of 9 products evaluateed by 11 judges. The responses is the 2D coordinates of the procetive mapping, for each judge individually (X1, Y1, X2,…, Y11), and CATA data on 30 attribues (Miso, Soft, Bitter,…, Stout). 18.2 A Collated version of the data 18.3 PCA on Collated data downweeeigting of attributes. On everything with normal scaling. Include judge-loadings in the output plots. 18.4 "],["tfih-exercises.html", "Chapter 19 TFIH Exercises 19.1 Exercise 1: Descriptive statistics and plots 19.2 Exercise 2: Consumer background and PCA 19.3 Exercise 3: PCA on CATA counts 19.4 Exercise 4: Cochran’s Q test on CATA binary data 19.5 Exercise 5: Hedonic ratings and consumer characteristics 19.6 Exercise 6: PCA on CATA counts and hedonic ratings 19.7 Exercise 7: Mixed modelling on hedonic ratings", " Chapter 19 TFIH Exercises Below you will find exercises relevant for the data analysis for the project work in the Thematic course in Food Innovation and Health. Exercises are meant as a guide on what you can do with your own data obtained through the practical work on the projects. Before you start to work on the exercises, you need to familiarize yourself with the knowledge in the introduction chapters of this book as well as the chapters written specifically of for this course. 19.1 Exercise 1: Descriptive statistics and plots From this exercise you should be able to describe the products in terms of common, rare and differential attributes, as well as the hedonic ratings. 19.1.1 CATA counts Create data set with CATA counts per product and make a table thereof Plot the CATA counts such that product differences are emphasized library(data4consumerscience) data(&quot;beercata&quot;) 19.1.2 Hedonics Plot hedonic ratings (beerliking$Liking) per product in histograms to see the distribution Calculate descriptive statistics for hedonic rating for each product data(&quot;beerliking&quot;) 19.2 Exercise 2: Consumer background and PCA From this exercise you should be able to describe who your consumers are. 19.2.1 Demographics Age, Gender, Income and Household size is classical demographic characteristics. As the evaluation of the products is done using a specific group of people, the generalizability of the results naturally condition of the characteristics of this population. Construct the distributions of the demographic characteristics. You can use the table() function, or maybe do it all at once using the tableone package. For inspiration, modify the code below to serve your needs. library(data4consumerscience) library(tableone) data(&quot;beerdemo&quot;) CreateTableOne(vars = c(&#39;Age&#39;,&#39;Gender&#39;),data = beerdemo) 19.2.2 PCA on Interests Calculate a PCA model including the Variables 7 ( Interest in food ) to 12 ( Future interest in beer ). Remember to standardize/scale the variables mdlPCA &lt;- prcomp(beerdemo[,7:12],scale. = T) Plot the scores and loadings in a biplot and look for groupings of the consumers in the scores. Group and color according to the background information not used in the model (Gender, Age,..) library(ggbiplot) ggbiplot(mdlPCA, groups = beerdemo$Gender, ellipse = T) Describe what you find. 19.3 Exercise 3: PCA on CATA counts From this exercise you should be able to describe your samples (beers) from the CATA counts, and which attributes that occur together, as well as which attribute patterns that discriminate products. Calculate a PCA model including all Variables and all Objects. PCAmdl &lt;- prcomp(beercata[,3:29], scale. = T) Plot the scores and describe the groupings of the samples. Plot the loadings and describe the correlations between the variables. Both of these plots are present in a biplot, but you may want to play around with the settings (such as varname.size, varname.adjust, …) to get all information out. ggbiplot(PCAmdl, groups = beercata$Beer, ellipse = T) Use this biplot to find out which samples are described by which CATA attributes. as well as similarity of products. 19.4 Exercise 4: Cochran’s Q test on CATA binary data In Exercise 1 the attributes characteristic for certain products, were summarized using descriptive stats and plots. Now we want to add statistical inference onto these observations. To get started, choose a few attributes. For each attribute 1) Perform Cochran’s Q test for overall difference, and if interesting enough 2) perform a Post hoc test for product segmentation. To get all attributes analyzed collectively, use the tidyverse code in the chapter For all attributes in one run (nice to know). Try this procedure out on a different dataset, e.g. the tempeto data. data(&quot;tempetofermentation&quot;) library(RVAideMemoire) tempetofermentation$Product2 &lt;- factor(tempetofermentation$Product) # needs to be a factor for getting labels on results. m &lt;- cochran.qtest(Sour ~ Product2 | Assessor, data = tempetofermentation) 19.5 Exercise 5: Hedonic ratings and consumer characteristics From this exercise you should be able to describe the liking of the beer samples and glimpse into consumer characteristics related to liking 19.5.1 PCA on joint data Calculate a PCA model including all Variables and all Objects. include_these &lt;- complete.cases(beerliking) PCAliking &lt;- prcomp(beerliking[include_these,-1], scale. = T) Plot a biplot or loading plot, and use the loadings and describe the correlations between the variables (liking of beers in this case). ggbiplot(PCAliking) Plot the scores and describe the groupings of the samples by colouring the score plot according to the consumer background variables. Note that the 160 rows in both datasets match each-other, so we can glue the demo information directly onto the liking model. If that was not the case, matching using left_join() or inner_join() would be necessary before analysis. ggbiplot(PCAliking,groups = beerdemo$Age[include_these], ellipse = T) Any trends? For instance, how is liking related to the individual consumer diversity of beer ( Beer types/month)? 19.5.2 All demographics … Some code to get all 7-scale demo information plots. You may want to export and view in a pdf viewer for zooming etc. gall &lt;- cbind(PCAliking$x[,1:2], beerdemo[include_these,]) %&gt;% gather(var,val,`Interest in food`:App_Vinous) %&gt;% ggplot(data = ., aes(PC1,PC2, color = factor(val))) + geom_point() + stat_ellipse() + facet_wrap(~var) ggsave(filename = &#39;anicebigfigure.pdf&#39;,gall, height = 20, width = 20) 19.6 Exercise 6: PCA on CATA counts and hedonic ratings From this exercise you should be able to conclude what attributes that overall drives the liking of products (beers). For each beer, the collated CATA counts and the averaged liking constitutes the multivariate response matrix. likingsum &lt;- beerliking %&gt;% group_by(Beer) %&gt;% dplyr::summarise(lik = mean(Liking, na.rm = T)) catasum &lt;- beercata %&gt;% gather(attrib,val,S_Flowers:S_Vinous ) %&gt;% group_by(Beer,attrib) %&gt;% dplyr::summarise(mn = mean(val, na.rm = T)) %&gt;% spread(attrib,mn) Join the two. cata_lik &lt;- catasum %&gt;% left_join(likingsum, by = &#39;Beer&#39;) Do a PCA PCAcata_lik &lt;- prcomp(cata_lik[,-1],scale. = T) ggbiplot(PCAcata_lik, labels = cata_lik$Beer) 19.7 Exercise 7: Mixed modelling on hedonic ratings In this exercise we want to model Liking as a response. We want to use the beer type as the main predictor, but take into account that the responses is repeated within consumers. The same judges have evaluated all beers. This is a mixed model Start with a plot ggplot(data = beerliking, aes(Beer, Liking)) + geom_boxplot() library(lme4) library(lmerTest) lik_mdl &lt;- lmer(data = beerliking, Liking ~ Beer + (1|Consumer.ID)) Evaluate this model by anova() and summary(), and interpret the result. How much within individual consumer variation is there compared to between consumer? You should get that the beers are not equally likable. But are there maybe some which are more similar? Perform a all-pairs post hoc test and try to group beers based on liking. library(multcomp) summary(glht(lik_mdl, linfct = mcp(Beer = &quot;Tukey&quot;)), test = adjusted(&quot;fdr&quot;)) Try to add consumer demographics and interest to the model to evaluate general consumer patterns as well as interactions with beer type. Try to answer questions such as: Are there any significant product differences for the liking? If so, what does the post hoc test tell us? How does this fit with what you have done in the PCA exercises. Is the liking in general affected by the age, gender, household size or beer knowledge? What is the effect? Try to think of a plot that can show the significant differences. Do men and women score the samples significantly different in liking? Calculate the sample/gender differences in averages. "],["chapters-to-appear.html", "Chapter 20 CHAPTERS to APPEAR", " Chapter 20 CHAPTERS to APPEAR "],["latent-factor-models.html", "Chapter 21 Latent Factor Models", " Chapter 21 Latent Factor Models There are many…. "],["lpls.html", "Chapter 22 LPLS", " Chapter 22 LPLS L-PLS - include reference. "],["confirmatory-factor-analysis-using-lavaan.html", "Chapter 23 Confirmatory Factor Analysis using lavaan 23.1 Example - Food Neophobia", " Chapter 23 Confirmatory Factor Analysis using lavaan 23.1 Example - Food Neophobia "],["structured-equation-modelling.html", "Chapter 24 Structured Equation Modelling 24.1 Example - Theory of Planned Behaviour", " Chapter 24 Structured Equation Modelling 24.1 Example - Theory of Planned Behaviour "],["plsda-on-cata-and-liking.html", "Chapter 25 PLSDA on CATA and liking", " Chapter 25 PLSDA on CATA and liking [This needs more love. Skal vente til 2023! hvordan får vi det ud af bogen så?] library(caret) mdl &lt;- plsda(data.frame(beercata[,3:29]),factor(beercata$Beer),ncomp = 3) scores &lt;- mdl$scores %&gt;% unclass %&gt;% as.data.frame %&gt;% cbind(beercata) loadings &lt;- mdl$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% mutate(attrib2 = substr(attrib,3,50)) # lets remove the S_ g1 &lt;- ggplot(data = loadings, aes(`Comp 1`, `Comp 2`, label = attrib2)) + # geom_point() + geom_text() g2 &lt;- ggplot(data = scores, aes(`Comp 1`, `Comp 2`, color = Beer)) + # geom_point() + stat_ellipse(level = 0.5) library(patchwork) g1 + g2 # do multiple splithalfs # INPUT: judge id. CATA, class, ncomp X &lt;- beercata[,3:29] clss &lt;- factor(beercata$Beer) judge &lt;- beercata$Consumer.ID k &lt;- 3 A &lt;- 30 mdl0 &lt;- plsda(X,clss,ncomp = k) lds0 &lt;- mdl0$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% gather(cmp,val0,-attrib) unjudge &lt;- unique(judge) nindiv &lt;- length(unjudge) LOADS &lt;- data.frame() for (i in 1:A){ ic &lt;- judge %in% sample(unjudge)[1:round(nindiv/2)] mdlSH &lt;- plsda(X[ic,],clss[ic],ncomp = k) df_flip &lt;- data.frame(sng = sign(diag(t(mdl0$loadings) %*% mdlSH$loadings))) %&gt;% rownames_to_column(&#39;cmp&#39;) lds &lt;- mdlSH$loadings %&gt;% unclass %&gt;% as.data.frame %&gt;% rownames_to_column(&#39;attrib&#39;) %&gt;% gather(cmp,val,-attrib) %&gt;% left_join(df_flip, by = &#39;cmp&#39;) %&gt;% mutate(SHiter = i, val = val*sng) LOADS &lt;- bind_rows(LOADS,lds) } fc &lt;- (1 / A)*((A - 1)/A) sdloads &lt;- LOADS %&gt;% left_join(lds0, by = c(&#39;attrib&#39;,&#39;cmp&#39;)) %&gt;% group_by(attrib,cmp) %&gt;% dplyr::summarise(sd = sum((val-val0)^2) *fc) %&gt;% mutate(cmp = paste(&#39;sd&#39;,cmp,sep = &#39;&#39;)) %&gt;% spread(cmp,sd) loadsSH &lt;- lds0 %&gt;% spread(cmp,val0) %&gt;% left_join(sdloads, by = &#39;attrib&#39;) library(ggforce) ggplot(data = loadsSH, aes(x0 = `Comp 1`,y0 = `Comp 2`,a = `sdComp 1`,b = `sdComp 2`,angle = 0)) + geom_ellipse() "],["text-mining-of-comsumer-reviews.html", "Chapter 26 Text mining of comsumer reviews", " Chapter 26 Text mining of comsumer reviews "],["text-mining-of-open-ended-survey-responses.html", "Chapter 27 Text mining of open-ended survey responses", " Chapter 27 Text mining of open-ended survey responses "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
