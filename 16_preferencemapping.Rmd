---
editor_options: 
  markdown: 
    wrap: 72
---

```{r, eval=FALSE,include=FALSE}
# Preference Mapping

[some narrative]

## Example of Preference Mapping

[Which???]

```


## Analysis by PLS

The data used in this section is from the paper: *Giacalone, Davide,
Wender L.P. Bredie, and Michael Bom Frøst. "'All-In-One Test' (AI1): A
Rapid and Easily Applicable Approach to Consumer Product Testing." Food
quality and preference 27.2 (2013): 108--119. Web.*. Data can be found
in the *data4consumerscience-package* as **beercata**, **beerliking**

### PLS basics

Partial Least Squares regression (PLS) is a linear regression approach
that shares similarities with Principal Component Analysis (PCA), as
data is decomposed using latent variables. In the case of PLS there are
two matrices: predictors($\boldsymbol{X}$) and
response($\boldsymbol{Y}$). The X-matrix is used to predict the
Y-matrix. For both matrices the scores and loadings and resiudals are
computed:

$$X = TP^T + E_x$$ $$Y = UQ^T + E_y$$

PLS orient latent variables to optimize covariance between X-scores
($\boldsymbol{T}$) and Y-scores ($\boldsymbol{U}$), overcoming
limitations of traditional Multiple Linear Regression, such as handling
cases where variables outnumber observations or when X-variables are
mutually correlated.

### PLS to predict liking based on CATA data

The object of this section is to find which descriptors e.g. sour, is
relevant for the liking, and which are irrelevant.

When dealing with CATA data, it's reasonable to anticipate a low
explained variance in the model due to the high degree of uncertainty
within the CATA data itself. This means that as PLS attempts to
distinguish information from noise, the noise will account for a large
amount of variation in the data.

As preparation for the PLS, the two datasets *beerliking* and *beercata*
is combined:

```{r,warning=FALSE,message=FALSE}
library(dplyr)
library(data4consumerscience)
data("beerliking")
data("beercata")

#Merge the two datasets:
beer <- na.omit(left_join(beercata, beerliking, by = c("Consumer.ID", "Beer")))
```

A dataframe is created by merging the two previously mentioned datasets, by the function *left_join*, based on the common columns *Consumer.ID* and *Beer*. *na.omit* is used to remove rows containing missing values, as the PLS function to be used later on, can not handle missing values in the variables used.

The PLS model can be built with relative ease using the *mdatools*
package:

```{r,warning=FALSE}
library(mdatools)
#Create the PLS object:
beerPLS <- pls(x = beer[,3:29],y = beer$Liking ,center = T,scale = F,ncomp = 10)

summary(beerPLS)
```

The *pls* function is used, and the predictors is put as the x-value,
i.e. the CATA variables in column 3 to 29. and the response i.e. liking. 
Both the y and x matrices are mean-centered, by specifying *center = T*, the *scale* argument specifies if the data should be Auto-scaled.
*ncomp* argument specifies how many components should be calculated - in this case 4.

The model gives a warning of no validation results found. This is
referring to the cross-validation that can be done, to find the optimal number of components needed to describe the optimal complexity of the model. Let us add cross-validation to the model:

```{r}
#Create the PLS object:
beerPLSCV <- pls(x = beer[,3:29],y = beer$Liking ,center = T,scale = F,ncomp = 10,cv = rep(rep(1:length(unique(beer$Consumer.ID)), each = 6), length.out = nrow(beer)))
```

The cross-validation used is specified in the *cv* argument. The
argument takes a vast majority of inputs. Perhaps the easiest to use is
Leave-One-Out cross-validation that is specified by *cv = 1*, but might
also leave to overoptimistic results. In this case we cross-validate
based on the consumer, i.e. we obtain a vector containing
1,1,1,1,1,1...155,155,155,155,155,155 corresponding to the 6-times each
consumer answered.

The summary function can be used to see various information about the
PLS model:

```{r}
summary(beerPLSCV)
```

The summary seems to think 2 components is the optimal amount.

Let us plot the RMSE and see if this correct:

```{r}
plotRMSE(beerPLSCV)
```

The increase in the RMSECV from components 2 to 3, seems to indicate
that the optimal number of components is indeed 2.

### Regression coefficients

In Partial Least Squares (PLS) regression, a regression vector refers to
the set of coefficients that are used to model the relationship between
the predictor variables ($\boldsymbol{X}$) and the response variable
($\boldsymbol{Y}$). These coefficients define how much each predictor
variable contributes to predicting the response variable (in this case
liking).

The regression coefficients can easily be visualized using the
*mdatools* package:

```{r}
plotRegcoeffs(beerPLSCV,show.labels = T, show.ci = T, ncomp = 2)
```

The function *plotRegcoeffs* is used, and the PLS object is specified.
*show.labels = T* specifies the labels from $\boldsymbol{X}$ is printed
on the bars. *show.ci = T* specifies that the confidence interval is
displayed on the bars. *ncomp* specifies the component the regression
coefficents should be displayed for.

From the regression vector plot, it seems the attributes S_Sour, S_Herbs
and S_Regional_Spices significantly lowers the liking of the beers. The
attributes S-Refreshing, S_aromatic, S_caramel among others seems to
significantly increase the liking of the beers. In addition there are
irrelevant non-significant attributes (i.e. confidence interval crossing
0) such as S_foamy and S_Berries among many other attributes.

The significance of the attributes can also be displayed in a table with coefficients, t-values, p-values and confidence interval:

```{r}
summary(beerPLSCV$coeffs, ncomp = 2)
```

These results are in line, with the visual interpretation above.

There are many other interesting plot methods, which can be found here:
[mdatools.com](https://mdatools.com/docs/pls--plotting-methods.html)

## L-PLS

The data used in this section is from the paper: *Giacalone, Davide,
Wender L.P. Bredie, and Michael Bom Frøst. "'All-In-One Test' (AI1): A
Rapid and Easily Applicable Approach to Consumer Product Testing." Food
quality and preference 27.2 (2013): 108--119. Web.*. Data can be found
in the *data4consumerscience-package* as **beercata**, **beerdemo**,
**beerliking**

*L-PLS* is a valuable tool in identifying the sensory attributes and
consumer background characteristics that contribute to a consumer's
liking of a product, such as beer.

A L-PLS model is created using three data blocks: **X1**, **X2**, and
**X3**. **X1** is an IxN matrix that contains consumer liking scores for
each sample, X2 is an IxJ matrix that includes sensory attributes for
each sample, and X3 is a KxN matrix that comprises consumer background
data. I represents the number of samples (beer), N represents the number
of consumers, J represents the number of sensory variables, and K
represents the number of consumer background variables.

There are two ways to perform L-PLS: exo and endo. In the exo-LPLSR
model, X1 serves as the regressor, while in the endo-LPLSR model, X2 and
X3 are the regressors. The exo-LPLSR model typically explains more of X
than the endo-LPLSR model because the exo-LPLSR derives its bilinear
components from X, whereas the endo-LPLSR derives its components from Y
and Z. The difference in the plot's appearance can be predicted from the
explained variance of each model type.

If both plots are similar in nature and in terms of the interpretation
of the correlation loading plots, it may indicate consistent
co-variation between the three matrices.

The exo version will only be showcased here, but the principle is the
same, and endo can be useful for data exploration.

```{r,warning=FALSE,message=FALSE}
library(data4consumerscience)
data("beercata")
data("beerdemo")
data("beerliking")
```

## Creating X1, X2, X3.

The preparation of the data, requires quite a lot of data manipulation.

```{r,warning=FALSE,message=FALSE}
# Load necessary libraries
library(tidyverse)
library(tibble)

# Prepare data for LPLS analysis

## Y - Beer Liking data
X1 <- beerliking %>%
  select(Beer, Liking, Consumer.ID) %>%
  pivot_wider(names_from = Consumer.ID, values_from = Liking) %>%
  column_to_rownames("Beer") %>%
  select_if(~!any(is.na(.)))

## X - Beer Attributes data
colnames(beercata) <- gsub("S_", "", colnames(beercata))
X2 <- beercata %>%
  select(Beer, Flowers:Vinous) %>%
  pivot_longer(cols = !Beer, names_to = "Attribute", values_to = "Value") %>%
  group_by(Beer, Attribute) %>%
  dplyr::summarise(Sum_value = sum(Value)) %>%
  pivot_wider(names_from = "Attribute", values_from = "Sum_value", values_fill = 0) %>%
  column_to_rownames("Beer")

## Z - Consumer Demographics data
# select subset of columns and create binary columns for categorical variables
binary_cols <- c("Gender", "Age", "Income", "Householdsize", "Beer types/month")
for (col in binary_cols) {
  if (is.factor(beerdemo[[col]]) | is.character(beerdemo[[col]])) {
    levels <- unique(beerdemo[[col]])
    for (level in levels) {
      new_col <- paste(col, level, sep = "_")
      beerdemo[[new_col]] <- ifelse(beerdemo[[col]] == level, "1", "0")
    }
  }
}

# pivot longer and summarize to calculate sum values for each Consumer ID-Attribute combination
X3 <- beerdemo %>%
  select(-(Gender:`Beer types/month`),`Interest in food`:`Beer types/month_9 - 16`,`Consumer ID`) %>%
  mutate_at(vars(-`Consumer ID`), 
            .funs = list(~as.factor(.) %>% as.numeric())) %>%
  pivot_longer(cols =`Interest in food`:`Beer types/month_9 - 16` , names_to = "Attribute", values_to = "Value") %>%
  group_by(`Consumer ID`, Attribute) %>%
  dplyr::summarise(sum_Value = sum(Value)) %>%
  pivot_wider(names_from = "Consumer ID", values_from = "sum_Value", values_fill = 0) %>%
  column_to_rownames("Attribute")

# keep only columns in Y
col_names <- names(X1)
X3 <- X3[,col_names]

# LPLS function only accepts matrix. Convert data frames to matrices.
X1_mat <- as.matrix(X1)
X2_mat <- as.matrix(X2)
X3_mat <- as.matrix(X3)

```

## Building the model: The L-PLS model can be built, using the *lpls*
function from the *multiblock* package.

```{r,warning=FALSE,message=FALSE, eval=FALSE}
library(multiblock)
set.seed(123)
#Scale the Z-matrix as we have semi-contious and binary data. 
lp_exo <- lpls(X1_mat,X2_mat,X3_mat,type = "exo",scale = c(F,F,T),doublecenter = T,ncomp = 5)
```

### Finding the explained variance for exo L-PLS

The explained variance for the exo L-PLS can easily be sacked from the
model object, and displayed nicely in a table using the *kableExtra*
package.

```{r,warning=FALSE,message=FALSE, eval=FALSE}
library(knitr)
library(kableExtra)
expVarExo <- t(as.data.frame(lp_exo$vars)*100)
colnames(expVarExo) <- paste0("Comp ", 1:5)
rownames(expVarExo) <- paste0("X", 1:3)

#Create a table
kbl(expVarExo, caption = "Explained Variance by Component (%)",digits = 0) %>%
  kable_paper("hover",full_width = F)
```

The explained variance is first sacked from the L-PLS object, and stored
in a data-frame, from where it is transposed, and converted to
percentage (%). *kbl* function is then used to get a nice table.

From looking at the table, one would expect the consumer descriptors to
be close to origin in terms of scores. One could have a look at the endo
version of the L-PLS to get a better understanding of the consumer
descriptors.

### Cross-validation Next let us have a look how many components we
should look at, by doing a jack-knifing cross-validation on the
consumers:

```{r, eval=FALSE}
lp.cv2 <- lplsCV(lp_exo, segments2 = as.list(1:dim(X1)[2]))
lp.cv2$rmsep
```

It looks like 2 components is sufficient.

### Vizulization Now we would like to visuzlalize our L-PLS model. When
using L-PLS for vizulization one should use the orthognoal exo, as this
will give the orthogonal scores. We build the orthogonal exo in the same
way as before, but chainging the type.

```{r, eval=FALSE}
lp_exo_ort <- lpls(X1_mat,X2_mat,X3_mat,type = "exo_ort",scale = c(F,F,T),doublecenter = T,ncomp = 5)
```

Now we plot the L-PLS model. One can chose to plot, the X1, X2 or X3
correlations, or perhaps a combination of 2, or all.

```{r, eval=FALSE}
par(mfrow=c(2,2))
plot(lp_exo,doplot = c(F,F,T)) #X3 correlations
plot(lp_exo,doplot = c(F,T,F)) #X1 correlations
plot(lp_exo,doplot = c(T,T,F)) #X2 and X1 correlations
plot(lp_exo,doplot = c(F,T,T)) #X1 and X3 correlations
```

```{r, eval=FALSE}
plot(lp_exo,doplot = c(T,T,T)) #combined
```

From the plots, the sensory attribute *Berries* is correlated with the
sensory attribute *Herbs*. These two sensory attributes is in turn
correlated with the beer *River Beer*. In turn *River Beer* is opposite
of the beer *Brown Ale*, which seems to be very *Foamy*. In this case
our consumer background descriptors do not seem to explain a whole lot.
But it does seem like a young age *Age 18-25* is associated with a low
income (*Income 0-10.000*). Where a consumer of such description tends
to like *Porse Bock*.

It is always a good idea to confirm the observed correlations by
plotting the raw data. Lets have a look at some of them.
