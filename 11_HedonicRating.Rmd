
# Hedonic rating (e.g. liking scores)

An evaluation of how much we like a food or drink is a hedonic response. Often this is given as a number on a scale, or by checking a category box with a description, such as the 9-point hedonic scale. For data analysis, the boxes are made numerical, with the assumption the there is equidistance between the meaning of the category labels, i.e dislike extremely and dislike very much is as different as dislike a little and neutral. The original English language anchors for the 9-point hedonic scale was develop and validated on tests using approximately 5500 American soldiers in the beginning of the 1950’s (Lawless & Heymann, Chapter 7 Scaling, 2010). There are a number of validated scales, including visual smiley scales for children. Dependent on the respondent 5, 7 or 9 point scales are used. 

Most often we want to know if the hedonic response is significantly different (not just different by chance) depending on the samples. We might also want to know if other factors have an influence on the hedonic rating. e.g. household income or sex. We also want to know what are the actual differences are in numerical size. All this requires different statistical estimates. 

## Plotting liking scores

For the beer data we have the liking in a long matrix. 

```{r, message=FALSE}
library(data4consumerscience)
library(ggplot2)
data(beerliking)
```

A histogram of the likings shows that some are symmetric ( _NY Lager_, _River Beer_ and to some extend _Porse Bock_), while _Brown Ale_ and _Ravnsborg Red_ is skeew, and _Wheat IPA_ is uniform. 

```{r, message=FALSE}
ggplot(data = beerliking, aes(Liking)) + 
  geom_histogram() + 
  facet_wrap(~Beer)
```

## Simple mixed models

In the following plot each liking score is connected within consumer across the beer types. The facet (according to Age) is just to avoid overplotting. 

Here, there is a trend towards, if you rate one beer high, the other likings within that consumer will also be high. 

```{r, message=FALSE}
ggplot(data = beerliking, aes(Beer,Liking, group = Consumer.ID, color = Consumer.ID)) + 
  geom_point() + 
  geom_line() + 
  theme(legend.position = 'none') + 
  facet_wrap(~Age)
```

Mixed models are used when there is repetitions in the response due to (here) the person tasting more than one product.

```{r}
library(lmerTest)
library(lme4)
mdl <- lmer(data = beerliking, Liking ~ Beer + (1|Consumer.ID)) 
summary(mdl)
```

Here we see that the residual uncertainty is $1.63$ on the 1-7 likert scale, while the uncertainty between consumers is $0.57$. This implies that the uncertainty between two ratings is higher when from two different consumers, compared to two ratings from the same consumer. 

We can use this model to evaluate the effect of the different beers. 

```{r}
anova(mdl)
```

It seems as the average liking is different between beers. 

### Post hoc test

The overall anova result implies that the 6 beers are _NOT_ equal in terms of liking, but some may be similar. 

This can be investigated by computing pairwise contrasts using for instance the _emmeans_ package or the _multcomp_ package.

Both are shown below

```{r, message=FALSE}
library(emmeans)
mdlemm <- emmeans(mdl,'Beer')
contrast(mdlemm,'tukey')
```

To calculate pairwise comparisons between e.g. samples and find letter-based representation you need a the package _multcomp_. 

```{r, message=FALSE}
library(multcomp)
cld(glht(mdl, linfct = mcp(Beer = "Tukey")))
```

The letters are based on the Post Hoc test Tukey, and they are calculated on the basis of the model _mdl_. Samples with the same letters are not significantly different. You can only use this function on factors. 

The resulst indicate _Brown Ale_ and _Ravnsborg Red_ is the most likable beers, with _River Beer_, _Wheat IPA_ and _Porse Bock_ as the least likable ones. These three at the bottom is also not significantly different.  


```{r, eval = F, include=FALSE}
[MERE TEKST PÅ HER]
[Forklar fixed og random effects? Måske bruge dette:Fixed effects are effects that we anticipate have the same direction, e.g., mutual differences between products. Would typically be the same from one experiment to another as the products are unchanging entities. Random effects are effects that we cannot predict, e.g., mutual differences between consumers may differ from one experiment to another as consumers are affected by various emotional, environmental, physiological or other influences in their lives]

[MORTEN: Sensorikere er virkelig glade for p-værdier for en variabel - det kan jeg ikke se, jeg kan få ud med lme4, derfor foreslår jeg  pakken lmerTest i stedet for. Lavet af Per Brockhoff til sensorikdata. og så også anova() i stedet for summary(). Så kommer der een overordnet p-værdi ud ]

To explain the model: take the dataset called _beerliking_ and calculate a model where _Beer_ is the fixed effect and the consumer is the random effect for the response variable _Liking_. Use the function _lmer_ and save the output as _mdl_. The anova() function will provide you with the p value(s). Remember you choose your own title of your model. 

[MANGLER: forklaring på output; At least two of the products by name are scored significantly different for the liking.]

[MORTEN: Hvis man nu har kontinuerte variable, hvordan fortolkes det så?]

[explain + output + interpret]
```


## Multivariable models


In the example above, only the impact of the different beers is evaluated, however, the liking scores may also depend on background information such as gender, age, ... as well as attitude towards beer and food. Further, is there any of the background variables that attenuate or make the differences between the beers stronger? These questions can be investigated by models including several explanatory variables as well as interaction terms. 

### Additive models

This can be included in the models as a sequence of explanatory variables. 

Given a set of possible explanatory variables, there is two ways to include them in the model. 

**Forward step-wise Selection** and **Backward Step-wise Elimination**. 

For both, the principle is simple and intuitive. 

In the forward procedure each variable is added to the model, and the strongest one (in terms of the lowest p-value) is kept. 

In the backward procedure all variables are added to the model and the least important one (in terms of the largest p-value) is removed. 

Both procedures stop when the model is not going to improve by adding or eliminating explanatory variables, and the final model will only contain the significant variables.

In the beer dataset we would like to know which of the explanatory variables that is most related to the liking. 

A large model with all explanatory variables is constructed

```{r}
mdl_be <- lmer(data = beerliking, Liking ~ Beer + Gender + Age + Income + 
                 Householdsize + `Beer types/month` + `Interest in food` + 
                 neophilia + `Interest in beer` + `Beer knowledge` + 
                 `Ingredients/labels` + `Future interest in beer` + 
                 (1|Consumer.ID)) 
anova(mdl_be)
```

From this, _Interest in food_ is the least significant one, and is hence removed. 

A sequential removal of the non-significant variables at a $p > 0.1$ level leads to the following model: 

```{r}
mdl_be_red <- lmer(data = beerliking, Liking ~ Beer +  Age + Income + 
                 `Beer types/month` + `Beer knowledge` + 
                  `Future interest in beer` + 
                 (1|Consumer.ID)) 
anova(mdl_be_red)
```

The results can be interpreted from the estimates: 

```{r}
summary(mdl_be_red)
```

Some notes: Liking is higher in the lowest _Income_ group, Liking is lower in the lowest age group, and liking is higher with higher _Future interest in beer_. 

### Effect modification and Interactions

It could be nice to calculate if the liking of specific sample are affected by the degree of future interest in beer. This can be visualized as scatterplots and modelled using interactions.

```{r, message=FALSE}
ggplot(data = beerliking, aes(`Future interest in beer`, Liking, color = Beer)) + 
  geom_point() + stat_smooth(se = F, method = lm)

mdl_interaction <- lmer(data = beerliking, Liking ~ Beer*`Future interest in beer` +  (1|Consumer.ID)) 
anova(mdl_interaction)
```

Although the slopes appear a bit different between the beers this is not significant ($p = 0.12$). 
**Be aware** that it appears as the main effect of beer is not significant. However, this value should not be interpreted, as there are interaction terms in the model including beer. 

Remember you choose your own “name” for the model
“+” between variables defines (additive) main effects
“:”  between variables defines an interaction
“*” between variables defines a parameterization with both interaction and main effect terms

For model selection here, you start by removing the interaction with the hight p-value and then recalculate the model. You cannot remove a single variable if an interaction including it is significant. 

